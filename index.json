[{"authors":null,"categories":null,"content":"I‚Äôm Noah Greifer (pronounced gree-fur; pronouns: he/him), a statistical consultant and programmer at Harvard University. I provide statistical consulting services as a member of the Data Science Services team at the Institute for Quantitative Social Science (IQSS) at Harvard. I also develop R packages, both as part of my job and in my personal time. Some of these packages include MatchIt, WeightIt, and cobalt, which facilitate the use of propensity score methods. See my other packages at the Software link above. I post on my blog about R programming, statistical analysis, and R package development.\nI currently reside in Cambridge, MA, but I grew up in Los Angeles, CA, and have lived in San Diego, CA, Portland, OR, and Durham, NC.\n Download my resum√©. -- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"05d2cb0df913990c9440f73073d0c063","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I‚Äôm Noah Greifer (pronounced gree-fur; pronouns: he/him), a statistical consultant and programmer at Harvard University. I provide statistical consulting services as a member of the Data Science Services team at the Institute for Quantitative Social Science (IQSS) at Harvard.","tags":null,"title":"Noah Greifer","type":"authors"},{"authors":null,"categories":["R"],"content":" Multiply imputed data always makes things a little harder. Essentially, you have to perform each step of the analysis in each imputed dataset and then combine the results together in a special way. For basic regression analysis, the mice package makes fitting models and combining estimates simple. But when we want to do propensity score matching or weighting before fitting our regression models, and when the quantity we want to estimate is not just a coefficient in a regression model, things get a bit harder.\nFor doing matching or weighting in multiply imputed data, the R package {MatchThem} does the job. It essentially provides wrappers for MatchIt::matchit() and WeightIt::weightit() for multiply imputed data. It extends {mice}‚Äôs functionality for fitting regression models in multiply imputed data by automatically incorporating the matched or weighted structure into the estimation of the outcome models. It uses mice::pool() to pool estimates across multiply imputed data.\nBut for estimating treatment effects, it‚Äôs often not as simple as using a regression coefficient. If we include covariates in our outcome model but want a marginal effect, we need to use an average marginal effects procedure (i.e., g-computation) to compute it within each imputed dataset, and then combine the results afterward. The {marginaleffects} package provides a wonderful interface for performing g-computation, but for multiply imputed data, it can require some programming by the analyst. In this guide, I‚Äôll show you how to do that programming to combine treatment effect estimates across multiple imputed datasets.\nAn alternative to using {marginaleffects} is to use the {clarify} package. {clarify} can also be used to perform g-computation, but it uses simulation-based inference to compute the uncertainty bounds for the estimate. An advantage of simulation-based inference for multiply imputed data is that combining estimates across imputed datasets is much more straightforward. In this guide, I‚Äôll also show you how to use {clarify} to combine treatment effect estimates across imputed datasets.\nPackages we‚Äôll need We will need the following packages for this demonstration: cobalt, mice, MatchThem, WeightIt, marginaleffects, and clarify.\n The data As usual, we‚Äôll be using a version of the lalonde dataset. Here will use the lalonde_mis dataset in {cobalt}, which has missing values.\ndata(\u0026#34;lalonde_mis\u0026#34;, package = \u0026#34;cobalt\u0026#34;) summary(lalonde_mis) ## treat age educ race married nodegree re74 re75 re78 ## Min. :0.0000 Min. :16.00 Min. : 0.00 black :243 Min. :0.0000 Min. :0.0000 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.:0.0000 1st Qu.:20.00 1st Qu.: 9.00 hispan: 72 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.: 0.0 1st Qu.: 0.0 1st Qu.: 238.3 ## Median :0.0000 Median :25.00 Median :11.00 white :299 Median :0.0000 Median :1.0000 Median : 984.5 Median : 585.4 Median : 4759.0 ## Mean :0.3013 Mean :27.36 Mean :10.27 Mean :0.4158 Mean :0.6303 Mean : 4420.2 Mean : 2170.3 Mean : 6792.8 ## 3rd Qu.:1.0000 3rd Qu.:32.00 3rd Qu.:12.00 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.: 7626.9 3rd Qu.: 3202.0 3rd Qu.:10893.6 ## Max. :1.0000 Max. :55.00 Max. :18.00 Max. :1.0000 Max. :1.0000 Max. :35040.1 Max. :25142.2 Max. :60307.9 ## NA\u0026#39;s :20 NA\u0026#39;s :40 NA\u0026#39;s :39 You can see there are some missing values in married, re74, and re75.\n Imputing the data Here, we‚Äôll use {mice} to impute the data. Although typically something like 20 imputation is sufficient, for the method {clarify} uses, it needs way more, so we‚Äôll use 50. We‚Äôll use the default settings, but you should tailor the imputation to fit the needs of your dataset. (I always like to use a machine learning method for my imputations). We‚Äôll also set a seed to ensure replicability.\nlibrary(\u0026#34;mice\u0026#34;) set.seed(12345) imp \u0026lt;- mice(lalonde_mis, m = 50, printFlag = FALSE) mice() returns a mids object, which contains the imputed datasets. Although we could extract the datasets using complete(), we‚Äôll supply this object directly to our function for estimating the propensity score weights.\n Weighting the imputed data We‚Äôll use MatchThem::weightthem() to estimate propensity score weights in the imputed datasets. We could also use MatchThem::matchthem() to do matching; the process is basically identical1. Here we‚Äôll use logistic regression (ü§¢) to estimate ATT weights to keep things quick and simple.\nlibrary(\u0026#34;MatchThem\u0026#34;) w.imp \u0026lt;- weightthem(treat ~ age + educ + race + married + nodegree + re74 + re75, data = imp, method = \u0026#34;ps\u0026#34;, estimand = \u0026#34;ATT\u0026#34;) Let‚Äôs assess balance using {cobalt}.\nlibrary(\u0026#34;cobalt\u0026#34;) bal.tab(w.imp, stats = c(\u0026#34;m\u0026#34;, \u0026#34;ks\u0026#34;), abs = TRUE) ## Balance summary across all imputations ## Type Mean.Diff.Adj Max.Diff.Adj Mean.KS.Adj Max.KS.Adj ## prop.score Distance 0.0235 0.0379 0.1166 0.1327 ## age Contin. 0.1120 0.1343 0.3053 0.3146 ## educ Contin. 0.0352 0.0485 0.0369 0.0412 ## race_black Binary 0.0024 0.0036 0.0024 0.0036 ## race_hispan Binary 0.0003 0.0007 0.0003 0.0007 ## race_white Binary 0.0022 0.0030 0.0022 0.0030 ## married ‚Ä¶","date":1675987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675987200,"objectID":"234eb4b23fdb18962d30bce146912cbd","permalink":"https://ngreifer.github.io/blog/treatment-effects-mi/","publishdate":"2023-02-10T00:00:00Z","relpermalink":"/blog/treatment-effects-mi/","section":"blog","summary":"Multiply imputed data always makes things a little harder. Essentially, you have to perform each step of the analysis in each imputed dataset and then combine the results together in a special way.","tags":["propensity-scores","R","multiple-imputation"],"title":"Estimating Treatment Effects After Weighting with Multiply Imputed Data","type":"blog"},{"authors":null,"categories":null,"content":" Genetic matching sounds cool and science-y, something we social scientists love because nobody thinks what we do is ‚Äúreal‚Äù science. And genetic matching is cool and science-y, but not because it has anything to do with genes or DNA. Genetic matching is a method of adjusting for confounding in observational studies; it is a close relative of propensity score matching and Mahalanobis distance matching and serves exactly the same purpose. Sekhon (2011) and Diamond and Sekhon (2013) describe genetic matching, but I‚Äôll explain it here in simple terms and with an emphasis on its generality, which is undersold by its implementations.\nThis post won‚Äôt make any sense if you don‚Äôt know what matching in general is. Go read Stuart (2010), Greifer and Stuart (2021), and the MatchIt vignette on matching methods to learn about them. The focus here will be on pair matching, which involves assigning units to pairs or strata based on the distances between them, then discarding unpaired units.\nThe goal of matching is balanced samples, i.e., samples where the distribution of covariates in the treated and control groups is the same so that an estimated treatment effect cannot be said to be due to differences in the covariate distributions. Why, then, do we make pairs? Close pairs create balance, in theory. How do we compute how close units are to each other? There are several ways; a common one is the Mahalanobis distance, as described for matching in Rubin (1980), and which I‚Äôll describe here.\nThe Mahalanobis distance between two units \\(i\\) and \\(j\\) is defined as\n\\[ \\delta^{md}_{i,j}=\\sqrt{(\\mathbf{x}_i-\\mathbf{x}_j)\\Sigma^{-1}(\\mathbf{x}_i-\\mathbf{x}_j)\u0026#39;} \\]\nwhere \\(\\mathbf{x}_i\\) is the vector of covariates for unit \\(i\\) (i.e., that unit‚Äôs row in the dataset) and \\(\\Sigma\\) is the covariance matrix of the covariates1. Equivalently, the Mahalanobis distance is the Euclidean distance (i.e., the regular distance) computed on the standardized principal components. The Mahalanobis distance is an improvement over the Euclidean distance of the covariates because it standardizes the covariates to be on the same scale and adjusts for correlations between covariates (so two highly correlated variables only count once). A great description of the Mahalanobis distance is here (though there it is not described in the context of matching).\nGenetic matching concerns a generalization of the Mahalanobis distance, called the generalized Mahalanobis distance, which additionally involves a weight matrix. The generalized Mahalanobis distance is defined as\n\\[ \\delta^{gmd}_{i,j}(W)=\\sqrt{(\\mathbf{x}_i-\\mathbf{x}_j)\u0026#39;\\left(\\Sigma^{-\\frac{1}{2}}\\right)\u0026#39; W\\Sigma^{-\\frac{1}{2}}(\\mathbf{x}_i-\\mathbf{x}_j)} \\]\nwhere \\(\\Sigma^{-\\frac{1}{2}}\\) is the ‚Äúsquare root‚Äù of the inverse of the covariance matrix (e.g., the Cholesky decomposition), and \\(W\\) is a symmetric weight matrix that can contain anything but in most cases is a diagonal matrix with a scalar weight for each covariate in \\(\\mathbf{x}\\) (not weights for each unit like in propensity score weighting; a weight for each covariate), i.e., \\(W = \\text{diag}(\\begin{bmatrix} w_1 \u0026amp; \\dots \u0026amp; w_p \\end{bmatrix})\\). The generalized Mahalanobis distance is equal to the usual Mahalanobis distance when \\(W=I\\), the identity matrix.\nWhat does any of this have to do with genetic matching? Well, ‚Äúgenetic matching‚Äù is a bit of a misnomer; it‚Äôs not a matching method. It‚Äôs a method of estimating \\(W\\). Genetic matching finds the \\(W\\) that, when incorporated in a generalized Mahalanobis distance used to match treated and control units, yields the best balance. Once you have found \\(W\\), you then do a regular round of matching, and that is your matched sample.\nTo put it slightly more formally, consider a function \\(\\text{match}(\\delta)\\), which takes in a distance matrix \\(\\delta\\) and produces a matched set of treated and control units, characterized by a set of matching weights (e.g., 1 if matched, 0 if unmatched) and pair membership for each unit. Consider a function \\(\\text{imbalance}(m)\\), which takes in the output of a \\(\\text{match}(\\delta)\\) and returns a scalar imbalance metric (e.g., the largest absolute standardized mean difference among all the covariates). We can then write the genetic matching problem as the following:\n\\[ \\underset{W}{\\operatorname{arg\\,min}} \\, \\text{imbalance}(\\text{match}(\\delta^{gmd}(W))) \\]\nGenetic matching is very general; there are many ways to do the matching (i.e., many ways to specify the \\(\\text{match}()\\) function) and many ways to characterize imbalance (i.e., many ways to specify the \\(\\text{imbalance}()\\) function) (and even several ways to specific \\(\\delta()\\)!). Although nearest neighbor matching is often used for \\(\\text{match}()\\), any matching method that uses a distance matrix could be as well. A specific imbalance measure (which I‚Äôll explain in more detail later) is most often used for \\(\\text{imbalance}()\\) because it is the default in the software that ‚Ä¶","date":1665187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665187200,"objectID":"1b788514d90afeaa9df4e6c04f68115a","permalink":"https://ngreifer.github.io/blog/genetic-matching/","publishdate":"2022-10-08T00:00:00Z","relpermalink":"/blog/genetic-matching/","section":"blog","summary":"Genetic matching sounds cool and science-y, something we social scientists love because nobody thinks what we do is ‚Äúreal‚Äù science. And genetic matching is cool and science-y, but not because it has anything to do with genes or DNA.","tags":["matching","R"],"title":"Genetic Matching, from the Ground Up","type":"blog"},{"authors":null,"categories":null,"content":" Today I‚Äôm going to demonstrate performing a subgroup analysis after propensity score matching using R. Subgroup analysis, also known as moderation analysis or the analysis of effect modification, concerns the estimation of treatment effects within subgroups of a pre-treatment covariate. This post assumes you understand how to do propensity score matching. For a general introduction to propensity score matching, I recommend Austin (2011) and the {MatchIt} introductory vignette. If you understand inverse probability weighting but aren‚Äôt too familiar with matching, I recommend my article with Liz Stuart (Greifer and Stuart 2021). For an introduction to subgroup analysis with propensity scores, you can also check out Green and Stuart (2014). Here, I‚Äôll mainly try to get to the point.\nThe dataset we‚Äôll use today is the famous Lalonde dataset, investigating the effect of a job training program on earnings. We‚Äôll use the version of this dataset that comes with the {MatchIt} package.\ndata(\u0026#34;lalonde\u0026#34;, package = \u0026#34;MatchIt\u0026#34;) head(lalonde) ## treat age educ race married nodegree re74 re75 re78 ## NSW1 1 37 11 black 1 1 0 0 9930.0460 ## NSW2 1 22 9 hispan 0 1 0 0 3595.8940 ## NSW3 1 30 12 black 0 0 0 0 24909.4500 ## NSW4 1 27 11 black 0 1 0 0 7506.1460 ## NSW5 1 33 8 black 0 1 0 0 289.7899 ## NSW6 1 22 9 black 0 1 0 0 4056.4940 The treatment is treat, the outcome in the original study was re78 (1978 earnings), and the other variables are pretreatment covariates that we want to adjust for using propensity score matching. In this example, I‚Äôll actually be using a different outcome, re78_0, which is whether the participant‚Äôs 1978 earnings were equal to 0 or not, because I want to demonstrate the procedure for a binary outcome. So, we hope the treatment effect is negative, i.e., the risk of 0 earnings decreases for those in the treatment.\nlalonde$re78_0 \u0026lt;- as.numeric(lalonde$re78 == 0) Our moderator will be race, a 3-category factor variable.\nwith(lalonde, table(race)) ## race ## black hispan white ## 243 72 299 Our estimand will be the subgroup-specific and marginal average treatment effect on the treated (ATT), using the risk difference as our effect measure.\nPackages You‚Äôll Need We‚Äôll need a few R packages for this analysis. We‚Äôll need {MatchIt} and {optmatch} for the matching, {cobalt} for the balance assessment, {marginaleffects} for estimating the treatment effects, and {sandwich} for computing the standard errors. You can install those using the code below:\ninstall.packages(c(\u0026#34;MatchIt\u0026#34;, \u0026#34;optmatch\u0026#34;, \u0026#34;cobalt\u0026#34;, \u0026#34;marginaleffects\u0026#34;, \u0026#34;sandwich\u0026#34;)) Let‚Äôs get into it!\n Step 1: Subgroup Matching Our first step is to perform the matching. Although there are a few strategies for performing matching for subgroup analysis, in general subgroup-specific matching tends to work best, though it requires a little extra work.\nWe‚Äôll do this by splitting the dataset by race and performing a separate matching analysis within each one.\n#Splitting the data lalonde_b \u0026lt;- subset(lalonde, race == \u0026#34;black\u0026#34;) lalonde_h \u0026lt;- subset(lalonde, race == \u0026#34;hispan\u0026#34;) lalonde_w \u0026lt;- subset(lalonde, race == \u0026#34;white\u0026#34;) Here we‚Äôll use full matching because 1:1 matching without replacement, the most common (but worst) way to do propensity score matching, doesn‚Äôt work well in this dataset. The process described below works exactly the same for 1:1 and most other kinds of matching as it does for full matching. We‚Äôll estimate propensity scores in each subgroup, here using probit regression, which happens to yield better balance than logistic regression does.\nlibrary(\u0026#34;MatchIt\u0026#34;) #Matching in race == \u0026#34;black\u0026#34; m.out_b \u0026lt;- matchit(treat ~ age + educ + married + nodegree + re74 + re75, data = lalonde_b, method = \u0026#34;full\u0026#34;, estimand = \u0026#34;ATT\u0026#34;, link = \u0026#34;probit\u0026#34;) #Matching in race == \u0026#34;hispan\u0026#34; m.out_h \u0026lt;- matchit(treat ~ age + educ + married + nodegree + re74 + re75, data = lalonde_h, method = \u0026#34;full\u0026#34;, estimand = \u0026#34;ATT\u0026#34;, link = \u0026#34;probit\u0026#34;) #Matching in race == \u0026#34;black\u0026#34; m.out_w \u0026lt;- matchit(treat ~ age + educ + married + nodegree + re74 + re75, data = lalonde_w, method = \u0026#34;full\u0026#34;, estimand = \u0026#34;ATT\u0026#34;, link = \u0026#34;probit\u0026#34;)  Step 2: Assessing Balance within Subgroups We need to assess subgroup balance; we can do that using summary() on each matchit object, or we can use functions from {cobalt}.\nBelow are examples of using summary() and cobalt::bal.tab() on one matchit object at a time1:\nsummary(m.out_b) ## ## Call: ## matchit(formula = treat ~ age + educ + married + nodegree + re74 + ## re75, data = lalonde_b, method = \u0026#34;full\u0026#34;, link = \u0026#34;probit\u0026#34;, ## estimand = \u0026#34;ATT\u0026#34;) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 0.6587 0.6121 0.4851 0.7278 0.1134 0.1972 ## age 25.9808 26.0690 -0.0121 0.4511 0.0902 0.2378 ## educ 10.3141 10.0920 0.1079 0.5436 0.0336 0.0807 ## married 0.1859 0.2874 -0.2608 . 0.1015 0.1015 ## nodegree 0.7244 0.6437 0.1806 . 0.0807 0.0807 ## re74 2155.0132 3117.0584 -0.1881 0.9436 0.0890 0.2863 ## re75 1490.7221 1834.4220 ‚Ä¶","date":1662336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662336000,"objectID":"2ce2bc3725b3a8aff6cde5b4b0c90240","permalink":"https://ngreifer.github.io/blog/subgroup-analysis-psm/","publishdate":"2022-09-05T00:00:00Z","relpermalink":"/blog/subgroup-analysis-psm/","section":"blog","summary":"Today I‚Äôm going to demonstrate performing a subgroup analysis after propensity score matching using R. Subgroup analysis, also known as moderation analysis or the analysis of effect modification, concerns the estimation of treatment effects within subgroups of a pre-treatment covariate.","tags":["matching","propensity-scores","R","subgroup analysis"],"title":"Subgroup Analysis After Propensity Score Matching Using R","type":"blog"},{"authors":null,"categories":null,"content":"This page documents the software packages I have worked on. if you have any questions about them, please submit your question to their GitHub issues page rather than emailing me. You are also welcome to ask a question on StackOverflow or CrossValidated, which I check often.\nR packages These packages are ones that I am a primary author on and have expertise on the methods implemented. I consider these packages to be ‚Äúmine‚Äù, at least partly, in the sense that I can speak not only on the implementation but on the methods as well.\n cobalt: Covariate Balance Tables and Plots  Noah Greifer | website | CRAN | source   WeightIt: Weighting for Covariate Balance in Observational Studies  Noah Greifer | website | CRAN | source   MatchIt: Nonparametric Preprocessing for Parametric Causal Inference  Daniel Ho, Kosuke Imai, Gary King, Elizabeth Stuart, and Noah Greifer | website | CRAN | source   MatchThem: Matching and Weighting Multiply Imputed Datasets  Farhad Pishgar and Noah Greifer | CRAN | source   optweight: Targeted Stable Balancing Weights Using Optimization  Noah Greifer | CRAN | source   MatchingFrontier: Computation of the Balance-Sample Size Frontier in Matching Methods for Causal Inference  Gary King, Christopher Lucas, Richard Nielsen, and Noah Greifer | website | source   fwb: Fractional Weighted Bootstrap  Noah Greifer | website | CRAN | source    These packages are ones that I have developed as part of my job but which I don‚Äôt consider ‚Äúmine‚Äù in the sense that I am not the primary maintainer and I don‚Äôt have expertise on the methods implemented. Please do not contact me about these packages.\n  netlit: Augment a literature review with network analysis statistics\n Devin Judge-Lord and Noah Greifer | website | source    EvoPhylo: Pre- And Postprocessing of Morphological Data from Relaxed Clock Bayesian Phylogenetics\n Tiago Sim√µes, Noah Greifer, and Stephanie Pierce | website | CRAN | source    Morphoscape: Computation and Visualization of Adaptive Landscapes\n Blake Dickson, Stephanie Pierce, and Noah Greifer | website | CRAN | source    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6fcae98d7df3b6c44952e7b5fed181e3","permalink":"https://ngreifer.github.io/software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/software/","section":"","summary":"This page documents the software packages I have worked on. if you have any questions about them, please submit your question to their GitHub issues page rather than emailing me. You are also welcome to ask a question on StackOverflow or CrossValidated, which I check often.","tags":null,"title":"Software","type":"page"}]