<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Noah Greifer</title>
    <link>https://ngreifer.github.io/category/r/</link>
      <atom:link href="https://ngreifer.github.io/category/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 21 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ngreifer.github.io/media/sharing.jpg</url>
      <title>R</title>
      <link>https://ngreifer.github.io/category/r/</link>
    </image>
    
    <item>
      <title>What&#39;s New in `WeightIt` Version 1.0.0</title>
      <link>https://ngreifer.github.io/blog/what-s-new-in-weightit-version-1-0-0/</link>
      <pubDate>Thu, 21 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/what-s-new-in-weightit-version-1-0-0/</guid>
      <description>


&lt;p&gt;My R package &lt;a href=&#34;https://ngreifer.github.io/WeightIt/&#34;&gt;&lt;code&gt;WeightIt&lt;/code&gt;&lt;/a&gt; has a huge new update, making it one of the biggest updates since I started the project. Version 1.0.0 introduces a few breaking changes, including the possibility that old results will not align with results from newer version of the package. In most cases, though, this only means improvements (i.e., better balance).&lt;/p&gt;
&lt;p&gt;For those that don’t know, &lt;code&gt;WeightIt&lt;/code&gt; is an R package designed to provide access to propensity score weighting (also known as inverse probability weighting) and its variations in a way that facilitates the use of advanced and modern methods and best practices. It provides a simple, unified interface to many different methods of estimating weights to balance groups in observational studies, including those that use generalized linear models, machine learning methods, and convex optimization. &lt;code&gt;WeightIt&lt;/code&gt; provides support for binary, multi-category, continuous, and longitudinal treatments and directly interfaces with &lt;a href=&#34;https://ngreifer.github.io/cobalt/&#34;&gt;&lt;code&gt;cobalt&lt;/code&gt;&lt;/a&gt; for assessing balance after weighting.&lt;/p&gt;
&lt;p&gt;Version 1.0.0 has several new features that I wanted to explain in a blog post rather than just in the &lt;a href=&#34;https://ngreifer.github.io/WeightIt/news/index.html&#34;&gt;NEWS&lt;/a&gt; document associated with the package because it’s important to me that these new features are appreciated. There are three primary updates that I’ll discuss, along with some minor ones. Those three are updates to the covariate balancing propensity score, a new method called inverse probability tilting, and new support for fitting weighted outcome regression models that account for estimation of the weights in the standard errors.&lt;/p&gt;
&lt;div id=&#34;new-implementation-of-cbps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New implementation of CBPS&lt;/h2&gt;
&lt;p&gt;The covariate balancing propensity score &lt;span class=&#34;citation&#34;&gt;(CBPS, &lt;a href=&#34;#ref-imai2014&#34;&gt;Imai and Ratkovic 2014&lt;/a&gt;)&lt;/span&gt; is a method of estimating propensity scores using generalized linear models (e.g., logistic regression). I described CBPS in my &lt;a href=&#34;https://ngreifer.github.io/blog/logistic-regression-cbps-overlap-weights/&#34;&gt;last blog post&lt;/a&gt;, so I’ll be brief here. Essentially, you have the score equations for a logistic regression, the roots of which are the logistic regression coefficients. You also have moment conditions that correspond to mean balance on the included covariates in the weighted sample. There are two versions of CBPS: the just-identified version, which finds the logistic regression coefficients that only solve the balance moment conditions, and the over-identified version, which finds the logistic regression coefficients that attempt to solve both the logistic regression score equations and the balance moments conditions. Because in general it is impossible to solve both sets of conditions exactly with a single set of coefficients, the conditions are weighted in a specific way to manage the trade-off between achieving balance and maximizing the likelihood using generalized method of moments (GMM) estimation. The form of this weighting occurs with a weighting matrix, which is a function of the coefficients and can either be estimated once, in what is called the “two-step” estimator, or it can be continuously updated as the coefficients are estimated.&lt;/p&gt;
&lt;p&gt;There are versions of CBPS for the ATT, ATC, and ATE (logistic regression on its own &lt;a href=&#34;https://ngreifer.github.io/blog/logistic-regression-cbps-overlap-weights/#:~:text=This%20is%20why%20there%20is%20no%20%E2%80%9CATO%E2%80%9D%20option%20when%20using%20CBPS%3B%20the%20usual%20logistic%20regression%20propensity%20scores%20are%20covariate%20balancing%20propensity%20scores%20when%20targeting%20the%20ATO!&#34;&gt;is a CBPS for the ATO&lt;/a&gt;), as well as versions for multi-category and continuous treatments. The multi-category version replaces the logistic regression coefficients with coefficients in a multinomial logistic regression, and the continuous treatment version replaces the logistic regression coefficients with linear regression coefficients for the generalized propensity score &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fongCovariateBalancingPropensity2018&#34;&gt;Fong, Hazlett, and Imai 2018&lt;/a&gt;)&lt;/span&gt;. There is also a version for longitudinal treatments &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-imaiRobustEstimationInverse2015&#34;&gt;Imai and Ratkovic 2015&lt;/a&gt;)&lt;/span&gt;, but that has never been supported by &lt;code&gt;WeightIt&lt;/code&gt; so I won’t discuss it here.&lt;/p&gt;
&lt;p&gt;Previously, estimating the CBPS propensity scores and weights by specifying &lt;code&gt;method = &#34;cbps&#34;&lt;/code&gt; in &lt;code&gt;weightit()&lt;/code&gt; was done entirely in the &lt;code&gt;CBPS&lt;/code&gt; package, which was written by the developers of the method. The package is great and does what it claims to do, but its code is very hard to read and debug, it is not very customizable, and there are ways in which the package is limited or buggy. I decided to finally figure out what CBPS was for myself as part of a broader task of understanding M-estimation (discussed later), and in doing so I realized I had the skills to program my own CBPS, and so that’s what I did, taking some inspiration from the original package. This means you can use CBPS in &lt;code&gt;WeightIt&lt;/code&gt; without requiring &lt;code&gt;CBPS&lt;/code&gt; as a dependency, and there are some additional options not available in &lt;code&gt;CBPS&lt;/code&gt; that are now available to &lt;code&gt;WeightIt&lt;/code&gt; users. I’ll outline the main changes below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The default methods are now different, which is important when comparing the output of &lt;code&gt;weightit()&lt;/code&gt; to that of &lt;code&gt;CBPS::CBPS()&lt;/code&gt; or versions of &lt;code&gt;WeightIt&lt;/code&gt; prior to 1.0.0. Now, the default is to use the just-identified version of CBPS, whereas previously the default was the over-identified version. The just-identified version has several benefits: 1) it is faster (since the over-identified version has to fit the just-identified version first anyway), 2) it yields better balance on the means, and 3) it is compatible with using M-estimation to account for uncertainty in the weights. It is still possible to request the over-identified version by setting &lt;code&gt;over = TRUE&lt;/code&gt;, and whether the two-step estimator is to be used can be controlled by the &lt;code&gt;twostep&lt;/code&gt; argument, which has the same default behavior as previously (&lt;code&gt;TRUE&lt;/code&gt; by default, which is faster but potentially less accurate).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With multi-category treatments, the ATT and ATE can be targeted natively and the estimation supports any number of treatment groups. Previously, CBPS for the ATT and for more than 4 groups were only supported in an &lt;em&gt;ad hoc&lt;/em&gt; way because these options are not available in the &lt;code&gt;CBPS&lt;/code&gt; package. Now they are programmed directly in, so the results for them align with what one would expect based on theory. It should also be much faster to run because only one optimization is required (whereas previously multiple calls to &lt;code&gt;CBPS::CBPS()&lt;/code&gt; were required). There was also a bug in the over-identified version of the CBPS for multi-category treatments in the &lt;code&gt;CBPS&lt;/code&gt; package; it turns out this bug nudged the coefficients toward better balance, which is usually a good thing, but it meant they no longer lined up with what you would expect based on theory. This bug is not present in the new &lt;code&gt;WeightIt&lt;/code&gt; implementation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With continuous treatments, the weighting matrix for the over-identified CBPS now has a slightly different form from that in &lt;code&gt;CBPS&lt;/code&gt;. (Specifically, it does not integrate out the treatment, which is done for the other treatment types). This makes it more likely to converge. That said, one should probably always use the just-identified version.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Link functions other than logit are allowed for binary treatments. The CBPS as described in &lt;span class=&#34;citation&#34;&gt;Imai and Ratkovic (&lt;a href=&#34;#ref-imai2014&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; was general enough that any generalized linear model could be used, but focus was primarily on logistic regression because it is the most common model for binary outcomes. The &lt;code&gt;CBPS&lt;/code&gt; package only supports logistic regression. &lt;code&gt;WeightIt&lt;/code&gt; supports almost any link (in particular, logit, probit, complimentary log-log, cauchit, log, and identity). It turns out these are very easy to implement; the only differences are changing the inverse link used to compute the propensity scores and modifying the logistic regression score equations to include additional terms that are required for non-canonical links. In practice, logistic regression is all one should need, but it is nice to have these other options available in case you get better balance or precision using one of them, just as is the case when using propensity scores estimated from generalized linear models alone.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some things are missing from the original CBPS implementation, though. Any other arguments to &lt;code&gt;CBPS::CBPS()&lt;/code&gt; (e.g., &lt;code&gt;baseline.formula&lt;/code&gt; and &lt;code&gt;diff.formula&lt;/code&gt;) are no longer supported, and any auxiliary CBPS outputs, like the regression coefficients on the scale of the original predictors or the J-statistic for testing the over-identification conditions, are no longer returned. I doubt these are considered by most users of CBPS, so I didn’t want to spend time computing them and returning them when people just want the propensity scores and weights. If you do want these values, let me know and I’ll add them in, or just use &lt;code&gt;CBPS()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I’m really proud of this implementation of CBPS, as it represents a culmination of my recent learning about M-estimation and generalized method of moments. There is still plenty about CBPS theory I don’t understand, but I’m happy to reduce a dependency and take charge of new advancements on the method and its implementation. I give great acknowledgement to the original authors of all papers explaining CBPS and to the developers and maintainers of the CBPS package, especially Kosuke Imai and Christian Fong, who have long supported my work and appreciated its value. I also want to thank Syd Amerikaner on CrossValidated, who &lt;a href=&#34;https://stats.stackexchange.com/q/641993/116195&#34;&gt;provided&lt;/a&gt; a key insight that helped me program the over-identified estimators.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;new-weighting-method-inverse-probability-tilting-ipt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New weighting method: inverse probability tilting (IPT)&lt;/h2&gt;
&lt;p&gt;IPT is a method of estimating propensity score weights described by &lt;span class=&#34;citation&#34;&gt;Graham, De Xavier Pinto, and Egel (&lt;a href=&#34;#ref-grahamInverseProbabilityTilting2012&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;. It works almost identically to the just-identified CBPS: estimating equations corresponding to covariate balance are specified, and their roots, which correspond to coefficients in a generalized linear model for the propensity score, are found and used to compute the estimated propensity scores. For the ATT and ATC, the implementation is identical to the just-identified CBPS. For the ATE, IPT uses a slightly different method which estimates propensity scores for the treated and control units separately to ensure exact mean balance not just between the treatment groups but also between each group and the original sample &lt;span class=&#34;citation&#34;&gt;(so-called “three-way balance,” &lt;a href=&#34;#ref-chanGloballyEfficientNonparametric2016&#34;&gt;Chan, Yam, and Zhang 2016&lt;/a&gt;)&lt;/span&gt;. The original paper on IPT only described the method for “missing data problems”, which usually means estimating a counterfactual mean in an ATE framework, but &lt;span class=&#34;citation&#34;&gt;Sant’Anna and Zhao (&lt;a href=&#34;#ref-santannaDoublyRobustDifferenceindifferences2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; describe a version for the ATT that is just as easy to implement. For multi-category treatments, rather than fitting a multinomial logistic regression model like CBPS does, IPT does something like fitting a logistic regression model for each category. This again guarantees three-way balance, but differs from the CBPS implementation.&lt;/p&gt;
&lt;p&gt;Previously, IPT was not available in any package in R. I wrote a &lt;a href=&#34;https://gist.github.com/ngreifer/ef34ff2ef7b0ea8214fe2c6b5a080450&#34;&gt;gist&lt;/a&gt; to implement it before I really understood what it was, but now it is fully available in &lt;code&gt;WeightIt&lt;/code&gt; by setting &lt;code&gt;method = &#34;ipt&#34;&lt;/code&gt; in &lt;code&gt;weightit()&lt;/code&gt;. Its only dependency is the &lt;code&gt;rootSolve&lt;/code&gt; package, which quickly finds the roots of score equations. I could have just used &lt;code&gt;optim()&lt;/code&gt; like I did with CBPS and entropy balancing, but &lt;code&gt;rootSolve&lt;/code&gt; is quite a bit faster and often more reliable. As with CBPS, links besides the logit link are available, and all methods are compatible with M-estimation.&lt;/p&gt;
&lt;p&gt;The original authors proposed a specification test that involves testing whether the coefficients for the treated and control groups in the ATE differ from each other, and that is available in their Stata package. I didn’t put it into &lt;code&gt;WeightIt&lt;/code&gt;, but it wouldn’t be crazy to implement, so, again, let me know if you’re interested.&lt;/p&gt;
&lt;p&gt;In general, I would recommend IPT over CBPS and entropy balancing for binary and multi-category treatments due to its speed and some theoretical advantages. For all estimands, IPT is doubly robust, meaning if the link function is correctly specified or the outcome model is linear in the covariates, the estimate is consistent. For the ATE, entropy balancing doesn’t retain this property, and CBPS is less protected against incidental error in the presence of heavy effect modification. Entropy balancing is best suited for continuous treatments (IPT doesn’t even have a version for continuous treatments), as the limitations of the linear model used in CBPS prevent certain important balancing conditions from being satisfied.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-weighted-regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting Weighted Regression Models&lt;/h2&gt;
&lt;p&gt;Okay, this is probably the biggest of the three biggest updates. &lt;code&gt;WeightIt&lt;/code&gt; now has functionality to fit weighted outcome regression models in a way so that the uncertainty in the estimation of the weights is accounted for in the variance matrix of the estimated parameters. This is huge because all otherwise accessible methods for estimating the variance of regression parameters after weighting treat the weights as fixed, which in most cases yields inferences that are too conservative &lt;span class=&#34;citation&#34;&gt;(i.e., inappropriately too imprecise, &lt;a href=&#34;#ref-austinBootstrapVsAsymptotic2022&#34;&gt;Austin 2022&lt;/a&gt;)&lt;/span&gt;, and in some cases yields inferences that are anti-conservative &lt;span class=&#34;citation&#34;&gt;(inappropriately too precise, &lt;a href=&#34;#ref-reifeisVarianceTreatmentEffect2022&#34;&gt;Reifeis and Hudgens 2022&lt;/a&gt;)&lt;/span&gt;. This functionality is facilitated through the new function &lt;code&gt;glm_weightit()&lt;/code&gt; (and its wrapper, &lt;code&gt;lm_weightit()&lt;/code&gt;), which are mostly wrappers around &lt;code&gt;glm()&lt;/code&gt;, but estimate the variance matrix in a away that incorporates information about the estimation of the weights. This is done in a few ways, but the most notable is through M-estimation, which I briefly mentioned above.&lt;/p&gt;
&lt;p&gt;M-estimation is a way of estimating a system of estimating equations in which the parameters can have arbitrary relationships, including across models. In this case, the parameters of the weighted outcome model depend on the weights, which in turn depend on the parameters used to estimate the weights (e.g., the coefficients in the propensity score model). By combining the estimating equations for the weight models and the estimating equations for the outcome model, we can fully account for uncertainty in both models when computing the variance of the outcome model parameters. This means that when we use g-computation based on the outcome model, we get a treatment effect estimate, the standard error of which correctly accounts for uncertainty in estimation of the weights.&lt;/p&gt;
&lt;p&gt;The way this works is as follows: one uses &lt;code&gt;weightit()&lt;/code&gt; or &lt;code&gt;weightitMSM()&lt;/code&gt; to estimate the balancing weights, and then one fits a model using &lt;code&gt;glm_weightit()&lt;/code&gt; with the output of &lt;code&gt;weightit()&lt;/code&gt; or &lt;code&gt;weightitMSM()&lt;/code&gt; supplied to the &lt;code&gt;weightit&lt;/code&gt; argument. This fits a weighted regression model using &lt;code&gt;glm()&lt;/code&gt;, and, if available, estimates the variance matrix using M-estimation. Components in the &lt;code&gt;weightit&lt;/code&gt; object contain the required information to account for the uncertainty of the weights in estimation of the variance of the outcome model parameters.&lt;/p&gt;
&lt;p&gt;This is only available for methods that support M-estimation, which unfortunately does not include all of them. Only estimation of weights using GLM propensity scores, entropy balancing, IPT, and just-identified CBPS are supported. This is still a pretty good list. For the others, the default is to return the HC0 robust variance matrix, which treats the weights as fixed but is robust to heteroscedasticity or other misspecification of the outcome model likelihood (the is also true of the M-estimation variance).&lt;/p&gt;
&lt;p&gt;Currently, no other package in R lets you do this, despite many recommendations for more accurate standard error estimation in the literature &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-reifeisVarianceTreatmentEffect2022&#34;&gt;Reifeis and Hudgens 2022&lt;/a&gt;; &lt;a href=&#34;#ref-austinBootstrapVsAsymptotic2022&#34;&gt;Austin 2022&lt;/a&gt;; &lt;a href=&#34;#ref-luncefordStratificationWeightingPropensity2004&#34;&gt;Lunceford and Davidian 2004&lt;/a&gt;; &lt;a href=&#34;#ref-williamsonVarianceReductionRandomised2014&#34;&gt;Williamson, Forbes, and White 2014&lt;/a&gt;; &lt;a href=&#34;#ref-gabrielInverseProbabilityTreatment2024&#34;&gt;Gabriel et al. 2024&lt;/a&gt;)&lt;/span&gt;. You would have to program this manually yourself or use the highly context-specific code provided in these articles. Other packages, in particular &lt;code&gt;PSweight&lt;/code&gt;, do allow you to estimate treatment effects that account for estimation of the weights in the standard error of the treatment effect, but only for the weighted difference in means or augmented inverse probability weighting (AIPW) (i.e., not for weighted g-computation) and only for GLM-based propensity scores. Stata offers &lt;code&gt;teffects ipwra&lt;/code&gt;, which provides the corrected standard error estimates for weighted g-computation-based estimates of the treatment effect, but it also only supports GLM propensity scores and a few estimands.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;glm_weightit()&lt;/code&gt; supports another method of estimating uncertainty, and that is the bootstrap. Both the traditional bootstrap and the fractional weighted bootstrap &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-xuApplicationsFractionalRandomWeightBootstrap2020a&#34;&gt;Xu et al. 2020&lt;/a&gt;)&lt;/span&gt; are supported. These bootstrap the original data, estimate the weights, and estimate the weighted outcome model automatically. Previously, one would have to program the bootstrap themselves, which wasn’t really that hard, but now this is done in one single function call. The bootstrap confidence intervals computed are Wald-type intervals, though, which perform worse than more advanced types like the BCa interval. Still, though, because the estimators are asymptotically normal, the Wald-type intervals should work well. I recommend using the fractional weighted bootstrap when possible rather than the traditional bootstrap. The choice of variance estimator is controlled by the &lt;code&gt;vcov&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;glm_weightit()&lt;/code&gt; also supports estimation of multinomial logistic regression models by setting &lt;code&gt;family = &#34;multinomial&#34;&lt;/code&gt;. The outcome model can be flexible, e.g., including polynomials or splines, and its output is compatible with most regression post-processing functions, including all functions in &lt;code&gt;marginaleffects&lt;/code&gt;, which can be used to perform g-computation to estimate the treatment effect. &lt;code&gt;glm_weightit()&lt;/code&gt; also has support for clustered covariance matrices for all variance estimation methods. My hope is to add support for fitting Cox proportional hazards models as well, though I still barely understand them. Any advice on this would be greatly appreciated.&lt;/p&gt;
&lt;p&gt;For general theory on M-estimation, I recommend &lt;span class=&#34;citation&#34;&gt;Stefanski and Boos (&lt;a href=&#34;#ref-stefanskiCalculusMEstimation2002&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, and for its application to weighted regression models, I recommend &lt;span class=&#34;citation&#34;&gt;Gabriel et al. (&lt;a href=&#34;#ref-gabrielInverseProbabilityTreatment2024&#34;&gt;2024&lt;/a&gt;)&lt;/span&gt;. I also think looking at the &lt;code&gt;WeightIt&lt;/code&gt; source code can be illuminating; I think I did a good job of keeping it pretty simple and providing example code that anyone could use to program their own M-estimators. It’s really a lot easier than I thought and I’m glad I took the time to learn. I know it’s a fundamental technique for many biostatisticians and econometricians, but I always used to see it as a highly advanced and arcane method that I would never come to understand, until I came to understand it (3.5 years out from my PhD).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other updates&lt;/h2&gt;
&lt;p&gt;There are a few smaller updates to &lt;code&gt;WeightIt&lt;/code&gt; that I would feel bad about omitting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, the package documentation now uses &lt;code&gt;Roxygen&lt;/code&gt;. This doesn’t affect use but might make it easier for someone reading the source code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I added a new function, &lt;code&gt;calibrate()&lt;/code&gt;, which calibrates propensity scores using Platt scaling as recommended by &lt;span class=&#34;citation&#34;&gt;Gutman, Karavani, and Shimoni (&lt;a href=&#34;#ref-gutmanPropensityScoreModels2022&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;. This can slightly improve performance when a machine learning model is used to estimate the propensity score, but is unlikely to be useful otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A new argument, &lt;code&gt;quantile&lt;/code&gt;, can be supplied to &lt;code&gt;weightit()&lt;/code&gt; for methods that also accept &lt;code&gt;moments&lt;/code&gt; and &lt;code&gt;int&lt;/code&gt; which can be used to balance quantiles of the covariate distribution (e.g., the median, etc.) instead of just the means and means of moments. This is based on &lt;span class=&#34;citation&#34;&gt;Beręsewicz (&lt;a href=&#34;#ref-beresewiczSurveyCalibrationCausal2023&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;trim()&lt;/code&gt; now has a &lt;code&gt;drop&lt;/code&gt; argument; setting to &lt;code&gt;TRUE&lt;/code&gt; sets the weights of all trimmed units to 0 (effectively dropping or censroing them). Previously you could only “truncate” the weights, i.e., set all weights higher than a given quantile to the weight at that quantile, but weight trimming (dropping units with extreme weights) has an extensive history as well &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-thoemmesPrimerInverseProbability2016&#34;&gt;Thoemmes and Ong 2016&lt;/a&gt;; &lt;a href=&#34;#ref-crumpDealingLimitedOverlap2009&#34;&gt;Crump et al. 2009&lt;/a&gt;)&lt;/span&gt;. In my opinion, it is always better to change your estimand prospectively (e.g., to the ATO) than to censor units, but the option is available.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I made a small change to how g-computation is done; following &lt;span class=&#34;citation&#34;&gt;Gabriel et al. (&lt;a href=&#34;#ref-gabrielInverseProbabilityTreatment2024&#34;&gt;2024&lt;/a&gt;)&lt;/span&gt;, I no longer recommend incorporating the weights in the g-computation step beyond including them in the outcome model. An exception is made when targeting an estimand other than the ATT, ATC, or ATE or when using sampling weights. In practice, it will not make a big difference whether the weights are included or not for the ATE (which is the only estimand affected by this change), but this brings recommendations in line with those in the literature and consistent with &lt;code&gt;teffects ipwra&lt;/code&gt; in Stata.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For multi-category treatments, the default multinomial logistic regression propensity scores (i.e., with &lt;code&gt;method = &#34;glm&#34;&lt;/code&gt;) require no dependencies. Other ways of estimating the propensity scores are possibly by specifying the &lt;code&gt;multi.method&lt;/code&gt; argument.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;Thank you so much for reading and checking out &lt;code&gt;WeightIt&lt;/code&gt;! I hope these new features improve your research or lead to new discoveries. Please let me know if any of my packages have helped you in your work; it makes a big difference to know that my efforts help people.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-austinBootstrapVsAsymptotic2022&#34; class=&#34;csl-entry&#34;&gt;
Austin, Peter C. 2022. &lt;span&gt;“Bootstrap Vs Asymptotic Variance Estimation When Using Propensity Score Weighting with Continuous and Binary Outcomes.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 41 (22): 4426–43. &lt;a href=&#34;https://doi.org/10.1002/sim.9519&#34;&gt;https://doi.org/10.1002/sim.9519&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-beresewiczSurveyCalibrationCausal2023&#34; class=&#34;csl-entry&#34;&gt;
Beręsewicz, Maciej. 2023. &lt;span&gt;“Survey Calibration for Causal Inference: A Simple Method to Balance Covariate Distributions.”&lt;/span&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2310.11969&#34;&gt;https://doi.org/10.48550/arXiv.2310.11969&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-chanGloballyEfficientNonparametric2016&#34; class=&#34;csl-entry&#34;&gt;
Chan, Kwun Chuen Gary, Sheung Chi Phillip Yam, and Zheng Zhang. 2016. &lt;span&gt;“Globally Efficient Non-Parametric Inference of Average Treatment Effects by Empirical Balancing Calibration Weighting.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 78 (3): 673–700. &lt;a href=&#34;https://doi.org/10.1111/rssb.12129&#34;&gt;https://doi.org/10.1111/rssb.12129&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-crumpDealingLimitedOverlap2009&#34; class=&#34;csl-entry&#34;&gt;
Crump, R. K., V. J. Hotz, G. W. Imbens, and O. A. Mitnik. 2009. &lt;span&gt;“Dealing with Limited Overlap in Estimation of Average Treatment Effects.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 96 (1): 187–99. &lt;a href=&#34;https://doi.org/10.1093/biomet/asn055&#34;&gt;https://doi.org/10.1093/biomet/asn055&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-fongCovariateBalancingPropensity2018&#34; class=&#34;csl-entry&#34;&gt;
Fong, Christian, Chad Hazlett, and Kosuke Imai. 2018. &lt;span&gt;“Covariate Balancing Propensity Score for a Continuous Treatment: Application to the Efficacy of Political Advertisements.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 12 (1): 156–77. &lt;a href=&#34;https://doi.org/10.1214/17-AOAS1101&#34;&gt;https://doi.org/10.1214/17-AOAS1101&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gabrielInverseProbabilityTreatment2024&#34; class=&#34;csl-entry&#34;&gt;
Gabriel, Erin E., Michael C. Sachs, Torben Martinussen, Ingeborg Waernbaum, Els Goetghebeur, Stijn Vansteelandt, and Arvid Sjölander. 2024. &lt;span&gt;“Inverse Probability of Treatment Weighting with Generalized Linear Outcome Models for Doubly Robust Estimation.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 43 (3): 534–47. &lt;a href=&#34;https://doi.org/10.1002/sim.9969&#34;&gt;https://doi.org/10.1002/sim.9969&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-grahamInverseProbabilityTilting2012&#34; class=&#34;csl-entry&#34;&gt;
Graham, Bryan S., Cristine Campos De Xavier Pinto, and Daniel Egel. 2012. &lt;span&gt;“Inverse Probability Tilting for Moment Condition Models with Missing Data.”&lt;/span&gt; &lt;em&gt;The Review of Economic Studies&lt;/em&gt; 79 (3): 1053–79. &lt;a href=&#34;https://doi.org/10.1093/restud/rdr047&#34;&gt;https://doi.org/10.1093/restud/rdr047&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-gutmanPropensityScoreModels2022&#34; class=&#34;csl-entry&#34;&gt;
Gutman, Rom, Ehud Karavani, and Yishai Shimoni. 2022. &lt;span&gt;“Propensity Score Models Are Better When Post-Calibrated.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-imai2014&#34; class=&#34;csl-entry&#34;&gt;
Imai, Kosuke, and Marc Ratkovic. 2014. &lt;span&gt;“Covariate Balancing Propensity Score.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (1): 243263. &lt;a href=&#34;https://doi.org/10.1111/rssb.12027&#34;&gt;https://doi.org/10.1111/rssb.12027&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-imaiRobustEstimationInverse2015&#34; class=&#34;csl-entry&#34;&gt;
———. 2015. &lt;span&gt;“Robust Estimation of Inverse Probability Weights for Marginal Structural Models.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 110 (511): 1013–23. &lt;a href=&#34;https://doi.org/10.1080/01621459.2014.956872&#34;&gt;https://doi.org/10.1080/01621459.2014.956872&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-luncefordStratificationWeightingPropensity2004&#34; class=&#34;csl-entry&#34;&gt;
Lunceford, Jared K., and Marie Davidian. 2004. &lt;span&gt;“Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects: A Comparative Study.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 23 (19): 29372960. &lt;a href=&#34;https://doi.org/10.1002/sim.1903&#34;&gt;https://doi.org/10.1002/sim.1903&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-reifeisVarianceTreatmentEffect2022&#34; class=&#34;csl-entry&#34;&gt;
Reifeis, Sarah A, and Michael G Hudgens. 2022. &lt;span&gt;“On Variance of the Treatment Effect in the Treated When Estimated by Inverse Probability Weighting.”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 191 (6): 1092–97. &lt;a href=&#34;https://doi.org/10.1093/aje/kwac014&#34;&gt;https://doi.org/10.1093/aje/kwac014&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-santannaDoublyRobustDifferenceindifferences2020&#34; class=&#34;csl-entry&#34;&gt;
Sant’Anna, Pedro H. C., and Jun Zhao. 2020. &lt;span&gt;“Doubly Robust Difference-in-Differences Estimators.”&lt;/span&gt; &lt;em&gt;Journal of Econometrics&lt;/em&gt; 219 (1): 101–22. &lt;a href=&#34;https://doi.org/10.1016/j.jeconom.2020.06.003&#34;&gt;https://doi.org/10.1016/j.jeconom.2020.06.003&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stefanskiCalculusMEstimation2002&#34; class=&#34;csl-entry&#34;&gt;
Stefanski, Leonard A., and Dennis D. Boos. 2002. &lt;span&gt;“The Calculus of m-Estimation.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt; 56 (1): 29–38. &lt;a href=&#34;https://doi.org/10.1198/000313002753631330&#34;&gt;https://doi.org/10.1198/000313002753631330&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-thoemmesPrimerInverseProbability2016&#34; class=&#34;csl-entry&#34;&gt;
Thoemmes, Felix J., and Anthony D. Ong. 2016. &lt;span&gt;“A Primer on Inverse Probability of Treatment Weighting and Marginal Structural Models.”&lt;/span&gt; &lt;em&gt;Emerging Adulthood&lt;/em&gt; 4 (1): 40–59. &lt;a href=&#34;https://doi.org/10.1177/2167696815621645&#34;&gt;https://doi.org/10.1177/2167696815621645&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-williamsonVarianceReductionRandomised2014&#34; class=&#34;csl-entry&#34;&gt;
Williamson, Elizabeth J., Andrew B. Forbes, and Ian R. White. 2014. &lt;span&gt;“Variance Reduction in Randomised Trials by Inverse Probability Weighting Using the Propensity Score.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 33 (5): 721–37. &lt;a href=&#34;https://doi.org/10.1002/sim.5991&#34;&gt;https://doi.org/10.1002/sim.5991&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-xuApplicationsFractionalRandomWeightBootstrap2020a&#34; class=&#34;csl-entry&#34;&gt;
Xu, Li, Chris Gotwalt, Yili Hong, Caleb B. King, and William Q. Meeker. 2020. &lt;span&gt;“Applications of the Fractional-Random-Weight Bootstrap.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt; 74 (4): 345–58. &lt;a href=&#34;https://doi.org/10.1080/00031305.2020.1731599&#34;&gt;https://doi.org/10.1080/00031305.2020.1731599&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating Treatment Effects After Weighting with Multiply Imputed Data</title>
      <link>https://ngreifer.github.io/blog/treatment-effects-mi/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/treatment-effects-mi/</guid>
      <description>


&lt;p&gt;Multiply imputed data always makes things a little harder. Essentially, you have to perform each step of the analysis in each imputed dataset and then combine the results together in a special way. For basic regression analysis, the &lt;code&gt;mice&lt;/code&gt; package makes fitting models and combining estimates simple. But when we want to do propensity score matching or weighting before fitting our regression models, and when the quantity we want to estimate is not just a coefficient in a regression model, things get a bit harder.&lt;/p&gt;
&lt;p&gt;For doing matching or weighting in multiply imputed data, the R package &lt;code&gt;{MatchThem}&lt;/code&gt; does the job. It essentially provides wrappers for &lt;code&gt;MatchIt::matchit()&lt;/code&gt; and &lt;code&gt;WeightIt::weightit()&lt;/code&gt; for multiply imputed data. It extends &lt;code&gt;{mice}&lt;/code&gt;’s functionality for fitting regression models in multiply imputed data by automatically incorporating the matched or weighted structure into the estimation of the outcome models. It uses &lt;code&gt;mice::pool()&lt;/code&gt; to pool estimates across multiply imputed data.&lt;/p&gt;
&lt;p&gt;But for estimating treatment effects, it’s often not as simple as using a regression coefficient. If we include covariates in our outcome model but want a marginal effect, we need to use an average marginal effects procedure (i.e., g-computation) to compute it within each imputed dataset, and then combine the results afterward. The &lt;code&gt;{marginaleffects}&lt;/code&gt; package provides a wonderful interface for performing g-computation, but for multiply imputed data, it can require some programming by the analyst. In this guide, I’ll show you how to do that programming to combine treatment effect estimates across multiple imputed datasets.&lt;/p&gt;
&lt;p&gt;An alternative to using &lt;code&gt;{marginaleffects}&lt;/code&gt; is to use the &lt;code&gt;{clarify}&lt;/code&gt; package. &lt;code&gt;{clarify}&lt;/code&gt; can also be used to perform g-computation, but it uses simulation-based inference to compute the uncertainty bounds for the estimate. An advantage of simulation-based inference for multiply imputed data is that combining estimates across imputed datasets is much more straightforward. In this guide, I’ll also show you how to use &lt;code&gt;{clarify}&lt;/code&gt; to combine treatment effect estimates across imputed datasets.&lt;/p&gt;
&lt;div id=&#34;packages-well-need&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages we’ll need&lt;/h3&gt;
&lt;p&gt;We will need the following packages for this demonstration: &lt;code&gt;cobalt&lt;/code&gt;, &lt;code&gt;mice&lt;/code&gt;, &lt;code&gt;MatchThem&lt;/code&gt;, &lt;code&gt;WeightIt&lt;/code&gt;, &lt;code&gt;marginaleffects&lt;/code&gt;, and &lt;code&gt;clarify&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data&lt;/h3&gt;
&lt;p&gt;As usual, we’ll be using a version of the &lt;code&gt;lalonde&lt;/code&gt; dataset. Here will use the &lt;code&gt;lalonde_mis&lt;/code&gt; dataset in &lt;code&gt;{cobalt}&lt;/code&gt;, which has missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;lalonde_mis&amp;quot;, package = &amp;quot;cobalt&amp;quot;)

summary(lalonde_mis)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      treat             age             educ           race        married          nodegree           re74              re75              re78        
##  Min.   :0.0000   Min.   :16.00   Min.   : 0.00   black :243   Min.   :0.0000   Min.   :0.0000   Min.   :    0.0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:0.0000   1st Qu.:20.00   1st Qu.: 9.00   hispan: 72   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:    0.0   1st Qu.:    0.0   1st Qu.:  238.3  
##  Median :0.0000   Median :25.00   Median :11.00   white :299   Median :0.0000   Median :1.0000   Median :  984.5   Median :  585.4   Median : 4759.0  
##  Mean   :0.3013   Mean   :27.36   Mean   :10.27                Mean   :0.4158   Mean   :0.6303   Mean   : 4420.2   Mean   : 2170.3   Mean   : 6792.8  
##  3rd Qu.:1.0000   3rd Qu.:32.00   3rd Qu.:12.00                3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 7626.9   3rd Qu.: 3202.0   3rd Qu.:10893.6  
##  Max.   :1.0000   Max.   :55.00   Max.   :18.00                Max.   :1.0000   Max.   :1.0000   Max.   :35040.1   Max.   :25142.2   Max.   :60307.9  
##                                                                NA&amp;#39;s   :20                        NA&amp;#39;s   :40        NA&amp;#39;s   :39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see there are some missing values in &lt;code&gt;married&lt;/code&gt;, &lt;code&gt;re74&lt;/code&gt;, and &lt;code&gt;re75&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;imputing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Imputing the data&lt;/h3&gt;
&lt;p&gt;Here, we’ll use &lt;code&gt;{mice}&lt;/code&gt; to impute the data. Although typically something like 20 imputation is sufficient, for the method &lt;code&gt;{clarify}&lt;/code&gt; uses, it needs way more, so we’ll use 50. We’ll use the default settings, but you should tailor the imputation to fit the needs of your dataset. (I always like to use a machine learning method for my imputations). We’ll also set a seed to ensure replicability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;mice&amp;quot;)
set.seed(12345)
imp &amp;lt;- mice(lalonde_mis, m = 50, printFlag = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mice()&lt;/code&gt; returns a &lt;code&gt;mids&lt;/code&gt; object, which contains the imputed datasets. Although we could extract the datasets using &lt;code&gt;complete()&lt;/code&gt;, we’ll supply this object directly to our function for estimating the propensity score weights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighting-the-imputed-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weighting the imputed data&lt;/h3&gt;
&lt;p&gt;We’ll use &lt;code&gt;MatchThem::weightthem()&lt;/code&gt; to estimate propensity score weights in the imputed datasets. We could also use &lt;code&gt;MatchThem::matchthem()&lt;/code&gt; to do matching; the process is basically identical&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Here we’ll use logistic regression (🤢) to estimate ATT weights to keep things quick and simple.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;MatchThem&amp;quot;)
w.imp &amp;lt;- weightthem(treat ~ age + educ + race + married + nodegree +
                      re74 + re75, data = imp, method = &amp;quot;ps&amp;quot;,
                    estimand = &amp;quot;ATT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s assess balance using &lt;code&gt;{cobalt}&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;cobalt&amp;quot;)
bal.tab(w.imp, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;), abs = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance summary across all imputations
##                 Type Mean.Diff.Adj Max.Diff.Adj Mean.KS.Adj Max.KS.Adj
## prop.score  Distance        0.0235       0.0379      0.1166     0.1327
## age          Contin.        0.1120       0.1343      0.3053     0.3146
## educ         Contin.        0.0352       0.0485      0.0369     0.0412
## race_black    Binary        0.0024       0.0036      0.0024     0.0036
## race_hispan   Binary        0.0003       0.0007      0.0003     0.0007
## race_white    Binary        0.0022       0.0030      0.0022     0.0030
## married       Binary        0.0168       0.0236      0.0168     0.0236
## nodegree      Binary        0.0191       0.0250      0.0191     0.0250
## re74         Contin.        0.0097       0.0281      0.2027     0.2261
## re75         Contin.        0.0075       0.0286      0.1388     0.1648
## 
## Average effective sample sizes across imputations
##                 0   1
## Unadjusted 429.   185
## Adjusted   100.19 185&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Balance could be a bit better on &lt;code&gt;age&lt;/code&gt;, but we’re going to move on because we have things to do.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-outcome-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitting the outcome models&lt;/h3&gt;
&lt;p&gt;Our next step is to fit the outcome model in each imputed dataset. Here, our outcome will be &lt;code&gt;re78 == 0&lt;/code&gt;, i.e., whether a unit’s earnings in 1978 were 0. Ideally, treatment reduces this risk. Although our estimand will be a risk ratio, because we’re doing g-computation, we can fit a model for the outcome that actually makes sense rather than choosing one based on the convenient interpretation of its coefficients. So, we’ll fit a probit outcome model to really hit home that we need a post-estimation method to estimate our quantity of interest and can’t rely on our model coefficients.&lt;/p&gt;
&lt;p&gt;Although &lt;code&gt;{MatchThem}&lt;/code&gt; has functionality for fitting models to the imputed datasets that incorporate the weights, for our purposes, it is better to extract the imputed datasets and fit each model manually in a loop. We’ll use &lt;code&gt;glm()&lt;/code&gt; to do so, though the &lt;code&gt;{MatchThem}&lt;/code&gt; and &lt;code&gt;{WeightIt}&lt;/code&gt; documentation may recommend &lt;code&gt;survey::svyglm()&lt;/code&gt; because it correctly computes the robust standard errors. We’ll do that later using &lt;code&gt;{marginaleffects}&lt;/code&gt; and &lt;code&gt;{clarify}&lt;/code&gt; functions so it’s okay that we don’t do it now. We’ll use a quasi-binomial model because we have weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits &amp;lt;- lapply(complete(w.imp, &amp;quot;all&amp;quot;), function(d) {
  glm(I(re78 == 0) ~ treat + age + educ + married + race +
        nodegree + re74 + re75, data = d,
      weights = weights, family = quasibinomial(&amp;quot;probit&amp;quot;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to interpret the pooled coefficients from our outcome model (and we had included correct estimation of the standard errors, which we didn’t here), we could use &lt;code&gt;pool(fits) |&amp;gt; summary()&lt;/code&gt; to get them. But none of that is true here so we’ll move on and save the pooling till after we estimate the quantity of interest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-marginaleffects-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;{marginaleffects}&lt;/code&gt; workflow&lt;/h2&gt;
&lt;p&gt;Now we have our list of models. Our next step is to estimate the ATT risk ratio in each one (with the correct standard error) and pool the results. If the only quantity we want is the treatment effect, this is easy. We can use &lt;code&gt;marginaleffects::avg_comparisons()&lt;/code&gt; on each model and then use &lt;code&gt;mice::pool()&lt;/code&gt; to pool the results. In our call to &lt;code&gt;avg_comparisons()&lt;/code&gt;, we need to subset the data used to fit each model to just the treated units and supply this to &lt;code&gt;newdata&lt;/code&gt;, supply the name of the variable containing the weights to &lt;code&gt;wts&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, supply the robust standard error type (HC3) to &lt;code&gt;vcov&lt;/code&gt;, and specify that we want the log risk ratio of the average estimated potential outcomes by supplying &lt;code&gt;&#34;lnratioavg&#34;&lt;/code&gt; to &lt;code&gt;transform_pre&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;marginaleffects&amp;quot;)
comp.imp &amp;lt;- lapply(fits, function(fit) {
  avg_comparisons(fit, newdata = subset(fit$data, treat == 1),
                  variables = &amp;quot;treat&amp;quot;, wts = &amp;quot;weights&amp;quot;, vcov = &amp;quot;HC3&amp;quot;,
                  transform_pre = &amp;quot;lnratioavg&amp;quot;)
})

pooled.comp &amp;lt;- mice::pool(comp.imp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can use &lt;code&gt;summary()&lt;/code&gt; on the resulting object, adding the arguments &lt;code&gt;conf.int = TRUE&lt;/code&gt; to request confidence intervals and &lt;code&gt;exponentiate = TRUE&lt;/code&gt; to get the risk ratio from the log risk ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pooled.comp, conf.int = TRUE,
        exponentiate = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    term              contrast  estimate std.error  statistic       df  p.value    2.5 %   97.5 %
## 1 treat ln(mean(1) / mean(0)) 0.9321569 0.2097534 -0.3349366 610.5055 0.737788 0.617436 1.407298&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find a risk ratio of approximately 0.932, 95% CI: [0.617, 1.407], indicating that in our sample, the risk of having zero earnings in 1978 decreased slightly for those who received treatment, but we don’t have strong evidence for such an effect in the population.&lt;/p&gt;
&lt;p&gt;Although this is nice and simple, things get a bit more complicated when we want to estimate multiple comparisons at the same time, estimate the marginal risks, or perform a more complex analysis. Additional programming is required to make &lt;code&gt;mice::pool()&lt;/code&gt; compatible with these more complex quantities. We’ll demonstrate how to hack &lt;code&gt;{marginaleffects}&lt;/code&gt; to make it work using the instructions in the &lt;code&gt;{marginaleffects}&lt;/code&gt; &lt;a href=&#34;https://vincentarelbundock.github.io/marginaleffects/articles/multiple_imputation.html&#34;&gt;vignette on multiple imputation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We’ll be using &lt;code&gt;avg_predictions()&lt;/code&gt; on each model to compute the marginal risks under each treatment level, which uses a similar syntax to &lt;code&gt;comparisons()&lt;/code&gt;. The challenge comes in that &lt;code&gt;avg_predictions()&lt;/code&gt; produces two rows of output (one for each treatment level), which are not correctly distinguished by &lt;code&gt;mice::pool()&lt;/code&gt;. So, we’ll have to create a new custom class and write a new &lt;code&gt;tidy()&lt;/code&gt; method for our class.&lt;/p&gt;
&lt;p&gt;First, we’ll generate our marginal risks and assign the output our new class, which is arbitrary but which I will call &lt;code&gt;&#34;pred_imp_custom&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.imp &amp;lt;- lapply(fits, function(fit) {
  out &amp;lt;- avg_predictions(fit, newdata = subset(fit$data, treat == 1),
                         variables = &amp;quot;treat&amp;quot;, wts = &amp;quot;weights&amp;quot;,
                         vcov = &amp;quot;HC3&amp;quot;, by = &amp;quot;treat&amp;quot;)
  
  # the next line assigns our custom class
  class(out) &amp;lt;- c(&amp;quot;pred_imp_custom&amp;quot;, class(out))
  return(out)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll write our new &lt;code&gt;tidy()&lt;/code&gt; method. (Make sure to replace &lt;code&gt;treat&lt;/code&gt; everywhere you see it with the name of your treatment variable.) We won’t actually be using this function at all; it is called internally by &lt;code&gt;mice::pool()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy.pred_imp_custom &amp;lt;- function(x, ...) {
    out &amp;lt;- marginaleffects:::tidy.predictions(x, ...)
    out$term &amp;lt;- paste(&amp;quot;treat =&amp;quot;, out$treat)
    return(out)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can use &lt;code&gt;mice::pool()&lt;/code&gt; and &lt;code&gt;summary()&lt;/code&gt; to get our marginal risks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mice::pool(pred.imp) |&amp;gt; summary(conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        term  estimate  std.error statistic       df      p.value     2.5 %    97.5 %
## 1 treat = 0 0.2607090 0.04264062  6.114100 609.4350 1.734761e-09 0.1769686 0.3444494
## 2 treat = 1 0.2430092 0.03197686  7.599534 611.9484 1.120645e-13 0.1802115 0.3058069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking the ratio of these risks gives us the risk ratio we computed above.&lt;/p&gt;
&lt;p&gt;Note that you have to customize the &lt;code&gt;tidy()&lt;/code&gt; method in a slightly different way when you are estimating treatment effects in subgroups. I’ll leave that as an exercise to the reader, or you can hire me to do it for you :)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-clarify-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;{clarify}&lt;/code&gt; workflow&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;{clarify}&lt;/code&gt; workflow for multiply imputed data is very similar to its workflow for regular data. How simulation-based inference works broadly is that sets of parameters are drawn from a distribution after fitting the model; this distribution is often assumed to be multivariate normal with the mean vector equal to the estimated coefficients and the covariance equal to the asymptotic covariance matrix of the coefficients. Many (e.g., 1000) sets of coefficients are drawn, and a quantity of interest is computed using each set, forming a “posterior” distribution of the quantity of interest. This posterior is then used for inference: its standard deviation can be used as the quantity’s standard error, and its quantiles can be used as confidence intervals. For more information on this methodology, see the &lt;code&gt;{clarify}&lt;/code&gt; &lt;a href=&#34;https://iqss.github.io/clarify/&#34;&gt;website&lt;/a&gt; and its references.&lt;/p&gt;
&lt;p&gt;With multiply imputed data, this process is done for the model fit to each imputed dataset, and then the distributions of the quantities of interest are simply combined to form a single distribution, which is used for inference. In Bayesian terms, this would be called “mixing draws”. The variance of this mixture distribution approaches the variance of the estimate computed using Rubin’s rules when the number of imputations is high.&lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;{clarify}&lt;/code&gt;, we supply the list of fitted models to &lt;code&gt;clarify::misim()&lt;/code&gt;, which draws the coefficients from their implied distributions from each model. We also need to specify the method for computing the covariance matrix (here, using the same HC3 robust covariance we used with &lt;code&gt;{marginaleffects}&lt;/code&gt; to account for the weights). We will only request 200 replications per fitted model since we have 50 imputations, which gives us 10,000 replicates (likely more than enough for stable inference).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;clarify&amp;quot;)

sim.imp &amp;lt;- misim(fits, n = 200, vcov = &amp;quot;HC3&amp;quot;)
sim.imp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A `clarify_misim` object
##  - 10 coefficients, 50 imputations with 200 simulated values each
##  - sampled distributions: multivariate t(604)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note: because we used a quasi-binomial model, a scaled t-distribution was used to draw the coefficients. In practice this will give similar draws to a normal distribution.)&lt;/p&gt;
&lt;p&gt;The output of &lt;code&gt;misim()&lt;/code&gt; is then fed to a function for computing the quantity of interest in each draw; here, we’ll be using &lt;code&gt;clarify::sim_ame()&lt;/code&gt;, which is appropriate for computing marginal risks in a subset of the data (i.e., the ATT risk ratio). We supply the treatment variable to &lt;code&gt;var&lt;/code&gt; and subset the data to just the treated units using &lt;code&gt;subset&lt;/code&gt; to request the ATT. Although we can use the &lt;code&gt;contrast&lt;/code&gt; argument to request the (log) risk ratio, we can compute that afterward quickly from the marginal risks. (Using &lt;code&gt;cl = 3&lt;/code&gt; uses parallel computing with 3 cores but only if you are on a Mac. See the &lt;code&gt;sim_ame()&lt;/code&gt; documentation for more information on how to use the &lt;code&gt;cl&lt;/code&gt; argument.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.att &amp;lt;- sim_ame(sim.imp, var = &amp;quot;treat&amp;quot;,
                   subset = treat == 1, cl = 3,
                   verbose = FALSE)
sim.att&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A `clarify_est` object (from `sim_ame()`)
##  - Average marginal effect of `treat`
##  - 10000 simulated values
##  - 2 quantities estimated:                  
##  E[Y(0)] 0.2605322
##  E[Y(1)] 0.2428401&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute the risk ratio, we can use &lt;code&gt;transform()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.att &amp;lt;- transform(sim.att, RR = `E[Y(1)]`/`E[Y(0)]`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can compute out confidence intervals and p-values around the estimated marginal risks and risk ratio using &lt;code&gt;summary()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sim.att, null = c(RR = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Estimate 2.5 % 97.5 % P-value
## E[Y(0)]    0.261 0.187  0.354       .
## E[Y(1)]    0.243 0.188  0.313       .
## RR         0.932 0.630  1.421    0.76&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we find a risk ratio of approximately 0.932, 95% CI: [0.63, 1.421]. The estimates, confidence intervals, and p-values we get from the two methods line up well.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;{clarify}&lt;/code&gt; uses quantile-based confidence intervals and computes the p-values by inverting them (i.e., finding the largest confidence level that yields an interval that excludes the null value and computing the p-value as one minus that level). Wald confidence intervals and p-values can also be request by setting &lt;code&gt;method = &#34;wald&#34;&lt;/code&gt; in the call to &lt;code&gt;summary()&lt;/code&gt;, but these are only recommended if the quantity has a normal distribution (which the risk ratio does not).&lt;/p&gt;
&lt;div id=&#34;explaining-differences-between-the-approaches&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explaining differences between the approaches&lt;/h3&gt;
&lt;p&gt;Both the delta method- and simulation-based inference approaches are valid, but sometimes you will get results that disagree. The estimates of the quantities of interest may disagree because of how &lt;code&gt;mice::pool()&lt;/code&gt; and &lt;code&gt;clarify::sim_ame()&lt;/code&gt; combine estimates across imputations.&lt;/p&gt;
&lt;p&gt;Rubin’s rules involve simply taking the mean of the estimates across imputations. This works well when the quantity is collapsible, linear, or has a symmetric (ideally normal) distribution. If the quantity of interest is none of those but can be transformed from a quantity that does have those properties, Rubin’s rules can be apply to this intermediate quantity before transforming the estimate to get the final results. This is exactly what we did in the &lt;code&gt;{marginaleffects}&lt;/code&gt; workflow when we computed the log risk ratio before pooling and then exponentiating the pooled log risk ratio to arrive at the risk ratio. If we had gone straight into pooling the risk ratio, the resulting estimate might not have been consistent.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{clarify}&lt;/code&gt; works by first using Rubin’s pooling rules on the model coefficients, which we assume to be normally distributed, and then computing the quantity of interest in each imputed dataset using draws from the pooled coefficients. A benefit of this strategy is that we don’t have to wonder whether the quantity of interest satisfies the above properties. The resulting estimates will be consistent because no pooling is done on them; the pooling happens only in the first step.&lt;/p&gt;
&lt;p&gt;Confidence intervals may differ slightly between the two methods, and this could be due to two reasons: 1) the delta method and simulation-based inferences naturally compute confidence intervals in different ways, with the delta method using a first-order Taylor series approximation and assuming normality of the quantity of interest, and simulation-based inference using simulation to generate a “posterior” for the quantity of interest and using its quantiles as the interval; and 2) simulation-based inference requires many imputations for the variance of the posterior to equal the variance of the Rubin’s rules pooled estimate. More imputations is always better for both methods, so do as many as you can.&lt;/p&gt;
&lt;p&gt;How should you choose between the delta method and simulation-based inference? Use whichever will get you published, of course! (Just kidding.) Use the one you find most trustworthy, that your audience will find the most trustworthy, and that balances the assumptions you are willing to make with the desired precision of the estimate. You might also use the one that seems more natural to you, either conceptually or in terms of usability. Frankly, I find &lt;code&gt;{clarify}&lt;/code&gt; to be easier to use when the quantity of interest is more complicated than a single comparison (e.g., for subgroup analysis or for computing average marginal risks), but &lt;code&gt;{marginaleffects}&lt;/code&gt; can be faster, doesn’t rely on a stochastic process, and is better-backed by statistical theory. Confirming you get similar results with both methods is always a good idea, and the plotting diagnostics in &lt;code&gt;{clarify}&lt;/code&gt; can be used to determine whether any difference might be due to the failure of the delta method due to violation of one of its assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The key differences is that pair membership needs to be accounted for in estimation of the variance of the outcome model coefficients; this is usually as simply as specifying &lt;code&gt;vcov = ~subclass&lt;/code&gt; to functions in &lt;code&gt;{marginaleffects}&lt;/code&gt; or &lt;code&gt;{clarify}&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This actually isn’t necessary for the ATT but it’s walys good practice.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Note: we need the log risk ratio because Rubin’s pooling rules don’t apply to the risk ratio but do to the log risk ratio. We will exponentiate the log risk ratio and its confidence interval after pooling.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
