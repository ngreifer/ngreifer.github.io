<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Noah Greifer</title>
    <link>https://ngreifer.github.io/</link>
      <atom:link href="https://ngreifer.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Noah Greifer</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 12 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ngreifer.github.io/media/sharing.jpg</url>
      <title>Noah Greifer</title>
      <link>https://ngreifer.github.io/</link>
    </image>
    
    <item>
      <title>Musings on Logistic Regression, CBPS, and Overlap Weights</title>
      <link>https://ngreifer.github.io/blog/logistic-regression-cbps-overlap-weights/</link>
      <pubDate>Tue, 12 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/logistic-regression-cbps-overlap-weights/</guid>
      <description>


&lt;p&gt;As I’ve been studying M-estimation and the covariate balancing propensity score (CBPS) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-imaiCovariateBalancingPropensity2014&#34;&gt;Imai and Ratkovic 2014&lt;/a&gt;)&lt;/span&gt;, I’ve been noticing some interesting connections between these methods and want to share them with you.&lt;/p&gt;
&lt;div id=&#34;logistic-regression-and-m-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic Regression and M-estimation&lt;/h2&gt;
&lt;p&gt;First, what is logistic regression? I’ll discuss that in more detail in another post, but briefly it’s a way of modeling the relationship between &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; predictors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; (which include an intercept) and an outcome &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (yes, I’ll use &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; for the outcome, and I’ll also use it to mean the treatment in an observational study since the context is fitting a model for the treatment to estimate propensity scores), where that relationship is specified to be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p_i = P(A_i = 1|\mathbf{X}_i)=\text{expit}(\mathbf{X}_i\beta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\text{expit}(z)=1/(1 + e^{-z})\)&lt;/span&gt;. Expit is also known as inverse logit, i.e., as &lt;span class=&#34;math inline&#34;&gt;\(\text{logit}^{-1}(z)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\text{logit}(p)=\ln\frac{p}{1-p}\)&lt;/span&gt;. We estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; usually using maximum likelihood, but here I’m going to talk about M-estimation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-stefanskiCalculusMEstimation2002&#34;&gt;Stefanski and Boos 2002&lt;/a&gt;; &lt;a href=&#34;#ref-rossMestimationCommonEpidemiological2024&#34;&gt;Ross et al. 2024&lt;/a&gt;)&lt;/span&gt;, in which we specify a “stack” of estimating equations and find the values of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; such that all the estimating equations are equal to 0. That is, we specify the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; estimating equations&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{N}\sum_{i=1}^N{\left(\array{s_1(\beta, A_i,\mathbf{X}_i) \\ \dots \\ s_k(\beta, A_i,\mathbf{X}_i)}\right)}=\mathbf{0}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and find the values of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that satisfy the equation. This is called finding the “roots” of the estimating equations. There is an estimating equation for each parameter to be estimated. When the estimating equations are the partial derivatives of the log likelihood with respect to each coefficient, then then &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that solve the estimating equations are also the maximum likelihood estimates.&lt;/p&gt;
&lt;p&gt;For logistic regression, the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; estimating estimating equations (one for each coefficient) look like the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{N}\sum_{i=1}^N{(A_i-p_i)X_{ki}}=0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It’s kind of crazy that it’s so simple when logistic regression seems so complicated and nonlinear. We estimate the coefficients in logistic regression simply by finding the values of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that are used to compute &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; that make this estimating equation equal 0 for each predictor. In practice, this can be done by using a “root-solver”, i.e., a function that finds the roots of a system of equations.&lt;/p&gt;
&lt;p&gt;Another cool thing (which we’ll come back to) is that this estimating equation is not just for logistic regression but is also for any generalized linear model with a canonical link; indeed, that’s what defines the canonical link. For binomial regression, the logit link is the canonical like. For Poisson regression, it’s the log link, and for linear regression, it’s the identity link. That is, these three models can be estimated using the exact estimating equations I wrote above but with &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; computed using the respective formula (&lt;span class=&#34;math inline&#34;&gt;\(p_i=\exp(\mathbf{X}_i\beta)\)&lt;/span&gt; for the log link and &lt;span class=&#34;math inline&#34;&gt;\(p_i=\mathbf{X}_i\beta\)&lt;/span&gt; for the identity link).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covariate-balancing-propensity-score-cbps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Covariate Balancing Propensity Score (CBPS)&lt;/h2&gt;
&lt;p&gt;You may have heard of CBPS before; it’s a way of estimating the propensity score in an observational study using logistic regression in such a way that balance is automatically achieved on the covariate means. How does this work? Instead of using the above estimation equations to estimate the coefficients, CBPS uses a different set of estimation equations, in particular&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{N}\sum_{i=1}^N{\left(\frac{A_i}{p_i} - \frac{1-A_i}{1-p_i}\right)X_{ki}}=0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The keen propensity score weighting enjoyer might notice something familiar about this estimating equation; it looks a lot like the weighted mean difference of &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; when using inverse probability weights for the ATE, that is &lt;span class=&#34;math inline&#34;&gt;\(w^{ATE}_i=\frac{A_i}{p_i} + \frac{1-A_i}{1-p_i}\)&lt;/span&gt;. And indeed, this is so! The weighted difference in means is usually expressed as &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sum_i{w_i A_i X_{ki}}}{\sum{w_i A_i}} - \frac{\sum_i{w_i (1-A_i) X_{ki}}}{\sum{w_i (1-A_i)}}\)&lt;/span&gt;, but when &lt;span class=&#34;math inline&#34;&gt;\(\sum_i{A_i/p_i} = \sum_i{(1-A_i)/(1-p_i)}=N\)&lt;/span&gt;, which is satisfied when an intercept is in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;, the estimating equation and the weighted difference in means are identical. That means what CBPS does is find the coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; such that weighted difference in means is equal to 0 for each covariate &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; when the weights are computed using the ATE formula applied to the probabilities generated by &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. It’s pretty ingenious, which is why it made such a splash!&lt;/p&gt;
&lt;p&gt;The idea of using estimating equations that correspond to covariate balance was also proposed by &lt;span class=&#34;citation&#34;&gt;Graham, De Xavier Pinto, and Egel (&lt;a href=&#34;#ref-grahamInverseProbabilityTilting2012&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; as “inverse probability tilting”; it turns out when applied to the ATT (described later) these methods are identical.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ato-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ATO Weights&lt;/h2&gt;
&lt;p&gt;Okay, but here is something else that is cool. A propensity score weighting enjoyer may have also heard of overlap weights or ATO weights &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-liBalancingCovariatesPropensity2018&#34;&gt;Li, Morgan, and Zaslavsky 2018&lt;/a&gt;)&lt;/span&gt;. These are weights computed as &lt;span class=&#34;math inline&#34;&gt;\(w^{ATO}_i=A_i(1-p_i) + (1-A_i)p_i=p_i(1-p_i)w^{ATE}_i\)&lt;/span&gt;. Overlap weights upweight the area of “overlap”, i.e., the are where units in the treated and control groups have approximately equal propensity scores (i.e., close to .5). They were developed as such because they yield an effect estimate with the most precision of any weights of the form &lt;span class=&#34;math inline&#34;&gt;\(h(A_i,X_i)w_i^{ATE}\)&lt;/span&gt; assuming the outcome variance is the same in both groups. Another cool thing about them is that they yield exact mean balance when logistic regression is used to estimate the propensity scores. It turns out this exact mean balance comes directly from the estimating equations for logistic regression, which I’ll demonstrate below.&lt;/p&gt;
&lt;p&gt;Recall the meat of the estimating equations for logistic regression, &lt;span class=&#34;math inline&#34;&gt;\((A_i-p_i)X_{ki}\)&lt;/span&gt;. With a little algebra, we can rewrite the estimating equations as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{N}\sum_{i=1}^N{(A_i-p_i)X_{ki}}=\frac{1}{N}\sum_{i=1}^N{\left(A_i(1-p_i)-(1-A_i)p_i \right)X_{ki}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This seems like an odd thing to do, but take a look at the formula for the ATO weights above. You’ll notice the logistic regression estimating equation now looks just like the formula for the weighted difference in means using the ATO weights! So, the solution to the logistic regression estimating equations, which sets their value to 0, also makes the weighted difference in means computed using the ATO weights exactly equal to 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;demonstration-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Demonstration in R&lt;/h2&gt;
&lt;p&gt;Okay, let’s take a look of all this in R. First, I’ll show you how to use M-estimation to fit a logistic regression model, then CBPS, and show you how each method balances the covariates in different ways.&lt;/p&gt;
&lt;p&gt;First, we specify the estimating equations. For logistic regression, we create the function &lt;code&gt;s_bar_lr()&lt;/code&gt;, which takes in a vector of coefficient estimates, the treatment, and the covariates, and returns the value of the estimating equations. The first step is to compute &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\text{expit}(\mathbf{X}_i\beta)\)&lt;/span&gt; (in R, expit is called &lt;code&gt;plogis()&lt;/code&gt;). Then, we supply that to the estimating equation and compute the means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_bar_lr &amp;lt;- function(b, A, X) {
  p &amp;lt;- plogis(drop(X %*% b))
  
  colMeans((A - p) * X)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we find the values of &lt;code&gt;b&lt;/code&gt; that make this vector equal to 0, we have found the maximum likelihood estimates of logistic regression coefficients. Let’s take a look at this in action. We’ll use &lt;code&gt;rootSolve::multiroot()&lt;/code&gt; to find the roots. This takes in a function, some starting values, and other variables you need to supply to the function, and finds the values of the desired parameters that make the function return a vector of 0s. We’ll use the &lt;code&gt;lalonde&lt;/code&gt; dataset as an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;cobalt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  cobalt (Version 4.5.4, Build Date: 2024-02-26)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;lalonde&amp;quot;)

X &amp;lt;- model.matrix(~age + educ + married + nodegree, data = lalonde)
A &amp;lt;- lalonde$treat

out &amp;lt;- rootSolve::multiroot(s_bar_lr,
                            start = rep(0, ncol(X)),
                            A = A, X = X)

setNames(out$root, colnames(X))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)         age        educ     married    nodegree 
## -2.54468907  0.01024968  0.12644332 -1.52238592  0.98034779&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’ll do the same using logistic regression implement in &lt;code&gt;glm()&lt;/code&gt; just to show we get the same estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glm(treat ~ age + educ + married + nodegree, data = lalonde,
           family = binomial(&amp;quot;logit&amp;quot;))
coef(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)         age        educ     married    nodegree 
## -2.54468907  0.01024968  0.12644332 -1.52238592  0.98034779&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(coef(fit), out$root, check.attributes = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can see that when we supply those values to the estimating equations, we get a vector of 0s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_bar_lr(out$root, A, X) |&amp;gt; round(12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)         age        educ     married    nodegree 
##           0           0           0           0           0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s compute ATO weights from the propensity scores estmated from this model and compute the weighted mean differences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- plogis(drop(X %*% out$root))

w_ATO &amp;lt;- A * (1 - p) + (1 - A) * p

col_w_smd(X[,-1], A, w_ATO) |&amp;gt; round(7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age     educ  married nodegree 
##        0        0        0        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ATO weights exactly balance the covariate means, as promised. If we compute the ATE weights, though, we find balance is good but not perfect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;w_ATE &amp;lt;- A / p + (1 - A) / (1 - p)

col_w_smd(X[,-1], A, w_ATE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          age         educ      married     nodegree 
## -0.066562991  0.059888054 -0.028829997 -0.004447331&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s use M-estimation to fit the CBPS model for the propensity scores. For this, we’ll use a different function that corresponds to the weighted difference in means for ATE weights. I’ll call this one &lt;code&gt;s_bar_cb()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_bar_cb &amp;lt;- function(b, A, X) {
  p &amp;lt;- plogis(drop(X %*% b))
  
  colMeans((A/p - (1-A)/(1-p)) * X)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll again estimate the coefficients using M-estimation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out &amp;lt;- rootSolve::multiroot(s_bar_cb,
                            start = rep(0, ncol(X)),
                            A = A, X = X)

setNames(out$root, colnames(X))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)          age         educ      married     nodegree 
## -2.907727232  0.004142096  0.168217307 -1.535394932  1.110766225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficients aren’t too different from the usual logistic regression coefficients. Let’s compare them to the coefficients estimated from &lt;code&gt;CBPS::CBPS()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_cb &amp;lt;- CBPS::CBPS(treat ~ age + educ + married + nodegree,
                     data = lalonde, ATT = 0, method = &amp;quot;exact&amp;quot;)

all.equal(out$root, drop(coef(fit_cb)), check.attributes = FALSE,
          tolerance = 1e-4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like we got them right&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;! Let’s compute balance using the estimated ATE weights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- plogis(drop(X %*% out$root))

w_ATE_cb &amp;lt;- A / p + (1 - A) / (1 - p)

col_w_smd(X[,-1], A, w_ATE_cb) |&amp;gt; round(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age     educ  married nodegree 
##        0        0        0        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out it’s pretty simple to make CBPS weights for the ATT or ATC, too; just apply the ATT or ATC weights formula to the estimating equations and then compute the weights from the resulting coefficients. I’ll demonstrate this without much commentary for the ATT below, noting that ATT weights are &lt;span class=&#34;math inline&#34;&gt;\(w_i^{ATT}=A + (1- A)\frac{p_i}{1-p_i}=p_i w_i^{ATE}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_bar_cb_att &amp;lt;- function(b, A, X) {
  p &amp;lt;- plogis(drop(X %*% b))
  
  colMeans((A - (1 - A) * p / (1 - p)) * X)
}

out &amp;lt;- rootSolve::multiroot(s_bar_cb_att,
                            start = rep(0, ncol(X)),
                            A = A, X = X)

p &amp;lt;- plogis(drop(X %*% out$root))

w_ATT_cb &amp;lt;- A + (1 - A) * p / (1 - p)

col_w_smd(X[,-1], A, w_ATT_cb) |&amp;gt; round(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age     educ  married nodegree 
##        0        0        0        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I told you earlier that all generalized linear models with a canonical link have the same estimating equations as logistic regression, and I also told you that the estimating equations for logistic regression imply exact balance on the covariate means when using ATO weights. It turns out that this means if you use any generalized linear model with a canonical link, you also get exact mean balance when using ATO weights computed from its predicted values. I’ll demonstrate that below with Poisson and linear regression. This time I’ll skip the M-estimation and just use &lt;code&gt;glm()&lt;/code&gt; to compute the propensity scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Poisson PS
p &amp;lt;- glm(treat ~ age + educ + married + nodegree, data = lalonde,
         family = poisson(&amp;quot;log&amp;quot;)) |&amp;gt; fitted()

w_ATO_pois &amp;lt;- A * (1 - p) + (1 - A) * p

col_w_smd(X[,-1], A, w_ATO_pois) |&amp;gt; round(7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age     educ  married nodegree 
##        0        0        0        0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Linear PS
p &amp;lt;- glm(treat ~ age + educ + married + nodegree, data = lalonde,
         family = gaussian(&amp;quot;identity&amp;quot;)) |&amp;gt; fitted()

w_ATO_lin &amp;lt;- A * (1 - p) + (1 - A) * p

col_w_smd(X[,-1], A, w_ATO_lin) |&amp;gt; round(7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age     educ  married nodegree 
##        0        0        0        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, but before I end, I want to show you something cool and maybe unexpected about the linear regression ATO weights. This was inspired by a finding in &lt;span class=&#34;citation&#34;&gt;Hazlett and Shinkre (&lt;a href=&#34;#ref-hazlettUnderstandingAvoidingWeights2024&#34;&gt;n.d.&lt;/a&gt;)&lt;/span&gt;. Consider estimating a treatment effect using a linear regression of the outcome on the treatment and covariates but with no interaction between them. It turns out the coefficient on treatment is exactly equal to the weighted difference in means computed using the linear regression propensity score ATO weights. See below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- lm(re78 ~ treat + age + educ + married + nodegree,
          data = lalonde)
coef(fit)[&amp;quot;treat&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    treat 
## 207.3495&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;col_w_smd(lalonde$re78, A, w_ATO_lin, std = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 207.3495&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s pretty crazy! That mean we can interpret this linear regression estimate as an overlap-weighted difference in means, where the propensity scores used in the overlap weights are fitted using linear regression (i.e., the linear probability model). Of course, the linear probability model is a pretty bad propensity score model since it can yield propensity scores greater than 1 and less than 0, which yield negative and possibly extreme ATO weights, unlike the logistic regression-based ATO weights, which are always positive and never extreme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Hopefully you found this moderately interesting and learned something about M-estimation, logistic regression, CBPS, and overlap weights!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-grahamInverseProbabilityTilting2012&#34; class=&#34;csl-entry&#34;&gt;
Graham, Bryan S., Cristine Campos De Xavier Pinto, and Daniel Egel. 2012. &lt;span&gt;“Inverse Probability Tilting for Moment Condition Models with Missing Data.”&lt;/span&gt; &lt;em&gt;The Review of Economic Studies&lt;/em&gt; 79 (3): 1053–79. &lt;a href=&#34;https://doi.org/10.1093/restud/rdr047&#34;&gt;https://doi.org/10.1093/restud/rdr047&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hazlettUnderstandingAvoidingWeights2024&#34; class=&#34;csl-entry&#34;&gt;
Hazlett, Chad, and Tanvi Shinkre. n.d. &lt;span&gt;“Understanding and Avoiding the &lt;span&gt;&#34;&lt;/span&gt;Weights of Regression&lt;span&gt;&#34;&lt;/span&gt;: Heterogeneous Effects, Misspecification, and Longstanding Solutions.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-imaiCovariateBalancingPropensity2014&#34; class=&#34;csl-entry&#34;&gt;
Imai, Kosuke, and Marc Ratkovic. 2014. &lt;span&gt;“Covariate Balancing Propensity Score.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (1): 243263. &lt;a href=&#34;https://doi.org/10.1111/rssb.12027&#34;&gt;https://doi.org/10.1111/rssb.12027&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-liBalancingCovariatesPropensity2018&#34; class=&#34;csl-entry&#34;&gt;
Li, Fan, Kari Lock Morgan, and Alan M. Zaslavsky. 2018. &lt;span&gt;“Balancing Covariates via Propensity Score Weighting.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 113 (521): 390–400. &lt;a href=&#34;https://doi.org/10.1080/01621459.2016.1260466&#34;&gt;https://doi.org/10.1080/01621459.2016.1260466&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rossMestimationCommonEpidemiological2024&#34; class=&#34;csl-entry&#34;&gt;
Ross, Rachael K, Paul N Zivich, Jeffrey S A Stringer, and Stephen R Cole. 2024. &lt;span&gt;“M-Estimation for Common Epidemiological Measures: Introduction and Applied Examples.”&lt;/span&gt; &lt;em&gt;International Journal of Epidemiology&lt;/em&gt; 53 (2): dyae030. &lt;a href=&#34;https://doi.org/10.1093/ije/dyae030&#34;&gt;https://doi.org/10.1093/ije/dyae030&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stefanskiCalculusMEstimation2002&#34; class=&#34;csl-entry&#34;&gt;
Stefanski, Leonard A., and Dennis D. Boos. 2002. &lt;span&gt;“The Calculus of m-Estimation.”&lt;/span&gt; &lt;em&gt;The American Statistician&lt;/em&gt; 56 (1): 29–38. &lt;a href=&#34;https://doi.org/10.1198/000313002753631330&#34;&gt;https://doi.org/10.1198/000313002753631330&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This is for the just- or exactly-identified CBPS; the over-identified version (which is the default in the &lt;code&gt;CBPS&lt;/code&gt; package) combines both the logistic regression and modified estimating equations below; because there are now more estimating equations than parameters, this is instead solved using generalized method of moments (GMM).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Actually, the coefficients we computed are more accurate than those from &lt;code&gt;CBPS()&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why &#34;Why Propensity Scores Should Not Be Used for Matching&#34; Should Not Be Used to Dismiss Propensity Score Matching</title>
      <link>https://ngreifer.github.io/blog/in-defense-of-propensity-scores/</link>
      <pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/in-defense-of-propensity-scores/</guid>
      <description>


&lt;p&gt;Propensity scores get a lot of hate. They get hate on Twitter, on CrossValidated, and in the literature. They are hated by trialists, epidemiologists, and economists. Among the many papers hating on propensity scores, perhaps the most famous is &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, “Why Propensity Scores Should Not Be Used for Matching”, a paper with a title so clear, it seems hardly necessary to read the actual paper, right?&lt;/p&gt;
&lt;p&gt;But there are plenty of people who love propensity scores. Medical researchers, especially those with little statistical sophistication, love them. I have a Google Scholar alert for “propensity score” and everyday I am bombarded by medical studies that used propensity score matching to estimate the effect of robot assisted surgery vs being thrown down a well among the subpopulation of men aged 45-47 with pancreatic cancer and HIV. Some of the most influential statisticians of the day have written positively about propensity scores, including legendary statistician Don Rubin (who, with observational studies guru Paul Rosenbaum, invented/discovered them in &lt;span class=&#34;citation&#34;&gt;Rosenbaum and Rubin (&lt;a href=&#34;#ref-rosenbaumCentralRolePropensity1983&#34;&gt;1983&lt;/a&gt;)&lt;/span&gt;), famed clear writer, acclaimed biostatistician, and wonderful person Liz Stuart, political methodologist extraordinaire Kosuke Imai, and genre-defining clinician-turned-epidemiologist-savant Jamie Robins.&lt;/p&gt;
&lt;p&gt;I, your humble narrator, have been on both sides of the debate, defending propensity scores online, critiquing the critiques, and insulting the fools who think methodological development stopped in 1983. I am the author and maintainer of several pieces of software that facilitate using propensity scores, but those same pieces of software also offer and emphasize alternatives to propensity scores. I was invited to a &lt;a href=&#34;https://quantitudepod.org/s3e27-propensity-scores/&#34;&gt;statistics podcast&lt;/a&gt; to talk about propensity scores and refused to define them until 22 minutes into the episode.&lt;/p&gt;
&lt;p&gt;In this post, I’m going to clarify some of the arguments against propensity scores and respond to them. A theme I want to highlight is that almost none of these arguments are against propensity scores themselves. They are about study designs, data analysis strategies and philosophies, and classes of statistical estimators. To start off, I want to clarify the role propensity scores take in observational studies (and it’s not as central as &lt;span class=&#34;citation&#34;&gt;Rosenbaum and Rubin (&lt;a href=&#34;#ref-rosenbaumCentralRolePropensity1983&#34;&gt;1983&lt;/a&gt;)&lt;/span&gt; “The Central Role of Propensity Scores in Observational Studies” suggests).&lt;/p&gt;
&lt;p&gt;First, I want to highlight where propensity scores fall into the study design/analysis methods hierarchy. Some people seem to think the heirarchy looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Flowchart 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is, you’re either doing a beautiful, perfect, scientific randomized trial, or you’re using propensity scores. Which means any critique you can throw against any design/method that isn’t a randomized trial is an argument against propensity scores. In fact, and as I explain in this CV answer and my Quantitude episode, the hierarchy looks more like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#FLowchart 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The hierarchy is explained well in &lt;span class=&#34;citation&#34;&gt;Matthay et al. (&lt;a href=&#34;#ref-matthayAlternativeCausalInference2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. At the top we have the distinction between randomized trials, in which participants are randomly assigned to treatment condition, and observational studies, in which assignment to treatment condition is done by the clinician, participant, or an unknown but nonignorable force. Among methods to analyze observational studies, there are several that can be used depending on the assumptions one can meet. If an instrumental variable is available that meets the required exclusion criteria, instrumental variable analysis can be used. If a pre-treatment measure of the outcome is available and the parallel trends assumptions is met, difference-in-differences (DiD) can be used. If you have collected a sufficient set of variables to eliminate confounding, then covariate adjustment methods can be used.&lt;/p&gt;
&lt;p&gt;Among covariate adjustment methods, there are those that are considered “analysis-based” (i.e., model the outcome), “design-based” (i.e., manipulate the sample without involving the outcome), or doubly-robust (which do both, or at least involve modeling both the treatment and outcome). Among design-based methods are matching (including stratification, subset selection, and pair matching) and weighting. And among matching and weighting methods, there are variations that use propensity scores and those that don’t.&lt;/p&gt;
&lt;p&gt;Critiques of propensity scores exist at all levels of the hierarchy. But these critiques are distinct and should not be muddled. It’s true that any critique at any level of this hierarchy is a critique of propensity score methods, though perhaps only indirectly. For example, one may say they dislike propensity score methods because they require you to meet the assumption of strong ignorability (a.k.a., conditional exchangeability, satisfaction of the backdoor criterion, selection on observables). But all covariate adjustment methods share this same critique. It’s not that the critique is invalid, but citing &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; in support of this argument makes no sense given that &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; is an argument about the very lowest level of the hierarchy, that is, between matching methods that use propensity scores and matching methods that don’t.&lt;/p&gt;
&lt;p&gt;In what follows, I will evaluate 5 arguments about why propensity scores should not be used. These arguments are levied against propensity scores at each of the levels of the hierarchy, as if propensity scores represent the entire body of causal inference methodology for observational studies. In many cases, the arguments are not wrong, but they are often misdirected, with naive combatants citing irrelevant facts and papers to attempt to support their point.&lt;/p&gt;
&lt;div id=&#34;propensity-scores-cant-be-used-to-establish-causality-only-a-randomized-trial-can&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Propensity scores can’t be used to establish causality; only a randomized trial can&lt;/h3&gt;
&lt;p&gt;This argument is a direct reaction to the frequent use of propensity scores to estimate causal effects in observational studies. The argument goes as follows: given that observational studies lack randomization, their ability to estimate causal effects requires untenable assumptions (no matter what those assumptions are), and the only design that can validly be used to assess causality involves double-blind randomization. Observational studies are neither double-blind (participants know what treatment they receive) nor randomized (participants receive treatment based on an impermeable cloud of complex mechanisms). This is not an argument against propensity scores; it is an argument against all observational studies and the methods used to analyze them.&lt;/p&gt;
&lt;p&gt;I agree that observational studies require very strong assumptions, many of which cannot even be hoped to be met in practice except in a few highly specific cases. But this argument has nothing to do with propensity scores. Propensity scores are so far from the concepts actually being discussed in this argument. They are a specific implementation of a specific method of a specific class of methods that require a specific assumption, not an embodiment of the analysis of all observational studies. If you want to make this claim, you can’t use propensity scores as a punching bag, and you can’t include critiques aimed specifically at propensity scores to bolster your point. You have to argue against all applications of instrumental variables, difference-in-differences, and covariate adjustment methods, including those that involve and don’t involve propensity scores. (It’s also known that propensity scores can be used to validly estimate effects in randomized trials &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-williamsonVarianceReductionRandomised2014&#34;&gt;Williamson, Forbes, and White 2014&lt;/a&gt;)&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;Another problem I and many others have with this argument is it means that so little science can actually get done. Randomized trials are are a tiny subset of the data that is available to us. Discarding all of it simply because it doesn’t satisfy the strict requirements of randomized treatment assignment would be a tremendous waste, not just because so much money is spent collecting observational data (e.g., in longitudinal surveys or passively in healthcare databases), but also because there actually is useful information to be gleaned from observational studies, even if that information is not assumption free.&lt;/p&gt;
&lt;p&gt;One critique goes that observational studies should not be run in medical research because it is unethical to use information laden with such strict assumptions to treat patients; I don’t like this argument because it is the fault of the clinician for using such information, not the researcher for producing it. That is, it might actually be unethical for a clinician to use observational data as evidence to treat patients. But what isn’t unethical is a researcher collecting observational data, estimating a conditional association using propensity scores or another method, and interpreting their estimate as a possibly biased estimate of a causal effect.&lt;/p&gt;
&lt;p&gt;Most of all, this argument has nothing to do with the claims if &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, who accept the premise that observational studies can provide useful results, strong ignorability is a plausible assumption, design-based analyses are useful, and matching is a valid effect estimator.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-scores-shouldnt-be-used-because-strong-ignorability-never-holds-in-practice-but-iv-or-did-assumptions-do&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Propensity scores shouldn’t be used because strong ignorability never holds in practice, but IV or DiD assumptions do&lt;/h3&gt;
&lt;p&gt;This argument is implicit in much economic research. Even though some of the biggest names in econometrics have written fruitfully about propensity score analysis, it is still rarely used in the field, and instrumental variable methods and difference-in-difference methods are given far greater attention. Most work on propensity score analysis in economics has been done by Abadie and Imbens (yes, Nobel-prize-sharing Imbens), with some recent work done by Martin Huber (no, not robust standard error Huber). Otherwise, propensity score analysis receives little attention, except when disguised as synthetic controls (which is a propensity score weighting method) or when used alongside modern DiD methods (like the weighting methods used in Calloway and Santanna’s TWFE-alternative estimators).&lt;/p&gt;
&lt;p&gt;As someone coming from psychology and biostatistics, it is absurd to me to think that the assumptions underlying instrumental variable methods or DiD could ever be more tenable than strong ignorability. Instrumental variables require the exclusion restriction, no confounding of the instrument, relevance of the instrument to the treatment, and monotonicity. They extrapolate from the data, are imprecise, and yield a mostly uninterpretable estimand. The burden of proof for these assumptions is as strong as it is for strong ignorability.&lt;/p&gt;
&lt;p&gt;Similarly, DiD requires two assumptions that to me seem highly unrealistic: that pre-treatment outcomes don’t cause selection into treatment and that the outcome trends in the absence of treatment are the same between the treatment groups. If the outcome is anything more than just a epiphenomenon and is actually somewhat involved in who receives treatment (which of course it would be, because treatments meant to affect the outcome would be given to units based on need as determined by prior outcomes), this assumptions is violated. And (conditional) parallel trends is one step away from strong ignorbaility; if two groups are different, why would their trends be the same? Conditional parallel trends requires adjusting for all common causes of treatment and trends, which sounds a lot like adjusting for all causes of treatment and the outcome. How these could be considered weaker assumptions than strong ignorability is beyond me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-scores-have-no-advantages-over-outcome-regression-and-therefore-shouldnt-be-used&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Propensity scores have no advantages over outcome regression, and therefore shouldn’t be used&lt;/h3&gt;
&lt;p&gt;This is a fairly common argument in the circles I live in, often instigated by a naive student wondering when to use propensity scores vs. regression. To be fair, there has been very little stated explicitly in the literature about this point. One of my most popular answers on CrossValidated &lt;a href=&#34;https://stats.stackexchange.com/a/544958/116195&#34;&gt;addresses&lt;/a&gt; this question specifically. I’ll summarize some of the arguments here, but I’ll also address some arguments I have seen on Twitter and elsewhere. Importantly, though, this is not really a debate about propensity scores vs regression; it is a debate bout analysis-based methods vs. designed-based methods. And of course the resolution is clear: combine them.&lt;/p&gt;
&lt;p&gt;This argument actually is mostly true. The best performing methods are those that flexibly model the outcome, as these methods not only eliminate bias due to confounding on the observed covariates but also reduce the unexplained variability in the outcome, which is what provides precision in the effect estimate. Methods like BART dramatically outperform designed-based methods in simulations &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-dorieAutomatedDoityourselfMethods2019&#34;&gt;Dorie et al. 2019&lt;/a&gt;; &lt;a href=&#34;#ref-huEstimationCausalEffects2020a&#34;&gt;Hu et al. 2020&lt;/a&gt;)&lt;/span&gt;, and AIPW and TMLE are among the only methods that have solid guarantees for root-n convergence of the estimator and asymptotically valid inference.&lt;/p&gt;
&lt;p&gt;Here are a few reasons why I don’t still don’t love this argument:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The simulations that use state-of-the-art regression methods use the most basic and ignorantly applied design-based methods. It’s definitely an advantage for flexible analysis-based methods to be used out of the box whereas design-based methods need careful fine-tuning by the analyst, but that doesn’t make the usual comparisons fair. &lt;span class=&#34;citation&#34;&gt;Keele and Small (&lt;a href=&#34;#ref-keeleComparingCovariatePrioritization2018&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; found that carefully use matching methods yielded similar results to using BART, and they weren’t even using matching methods that well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The whole point of designed-based methods is to capitalize on the separation between the design and analysis of the study. This isn’t just for some wishy-washy honesty/objectivity reason that has been purported in the early literature &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rubinDesignAnalysisObservational2007&#34;&gt;Rubin 2007&lt;/a&gt;)&lt;/span&gt;; rather, it is about the epistemic advantage of being able to prove to your reader that you eliminated confounding due to the measured variables. That is, design-based methods allow you to assess and demonstrate covariate balance. A balance table that completely and holistically demonstrates balance is far more justification for the confidence in an unbiased effect estimate than the asymptotic guarantees of TMLE or the good simulation performance of BART. The cross-validation accuracy of a machine learning method is not related to its performance at eliminating bias, and there are no other diagnostics to compare the performance of a complicated method like TMLE with Superlearner composed of HAL and 20 candidate learners vs a simple method like g-computation with a random forest. For many, including me, this epistemic advantage that design-based methods have is worth the decrease in precision we incur from using them. But that benefit is only realized when balance is assessed fully and holistically; the usual balance statistics people report (i.e., standardized mean differences) are not sufficient to realize these benefits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Design-based methods are and always have been designed to be used with an outcome regression model. It has never been an either-or situation except in simulation studies or academic debates. The early literature on matching and many of the most influential papers on the topic recommend combining matching with regression adjustment &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rubinCombiningPropensityScore2000&#34;&gt;Rubin and Thomas 2000&lt;/a&gt;; &lt;a href=&#34;#ref-rubinUseMatchedSampling1973&#34;&gt;Rubin 1973&lt;/a&gt;; &lt;a href=&#34;#ref-abadieBiasCorrectedMatchingEstimators2011&#34;&gt;Abadie and Imbens 2011&lt;/a&gt;)&lt;/span&gt;. Similarly, weighted g-computation and AIPW have long been recommended for use after weighting &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-vansteelandtInvitedCommentaryGComputation2011&#34;&gt;Vansteelandt and Keiding 2011&lt;/a&gt;; &lt;a href=&#34;#ref-coleConstructingInverseProbability2008&#34;&gt;Cole and Hernán 2008&lt;/a&gt;)&lt;/span&gt;. The mainstream philosophy of matching, matching as nonparametric pre-processing as espoused by &lt;span class=&#34;citation&#34;&gt;Ho et al. (&lt;a href=&#34;#ref-hoMatchingNonparametricPreprocessing2007&#34;&gt;2007&lt;/a&gt;)&lt;/span&gt;, is all about using matching to increase the robustness of and decrease extrapolation due to outcome regression. The guides on best practices for estimating effects after matching weighting using &lt;code&gt;MatchIt&lt;/code&gt; and &lt;code&gt;WeightIt&lt;/code&gt; only show how to estimate effects with covariates adjusted for in the outcome model. The software for matching imputation, the less mainstream philosophy of matching described by &lt;span class=&#34;citation&#34;&gt;Abadie and Imbens (&lt;a href=&#34;#ref-abadieLargeSampleProperties2006&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;, makes it extremely easy to incorporate the outcome model into the effect estimation as recommended by &lt;span class=&#34;citation&#34;&gt;Abadie and Imbens (&lt;a href=&#34;#ref-abadieBiasCorrectedMatchingEstimators2011&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;. All advantages outcome regression has over pure-designed based methods are shared by these combined methods, with the additional epistemic advantage of getting to prove to your audience you have achieved balance (and such that the price in precision for this advantage is decreased due to the increased precision afforded by the outcome model).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Outcome regression models are numerically equivalent to certain kinds of balancing weights that imply a given propensity score model. That is, the distinction between purely analysis-based and purely designed-based methods is blurry anyway. &lt;span class=&#34;citation&#34;&gt;Zubizarreta (&lt;a href=&#34;#ref-zubizarretaStableWeightsThat2015&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; described a method of estimating weights (without involving the outcome) that minimizes the variance of the weights subject to balance constraints on the covariate means and that the weights must be positive. &lt;span class=&#34;citation&#34;&gt;Chattopadhyay and Zubizarreta (&lt;a href=&#34;#ref-chattopadhyayImpliedWeightsLinear2022a&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt; showed that g-computation using a linear regression model implies the exact same weights as that method except without the constraint that the weights are positive. That is, linear regression is identical to balancing weights except that linear regression can extrapolate. Similar equivalences have been demonstrated by &lt;span class=&#34;citation&#34;&gt;Bruns-Smith et al. (&lt;a href=&#34;#ref-bruns-smithAugmentedBalancingWeights2023&#34;&gt;n.d.&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Ben-Michael, Feller, and Rothstein (&lt;a href=&#34;#ref-ben-michaelAugmentedSyntheticControl2021&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are the main arguments I see for why you should avoid propensity score methods (and implicitly, all design-based methods). Frank Harrell also has specific and more nuanced arguments that I want to address. His arguments are more specifically addressed to the context he works in, which is in the estimation of the treatment effects on binary outcomes in medical research. Frank’s recommended alternative is to use the coefficient on treatment in a logistic regression of the outcome on the treatment and covariates, which should be modeled flexibly with splines. At worst, when there are too many covariates to adjust for and there is some use for the propensity score as a dimension reduction method, the propensity score should be included in the outcome model with a flexible spline and not used for matching or weighting.&lt;/p&gt;
&lt;p&gt;As far as I can tell, Frank’s arguments boil down to these claims:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Propensity score matching needlessly destroys precision by discarding useful data and not reducing unexplained variability in the outcome.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The estimand targeted by propensity score matching, the ATE or ATT, is far less clinically useful than the conditional ATE (CATE) targeted by the logistic regression method, both in terms of scientific relevance and generalizability beyond the specific dataset that happened to be used to estimate the effect.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Logistic regression is the most plausible model for binary outcomes, and the odds ratio is the most plausible scale for which there could be no modification of the treatment effect by the covariates, which is why the coefficient on treatment in a logistic regression is a satisfactory estimate of the CATEs, which are assumed to be equal on the odds ratio scale.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I don’t disagree with all those points, but I disagree that they imply design-based methods shouldn’t be used or that Frank’s logistic regression-based alternative is a viable solution. I already explained why I don’t think the precision trade-off is so pernicious: it is a trade I and many researchers are willing to make in order to obtain the epistemic advantages of being confident bias has been eliminated, and it can be mitigated by using an outcome model on the adjusted sample as recommended heavily in the literature and tutorials using methods easily accessible to researchers using mainstream software. But there is no doubt that one is making this trade-off, and if you are a person for whom that trade-off isn’t worth it (i.e., you are more convinced by the asymptotic guarantees of AIPW and TMLE or the empirical performance of BART), then you should avoid design-based methods. Frank is evidently one of those people, and I respect that opinion.&lt;/p&gt;
&lt;p&gt;Frank’s second point is less about propensity scores or even design-based methods and more about the estimand being used and the ability of different methods to target that estimand. There is no doubt the CATE is more clinically useful than an ATE. A CATE for a given patient profile allows a clinician to tailor treatment to such a patient and doesn’t depend on the population from which the CATE was estimated; that is, in an ideal scenario, the same CATE would be estimated for a given profile regardless of the origin of the dataset as long as the same variables were conditioned on. In that sense, the CATE is more generalizable than an ATE, which involves averaging over the covariate distribution of the sample that happened to be used in the analysis. In addition, the ATE doesn’t help a clinician decide on a treatment for an individual patient because most effect measures for binary treatments (e.g., the risk difference or risk ratio) necessarily depend on the level of baseline risk, and the odds ratio is not collapsible, meaning the marginal odds ratio is not equal to the conditional odds ratio even when all conditional odds ratios are equal. Frank’s argument is that propensity score methods (and other design-based methods) can only estimate ATEs, and therefore should not be used, whereas logistic regression can estimate the CATE and therefore should be used.&lt;/p&gt;
&lt;p&gt;My rebuttal to this line of reasoning is that CATEs are a pipedream and the coefficient on treatment in a logistic regression is not a valid estimate of them. This view is shared by many causal inference researchers who study methods that target the ATE and heterogeneous treatment effects. To estimate a CATE in the most conservative way, one must perform the entire analysis, whatever that may be, within each subgroup defined by the conditioning covariates. That means the estimate must be adequately powered in each subgroup, which is virtually impossible without absolutely massive datasets and oversampling small groups. One alternative is to make some smoothing assumptions or regularize the differences among the CATEs, which is done by modern machine learning methods for heterogeneous treatment effects. Even these yield CATEs that are highly imprecise or marginalize over subsets of covariates.&lt;/p&gt;
&lt;p&gt;At the other end of the spectrum is to assume all CATEs are the same on the odds ratio scale (i.e., so treatment doesn’t interact with any covariates) and the outcome-covariate relationships can be captured in a (possibly flexible) logistic regression model. To me, these are extremely strong assumptions. Simply failing to find treatment-covariate interactions in studies not powered to detect them is not evidence for the absence of such interactions. Getting the outcome model wrong will yield biased estimates for the CATE &lt;em&gt;even if the CATEs are all the same and treatment is randomized&lt;/em&gt;. In addition, even if the model were correctly specified, there is no way to prove such to your audience. They simply have to take your word that the model you specified, with all its arbitrariness and strict constraints, is correct. Good cross-validated predictive accuracy (or a good AUC) is neither necessary nor sufficient for unbiased effect estimation. I, as a reader, would be extremely suspect of such an analysis and would place no faith in its estimates, even if I were to believe in strong ignorability and the homogeneity of treatment effects on the odds ratio scale.&lt;/p&gt;
&lt;p&gt;Frank often says things like a problem with propensity score analysis is that it doesn’t take into account treatment-covariate interactions. Again, I think this is not a problem with propensity score analysis in particular but simply with the choice of estimand. The ATE marginalizes over the CATEs, and in that sense does ignore treatment-covariate interactions. I agree that this makes the ATE far less scientifically and clinically useful than a CATE. And I also agree that propensity score methods can only target ATEs. But that is not a reason to avoid propensity score analysis if you want to target the ATE; it a reason to avoid targeting the ATE. And I think the ATE often genuinely is of interest, and even when it isn’t, it is the best one can do without making extremely strict and unrealistic assumptions. Propensity score and other design-based analyses have the advantage over analysis-based methods in that they can be agnostic as to whether there is treatment effect heterogeneity and still arrive at a valid estimate of the ATE, whereas analysis-based method must correctly model the outcome, including all interactions between treatment and covariates, to estimate the ATE validly. That is, it is a &lt;em&gt;strength&lt;/em&gt;, not a weakness, of propensity score methods that they allow you to ignore treatment effect heterogeneity.&lt;/p&gt;
&lt;p&gt;If you categorically reject the ATE as a useful estimand, then I am sympathetic to the rejection of all design-based methods and analysis-based methods that target the ATE, including those that involve propensity scores. I still don’t think a flexibly modeled logistic regression is the answer.&lt;/p&gt;
&lt;p&gt;To summarize, I don’t totally disagree with this argument because I agree that analysis-based methods often outperform design-based methods in terms of precision and validity of asymptotic inference. But this argument ignores the whole benefit of design-based methods, which is an epistemic, not statistical, advantage. Only with design-based methods can you provide evidence to your audience that you have removed structural bias due to imbalance. We have found that some analysis-based methods are equivalent to designed based methods but simply trade extrapolation for precision. Whether a researcher is willing to make such a trade-off is up to them, and whether the epistemic benefit of designed-based methods is worth the decrease in precision is up to them. Of course, we can minimize the costs of the trade-off by simply doing both, e.g., adjusting for covariates in the outcome model in the matched sample, which is a recommended and accessible strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-score-matching-is-worse-than-inverse-probability-weighting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Propensity score matching is worse than inverse probability weighting&lt;/h3&gt;
&lt;p&gt;This is a somewhat esoteric argument but is one I see espoused by epidemiologists, which suggests why propensity score matching is so uncommon for epidemiological research but inverse probability weighting is more accepted. One reason for the prominence of this argument is that it appears in (at least) two major epidemiological textbooks: &lt;em&gt;Modern Epidemiology&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rothmanModernEpidemiology2021&#34;&gt;Rothman et al. 2021&lt;/a&gt;)&lt;/span&gt; and &lt;em&gt;What If?&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hernanCausalInferenceWhat2020&#34;&gt;Hernán and Robins 2020&lt;/a&gt;)&lt;/span&gt;. An exception is in the subfield of pharmacoepidemiology, where for some reason it has flourished due to the interest of a group of researchers including XXX.&lt;/p&gt;
&lt;p&gt;Matching and weighting, whether on the propensity score or not, are identical methods conceptually and causally, differing only in their statistical performance, which will vary from dataset to dataset and from the specific implementation of each method. There is no reason to categorically claim one method is better than the other as these textbooks do. In simulation studies comparing the most basic version of the methods, they often perform equally well or differ in performance depending on the simulation factors &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kushCovariateBalanceObservational2022&#34;&gt;Kush et al. 2022&lt;/a&gt;; &lt;a href=&#34;#ref-waernbaumModelMisspecificationRobustness2012&#34;&gt;Waernbaum 2012&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Liz and I try to identify some of the reasons one might prefer one or the other or expect one to perform better than the other in our paper together &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-greiferMatchingMethodsConfounder2021a&#34;&gt;Greifer and Stuart 2021&lt;/a&gt;)&lt;/span&gt;. But the conclusion is what it always is when researchers ask which method is better: try both and use the one that optimizes the three key properties of a design-based method: balance, effective sample size, and representativeness. There is no guarantee that weighting will outperform matching in these domains, and there are many ways, often ignored in these epidemiology textbooks, of customizing the matching specification to avoid the problems they can face with respect to these qualities. For example, &lt;span class=&#34;citation&#34;&gt;Hernán and Robins (&lt;a href=&#34;#ref-hernanCausalInferenceWhat2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; argue that propensity score matching should be avoided because units are discarded, so the effect estimate may not transport to a relevant population. It’s true that some methods of matching do this, but others, like stratification, full matching, or matching imputation can easily target the ATE, the same estimand as inverse probability weighting. In addition, sometimes targeting a narrower population is a strength, not a weakness, because it may be easier to achieve balance and maintain precision when using matching compared to weighting, and not all studies are designed to generalize to a specific population &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-maoPropensityScoreWeighting2018&#34;&gt;Mao, Li, and Greene 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The thing to remember is that 1:1 propensity score matching without replacement and standard inverse probability weighting using a logistic regression propensity score are the tips of the respective matching and weighting icebergs. On the matching side, we can do matching with replacement, which is asymptotically equivalent to inverse probability weighting &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-linEstimationBasedNearest2021&#34;&gt;Lin, Ding, and Han 2021&lt;/a&gt;)&lt;/span&gt;, or cardinality and profile matching, which use integer programming to optimize the matched sample size subject to user-specified balance constraints &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-zubizarretaMatchingBalancePairing2014&#34;&gt;Zubizarreta, Paredes, and Rosenbaum 2014&lt;/a&gt;; &lt;a href=&#34;#ref-cohnProfileMatchingGeneralization2022&#34;&gt;Cohn and Zubizarreta 2022&lt;/a&gt;)&lt;/span&gt;, or genetic matching, which uses a genetic algorithm to optimally prioritize covariates to minimize imbalance &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34;&gt;Diamond and Sekhon 2013&lt;/a&gt;)&lt;/span&gt;, or full matching, which assigns all units to subclasses to optimize a global distance criterion &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hansenOptimalFullMatching2006&#34;&gt;Hansen and Klopfer 2006&lt;/a&gt;; &lt;a href=&#34;#ref-stuartUsingFullMatching2008&#34;&gt;Stuart and Green 2008&lt;/a&gt;)&lt;/span&gt;, etc. On the weighting side, we can do entropy balancing and stable balancing weights, which maximize the effective sample size subject to balance constraints &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hainmuellerEntropyBalancingCausal2012&#34;&gt;Hainmueller 2012&lt;/a&gt;; &lt;a href=&#34;#ref-zubizarretaStableWeightsThat2015&#34;&gt;Zubizarreta 2015&lt;/a&gt;; &lt;a href=&#34;#ref-kallbergLargeSampleProperties2022&#34;&gt;Källberg and Waernbaum 2022&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;McCaffrey, Ridgeway, and Morral (&lt;a href=&#34;#ref-mccaffreyPropensityScoreEstimation2004&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; style generalized boosted modeling, which tunes a tuning parameter in the propensity score model to optimize balance in the weighted dataset, CBPS, which combines logistic regression with balance constraints &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-imaiCovariateBalancingPropensity2014&#34;&gt;Imai and Ratkovic 2014&lt;/a&gt;)&lt;/span&gt;, or energy balancing, which balances the whole covariate distribution by minimizing a global imbalance statistic &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hulingEnergyBalancingCovariate2022&#34;&gt;Huling and Mak, n.d.&lt;/a&gt;)&lt;/span&gt;, etc. To say weighting is superior to matching is silly when there are so many variants of each method that are targeted to different constraints and desiderata in a study.&lt;/p&gt;
&lt;p&gt;Why I don’t hate this argument is that I actually kind of agree with it. Because weights are smooth whereas matching weights involve discreteness by construction, it is easier to use standard and fast derivative-based optimization methods to find truly optimal weights, whereas optimization problems for matching are much slower due to the integer constraints or lumpiness in the objective function. The robustness benefits matching can have over weighting (which are due to the integer constraints) can be reduced by using weighting methods that specifically target the kinds of imbalance matching is meant to be robust to. That is, with weighting, it is easier to optimize criteria, manage trade-offs more effectively, and exert finer control over the resulting adjusted sample. When matching outperforms weighting, it does so by chance and in ways that are often hard to assess (e.g., balance on an unseen part of the covariate distribution).&lt;/p&gt;
&lt;p&gt;Weighting is also easier to generalize to more complex cases, like multi-category, continuous, and longitudinal treatments, but that isn’t a good reason to avoid matching when matching is appropriate. It’s not like you have to pick one type of method for all scenarios and only use that one; you can and should use the method that is most appropriate for the problem at hand. It may be harder to incorporate weights into an outcome model than to simply run the model in the matched sample. Or, it may be easier to use a known-to-be-valid bootstrap procedure when weighting than to correctly incorporate pair membership into a post-matching inference.&lt;/p&gt;
&lt;p&gt;So, in summary, I agree that weighting methods often outperform matching methods, but that is not always the case, and matching methods should not be dismissed outright as they are by epidemiology textbooks. Conceptually, matching and weighting are identical, and their only difference is in their statistical performance, which depends on the unique features of each dataset and which can often be assessed before moving forward with effect estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;propensity-score-matching-is-worse-than-other-matching-methods&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5. Propensity score matching is worse than other matching methods&lt;/h3&gt;
&lt;p&gt;This is the sole thesis of &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Propensity score matching is bad for various reasons, and other matching methods that don’t share its flaws should be used instead. &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; is, in my opinion, one of the least understood papers in the literature (or at least the least understood among those that are the easiest to read). I have written at &lt;a href=&#34;https://stats.stackexchange.com/a/481130/116195&#34;&gt;some length&lt;/a&gt; about my thoughts on the paper, so I won’t go into too much depth here. Instead, I’ll focus on my evaluation of the use of the paper as an argument against using propensity scores.&lt;/p&gt;
&lt;p&gt;I dislike this argument, mostly because it annoys me. People who say things like “propensity score matching should never be used, just see &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;” piss me off, because it is clear they only read the title of the paper or at best the abstract and decided to believe it. To be fair, this is one of the most click-baity titles in the propensity score literature, written by a researcher sometimes known for his &lt;a href=&#34;https://doi.org/10.1093/pan/mpu015&#34;&gt;click-baity titles&lt;/a&gt;. But the article, of course, makes a far more nuanced point, one which is basically impossible to disagree with but which is the opposite of what the title says: propensity score matching can be useful, but it must be done with thought and care. King and Nielsen don’t trust the unwashed masses with propensity score matching, and frankly, I don’t either. Their solution is to recommend against using it outright, while mine is to educate people on best practices and provide tools for implementing those practices. If you are the kind of person that believes the title of the paper, you are exactly the kind of person the paper was written for.&lt;/p&gt;
&lt;p&gt;This argument, whether stated by King and Nielsen or by other statisticians who have found other methods to perform better than propensity score matching either theoretically or in simulations, is a purely statistical argument. It has nothing to do with causal inference or its assumptions and nothing to do with matching vs regression or matching vs instrumental variables or DiD. It is a very specific argument that takes for granted that strong ignorability is true and design-based methods are superior and matching is to be done instead of weighting. If you disagree with any of those points, &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; is immediately irrelevant for your argument.&lt;/p&gt;
&lt;p&gt;Here are some other problems I have with this argument.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The problems King and Nielsen identify with propensity score matching can be assessed in one’s dataset; you are not cursed to suffer its ills by considering propensity score matching as a candidate matching specification. If propensity score matching sucks in your dataset, then use another method. The ability to respecify is one of the great advantages of design-based methods and is already recommended in the literature as a best practice. Omitting propensity score matching as a candidate specification means you could be missing out on an adjustment strategy that is effective in your dataset.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The methods King and Nielsen recommend as replacements for propensity score matching tend to perform terribly in most datasets. They recommend Mahalanobis distance matching and coarsened exact matching, both of which are known to perform terribly in the presence of many categorical covariates and which have been found in many studies to perform worse than propensity score matching empirically &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ripolloneEvaluatingUtilityCoarsened2020&#34;&gt;John E. Ripollone et al. 2020&lt;/a&gt;)&lt;/span&gt;. Indeed, some go so far as to argue that coarsened exact matching should never be used &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blackTroubleCoarsenedExact2020&#34;&gt;Black, Lalkiya, and Lerner 2020&lt;/a&gt;)&lt;/span&gt;. Whether Mahalanobis distance matching works better than propensity score matching for a given application depends on the unique features of the dataset. &lt;span class=&#34;citation&#34;&gt;John E. Ripollone et al. (&lt;a href=&#34;#ref-ripolloneImplicationsPropensityScore2018&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, who explicitly respond to King and Nielsen’s argument, find that in one dataset, propensity score matching does better than Mahalanobis distance matching, and in another dataset, the opposite is true.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The specific implementation of propensity score matching studied by King and Nielsen is not the only one. One of their key arguments is that the propensity score matching induces the propensity score paradox, where progressively decreasing the width of the caliper makes balance worse at some point, despite making units closer on the propensity score. But not all implementations of propensity score matching involve a caliper. And even when they do, the propensity score paradox doesn’t always kick in, and even when it does, it can be assessed and avoided by using a different caliper. The propensity score paradox is only harmful to the thoughtless researcher who blindly treats propensity score matching as a rote procedure expected to perform well, ignoring all best practices.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It might seem like I’m defending propensity score matching. I’m not. Propensity score matching is uniformly worse than some other matching methods. My problem is with people who use this paper as a blunt critique of any application of propensity score matching, whether the paper applies or not. And even when used in the right context, the paper’s conclusions are not to avoid propensity score matching; they are to use propensity score matching carefully, looking out for the specific problems identified in the paper.&lt;/p&gt;
&lt;p&gt;What should you do? If you are wedded to matching, try propensity score matching, and try other methods. I guarantee you will find a method better than propensity score matching for your dataset. That doesn’t mean propensity score matching should never be used, and it doesn’t mean that any study that used propensity score matching should be dismissed outright (at least for this reason; if you think strong ignorability is untenable, then critique the paper on those terms).&lt;/p&gt;
&lt;p&gt;Does that mean this paper is useless? No. Here is how to correctly use &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; to critique an application of propensity score matching:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Did the authors use a caliper without assessing balance on a propensity score matching specification that avoided a caliper? If so, then it is possible the propensity score paradox was in effect and they made things worse by using a caliper. Ask the authors to verify and demonstrate that the caliper not only did not make balance worse but also was necessary to achieve balance. You can use &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; to justify this critique.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Did the authors use propensity score matching without making any attempt to guarantee close pairs on the covariates? If so, then it is possible they missed out on a better performing, more robust method of matching that reduces model dependence by being more precise. Examples of such methods include genetic matching, pair matching with exact matching constraints or calipers on the &lt;em&gt;covariates&lt;/em&gt;, or Mahalanobis distance matching in a cardinality matched sample or within a propensity score calipers. If the authors’ only attempt at achieving balance was to use propensity score matching with no other modifications, then you can use &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; to justify the critique that their matching specification may be poor and recommend the authors try a different matching method that prioritizes close pairs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But if the authors used propensity score matching, did work to demonstrate that propensity score matching was optimal in their dataset, demonstrated that covariate balance was achieved in a deep and holistic way, and incorporated constraints to increase the closeness of pairs on the covariates, then you should not use &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; to critique the application.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-abadieLargeSampleProperties2006&#34; class=&#34;csl-entry&#34;&gt;
Abadie, Alberto, and Guido W. Imbens. 2006. &lt;span&gt;“Large Sample Properties of Matching Estimators for Average Treatment Effects.”&lt;/span&gt; &lt;em&gt;Econometrica&lt;/em&gt; 74 (1): 235–67. &lt;a href=&#34;https://doi.org/10.1111/j.1468-0262.2006.00655.x&#34;&gt;https://doi.org/10.1111/j.1468-0262.2006.00655.x&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-abadieBiasCorrectedMatchingEstimators2011&#34; class=&#34;csl-entry&#34;&gt;
———. 2011. &lt;span&gt;“Bias-Corrected Matching Estimators for Average Treatment Effects.”&lt;/span&gt; &lt;em&gt;Journal of Business &amp;amp; Economic Statistics&lt;/em&gt; 29 (1): 1–11. &lt;a href=&#34;https://doi.org/10.1198/jbes.2009.07333&#34;&gt;https://doi.org/10.1198/jbes.2009.07333&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ben-michaelAugmentedSyntheticControl2021&#34; class=&#34;csl-entry&#34;&gt;
Ben-Michael, Eli, Avi Feller, and Jesse Rothstein. 2021. &lt;span&gt;“The Augmented Synthetic Control Method.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 116 (536): 1789–1803. &lt;a href=&#34;https://doi.org/10.1080/01621459.2021.1929245&#34;&gt;https://doi.org/10.1080/01621459.2021.1929245&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-blackTroubleCoarsenedExact2020&#34; class=&#34;csl-entry&#34;&gt;
Black, Bernard S., Parth Lalkiya, and Joshua Y. Lerner. 2020. &lt;span&gt;“The Trouble with Coarsened Exact Matching.”&lt;/span&gt; &lt;em&gt;SSRN Electronic Journal&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.2139/ssrn.3694749&#34;&gt;https://doi.org/10.2139/ssrn.3694749&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-bruns-smithAugmentedBalancingWeights2023&#34; class=&#34;csl-entry&#34;&gt;
Bruns-Smith, David, Oliver Dukes, Avi Feller, and Elizabeth L. Ogburn. n.d. &lt;span&gt;“Augmented Balancing Weights as Linear Regression.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-chattopadhyayImpliedWeightsLinear2022a&#34; class=&#34;csl-entry&#34;&gt;
Chattopadhyay, Ambarish, and José R Zubizarreta. 2022. &lt;span&gt;“On the Implied Weights of Linear Regression for Causal Inference.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt;, October, asac058. &lt;a href=&#34;https://doi.org/10.1093/biomet/asac058&#34;&gt;https://doi.org/10.1093/biomet/asac058&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-cohnProfileMatchingGeneralization2022&#34; class=&#34;csl-entry&#34;&gt;
Cohn, Eric R., and José R. Zubizarreta. 2022. &lt;span&gt;“Profile Matching for the Generalization and Personalization of Causal Inferences.”&lt;/span&gt; &lt;em&gt;Epidemiology&lt;/em&gt; 33 (5): 678. &lt;a href=&#34;https://doi.org/10.1097/EDE.0000000000001517&#34;&gt;https://doi.org/10.1097/EDE.0000000000001517&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-coleConstructingInverseProbability2008&#34; class=&#34;csl-entry&#34;&gt;
Cole, Stephen R., and Miguel A Hernán. 2008. &lt;span&gt;“Constructing Inverse Probability Weights for Marginal Structural Models.”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 168 (6): 656–64. &lt;a href=&#34;https://doi.org/10.1093/aje/kwn164&#34;&gt;https://doi.org/10.1093/aje/kwn164&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-diamondGeneticMatchingEstimating2013&#34; class=&#34;csl-entry&#34;&gt;
Diamond, Alexis, and Jasjeet S. Sekhon. 2013. &lt;span&gt;“Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.”&lt;/span&gt; &lt;em&gt;Review of Economics and Statistics&lt;/em&gt; 95 (3): 932945. &lt;a href=&#34;https://doi.org/10.1162/REST_a_00318&#34;&gt;https://doi.org/10.1162/REST_a_00318&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-dorieAutomatedDoityourselfMethods2019&#34; class=&#34;csl-entry&#34;&gt;
Dorie, Vincent, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. 2019. &lt;span&gt;“Automated Versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition.”&lt;/span&gt; &lt;em&gt;Statistical Science&lt;/em&gt; 34 (1): 43–68. &lt;a href=&#34;https://doi.org/10.1214/18-STS667&#34;&gt;https://doi.org/10.1214/18-STS667&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-greiferMatchingMethodsConfounder2021a&#34; class=&#34;csl-entry&#34;&gt;
Greifer, Noah, and Elizabeth A Stuart. 2021. &lt;span&gt;“Matching Methods for Confounder Adjustment: An Addition to the Epidemiologist&lt;span&gt;’&lt;/span&gt;s Toolbox.”&lt;/span&gt; &lt;em&gt;Epidemiologic Reviews&lt;/em&gt;, June, mxab003. &lt;a href=&#34;https://doi.org/10.1093/epirev/mxab003&#34;&gt;https://doi.org/10.1093/epirev/mxab003&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hainmuellerEntropyBalancingCausal2012&#34; class=&#34;csl-entry&#34;&gt;
Hainmueller, J. 2012. &lt;span&gt;“Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.”&lt;/span&gt; &lt;em&gt;Political Analysis&lt;/em&gt; 20 (1): 25–46. &lt;a href=&#34;https://doi.org/10.1093/pan/mpr025&#34;&gt;https://doi.org/10.1093/pan/mpr025&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hansenOptimalFullMatching2006&#34; class=&#34;csl-entry&#34;&gt;
Hansen, Ben B, and Stephanie Olsen Klopfer. 2006. &lt;span&gt;“Optimal Full Matching and Related Designs via Network Flows.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 15 (3): 609–27. &lt;a href=&#34;https://doi.org/10.1198/106186006X137047&#34;&gt;https://doi.org/10.1198/106186006X137047&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hernanCausalInferenceWhat2020&#34; class=&#34;csl-entry&#34;&gt;
Hernán, Miguel A, and James M Robins. 2020. &lt;em&gt;Causal Inference: What If&lt;/em&gt;. Boca Raton: Chapman &amp;amp; Hall/CRC. &lt;a href=&#34;https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2020/01/ci_hernanrobins_21jan20.pdf&#34;&gt;https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2020/01/ci_hernanrobins_21jan20.pdf&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hoMatchingNonparametricPreprocessing2007&#34; class=&#34;csl-entry&#34;&gt;
Ho, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007. &lt;span&gt;“Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference.”&lt;/span&gt; &lt;em&gt;Political Analysis&lt;/em&gt; 15 (3): 199–236. &lt;a href=&#34;https://doi.org/10.1093/pan/mpl013&#34;&gt;https://doi.org/10.1093/pan/mpl013&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-huEstimationCausalEffects2020a&#34; class=&#34;csl-entry&#34;&gt;
Hu, Liangyuan, Chenyang Gu, Michael Lopez, Jiayi Ji, and Juan Wisnivesky. 2020. &lt;span&gt;“Estimation of Causal Effects of Multiple Treatments in Observational Studies with a Binary Outcome.”&lt;/span&gt; &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt; 29 (11): 3218–34. &lt;a href=&#34;https://doi.org/10.1177/0962280220921909&#34;&gt;https://doi.org/10.1177/0962280220921909&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hulingEnergyBalancingCovariate2022&#34; class=&#34;csl-entry&#34;&gt;
Huling, Jared D., and Simon Mak. n.d. &lt;span&gt;“Energy Balancing of Covariate Distributions.”&lt;/span&gt; &lt;a href=&#34;https://doi.org/10.48550/arXiv.2004.13962&#34;&gt;https://doi.org/10.48550/arXiv.2004.13962&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-imaiCovariateBalancingPropensity2014&#34; class=&#34;csl-entry&#34;&gt;
Imai, Kosuke, and Marc Ratkovic. 2014. &lt;span&gt;“Covariate Balancing Propensity Score.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 76 (1): 243263. &lt;a href=&#34;https://doi.org/10.1111/rssb.12027&#34;&gt;https://doi.org/10.1111/rssb.12027&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kallbergLargeSampleProperties2022&#34; class=&#34;csl-entry&#34;&gt;
Källberg, David, and Ingeborg Waernbaum. 2022. &lt;span&gt;“Large Sample Properties of Entropy Balancing Estimators of Average Causal Effects.”&lt;/span&gt; &lt;em&gt;arXiv:2204.10623 [Stat]&lt;/em&gt;, April. &lt;a href=&#34;http://arxiv.org/abs/2204.10623&#34;&gt;http://arxiv.org/abs/2204.10623&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-keeleComparingCovariatePrioritization2018&#34; class=&#34;csl-entry&#34;&gt;
Keele, Luke, and Dylan Small. 2018. &lt;span&gt;“Comparing Covariate Prioritization via Matching to Machine Learning Methods for Causal Inference Using Five Empirical Applications.”&lt;/span&gt; &lt;em&gt;arXiv:1805.03743 [Stat]&lt;/em&gt;, May. &lt;a href=&#34;http://arxiv.org/abs/1805.03743&#34;&gt;http://arxiv.org/abs/1805.03743&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kingWhyPropensityScores2019&#34; class=&#34;csl-entry&#34;&gt;
King, Gary, and Richard Nielsen. 2019. &lt;span&gt;“Why Propensity Scores Should Not Be Used for Matching.”&lt;/span&gt; &lt;em&gt;Political Analysis&lt;/em&gt;, May, 1–20. &lt;a href=&#34;https://doi.org/10.1017/pan.2019.11&#34;&gt;https://doi.org/10.1017/pan.2019.11&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kushCovariateBalanceObservational2022&#34; class=&#34;csl-entry&#34;&gt;
Kush, Joseph M., Elise T. Pas, Rashelle J. Musci, and Catherine P. Bradshaw. 2022. &lt;span&gt;“Covariate Balance for Observational Effectiveness Studies: A Comparison of Matching and Weighting.”&lt;/span&gt; &lt;em&gt;Journal of Research on Educational Effectiveness&lt;/em&gt; 0 (0): 1–24. &lt;a href=&#34;https://doi.org/10.1080/19345747.2022.2110545&#34;&gt;https://doi.org/10.1080/19345747.2022.2110545&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-linEstimationBasedNearest2021&#34; class=&#34;csl-entry&#34;&gt;
Lin, Zhexiao, Peng Ding, and Fang Han. 2021. &lt;span&gt;“Estimation Based on Nearest Neighbor Matching: From Density Ratio to Average Treatment Effect.”&lt;/span&gt; &lt;em&gt;arXiv:2112.13506 [Econ, Math, Stat]&lt;/em&gt;, December. &lt;a href=&#34;http://arxiv.org/abs/2112.13506&#34;&gt;http://arxiv.org/abs/2112.13506&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-maoPropensityScoreWeighting2018&#34; class=&#34;csl-entry&#34;&gt;
Mao, Huzhang, Liang Li, and Tom Greene. 2018. &lt;span&gt;“Propensity Score Weighting Analysis and Treatment Effect Discovery.”&lt;/span&gt; &lt;em&gt;Statistical Methods in Medical Research&lt;/em&gt;, June, 096228021878117. &lt;a href=&#34;https://doi.org/10.1177/0962280218781171&#34;&gt;https://doi.org/10.1177/0962280218781171&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-matthayAlternativeCausalInference2020&#34; class=&#34;csl-entry&#34;&gt;
Matthay, Ellicott C., Erin Hagan, Laura M. Gottlieb, May Lynn Tan, David Vlahov, Nancy E. Adler, and M. Maria Glymour. 2020. &lt;span&gt;“Alternative Causal Inference Methods in Population Health Research: Evaluating Tradeoffs and Triangulating Evidence.”&lt;/span&gt; &lt;em&gt;SSM - Population Health&lt;/em&gt; 10 (April): 100526. &lt;a href=&#34;https://doi.org/10.1016/j.ssmph.2019.100526&#34;&gt;https://doi.org/10.1016/j.ssmph.2019.100526&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-mccaffreyPropensityScoreEstimation2004&#34; class=&#34;csl-entry&#34;&gt;
McCaffrey, Daniel F., Greg Ridgeway, and Andrew R. Morral. 2004. &lt;span&gt;“Propensity Score Estimation With Boosted Regression for Evaluating Causal Effects in Observational Studies.”&lt;/span&gt; &lt;em&gt;Psychological Methods&lt;/em&gt; 9 (4): 403–25. &lt;a href=&#34;https://doi.org/10.1037/1082-989X.9.4.403&#34;&gt;https://doi.org/10.1037/1082-989X.9.4.403&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ripolloneImplicationsPropensityScore2018&#34; class=&#34;csl-entry&#34;&gt;
Ripollone, John E., Krista F. Huybrechts, Kenneth J. Rothman, Ryan E. Ferguson, and Jessica M. Franklin. 2018. &lt;span&gt;“Implications of the Propensity Score Matching Paradox in Pharmacoepidemiology.”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 187 (9): 1951–61. &lt;a href=&#34;https://doi.org/10.1093/aje/kwy078&#34;&gt;https://doi.org/10.1093/aje/kwy078&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-ripolloneEvaluatingUtilityCoarsened2020&#34; class=&#34;csl-entry&#34;&gt;
Ripollone, John E, Krista F Huybrechts, Kenneth J Rothman, Ryan E Ferguson, and Jessica M Franklin. 2020. &lt;span&gt;“Evaluating the Utility of Coarsened Exact Matching for Pharmacoepidemiology Using Real and Simulated Claims Data.”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 189 (6): 613–22. &lt;a href=&#34;https://doi.org/10.1093/aje/kwz268&#34;&gt;https://doi.org/10.1093/aje/kwz268&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaumCentralRolePropensity1983&#34; class=&#34;csl-entry&#34;&gt;
Rosenbaum, Paul R., and Donald B. Rubin. 1983. &lt;span&gt;“The Central Role of the Propensity Score in Observational Studies for Causal Effects.”&lt;/span&gt; &lt;em&gt;Biometrika&lt;/em&gt; 70 (1): 41–55. &lt;a href=&#34;https://doi.org/10.1093/biomet/70.1.41&#34;&gt;https://doi.org/10.1093/biomet/70.1.41&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rothmanModernEpidemiology2021&#34; class=&#34;csl-entry&#34;&gt;
Rothman, Kenneth J., Timothy L. Lash, Tyler J. VanderWeele, and Sebastien Haneuse. 2021. &lt;em&gt;Modern Epidemiology&lt;/em&gt;. Fourth edition. Philadelphia: Wolters Kluwer.
&lt;/div&gt;
&lt;div id=&#34;ref-rubinUseMatchedSampling1973&#34; class=&#34;csl-entry&#34;&gt;
Rubin, Donald B. 1973. &lt;span&gt;“The Use of Matched Sampling and Regression Adjustment to Remove Bias in Observational Studies.”&lt;/span&gt; &lt;em&gt;Biometrics&lt;/em&gt; 29 (1): 185–203. &lt;a href=&#34;https://doi.org/10.2307/2529685&#34;&gt;https://doi.org/10.2307/2529685&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rubinDesignAnalysisObservational2007&#34; class=&#34;csl-entry&#34;&gt;
———. 2007. &lt;span&gt;“The Design Versus the Analysis of Observational Studies for Causal Effects: Parallels with the Design of Randomized Trials.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 26 (1): 20–36. https://doi.org/&lt;a href=&#34;https://doi.org/10.1002/sim.2739&#34;&gt;https://doi.org/10.1002/sim.2739&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rubinCombiningPropensityScore2000&#34; class=&#34;csl-entry&#34;&gt;
Rubin, Donald B., and Neal Thomas. 2000. &lt;span&gt;“Combining Propensity Score Matching with Additional Adjustments for Prognostic Covariates.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 95 (450): 573–85. &lt;a href=&#34;https://doi.org/10.1080/01621459.2000.10474233&#34;&gt;https://doi.org/10.1080/01621459.2000.10474233&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuartUsingFullMatching2008&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A., and Kerry M. Green. 2008. &lt;span&gt;“Using Full Matching to Estimate Causal Effects in Nonexperimental Studies: Examining the Relationship Between Adolescent Marijuana Use and Adult Outcomes.”&lt;/span&gt; &lt;em&gt;Developmental Psychology&lt;/em&gt;, New methods for new questions in developmental psychology, 44 (2): 395–406. &lt;a href=&#34;https://doi.org/10.1037/0012-1649.44.2.395&#34;&gt;https://doi.org/10.1037/0012-1649.44.2.395&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-vansteelandtInvitedCommentaryGComputation2011&#34; class=&#34;csl-entry&#34;&gt;
Vansteelandt, Stijn, and Niels Keiding. 2011. &lt;span&gt;“Invited Commentary: G-Computation&lt;span&gt;&lt;/span&gt;lost in Translation?”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 173 (7): 739–42. &lt;a href=&#34;https://doi.org/10.1093/aje/kwq474&#34;&gt;https://doi.org/10.1093/aje/kwq474&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-waernbaumModelMisspecificationRobustness2012&#34; class=&#34;csl-entry&#34;&gt;
Waernbaum, Ingeborg. 2012. &lt;span&gt;“Model Misspecification and Robustness in Causal Inference: Comparing Matching with Doubly Robust Estimation.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 31 (15): 1572–81. &lt;a href=&#34;https://doi.org/10.1002/sim.4496&#34;&gt;https://doi.org/10.1002/sim.4496&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-williamsonVarianceReductionRandomised2014&#34; class=&#34;csl-entry&#34;&gt;
Williamson, Elizabeth J., Andrew B. Forbes, and Ian R. White. 2014. &lt;span&gt;“Variance Reduction in Randomised Trials by Inverse Probability Weighting Using the Propensity Score.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 33 (5): 721–37. &lt;a href=&#34;https://doi.org/10.1002/sim.5991&#34;&gt;https://doi.org/10.1002/sim.5991&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-zubizarretaStableWeightsThat2015&#34; class=&#34;csl-entry&#34;&gt;
Zubizarreta, José R. 2015. &lt;span&gt;“Stable Weights That Balance Covariates for Estimation with Incomplete Outcome Data.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 110 (511): 910–22. &lt;a href=&#34;https://doi.org/10.1080/01621459.2015.1023805&#34;&gt;https://doi.org/10.1080/01621459.2015.1023805&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-zubizarretaMatchingBalancePairing2014&#34; class=&#34;csl-entry&#34;&gt;
Zubizarreta, José R., Ricardo D. Paredes, and Paul R. Rosenbaum. 2014. &lt;span&gt;“Matching for Balance, Pairing for Heterogeneity in an Observational Study of the Effectiveness of for-Profit and Not-for-Profit High Schools in Chile.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 8 (1): 204–31. &lt;a href=&#34;https://doi.org/10.1214/13-AOAS713&#34;&gt;https://doi.org/10.1214/13-AOAS713&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matching Weights are Propensity Score Weights</title>
      <link>https://ngreifer.github.io/blog/matching-weights/</link>
      <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/matching-weights/</guid>
      <description>


&lt;p&gt;I’m &lt;a href=&#34;https://github.com/kosukeimai/MatchIt/issues/155&#34;&gt;often&lt;/a&gt; &lt;a href=&#34;https://stats.stackexchange.com/q/536197/116195&#34;&gt;asked&lt;/a&gt; how the matching weights produced by &lt;code&gt;MatchIt&lt;/code&gt; are computed. The weights are necessary for estimating the treatment effect in the matched sample; indeed, the weights &lt;em&gt;determine&lt;/em&gt; the matched sample. While the weights for simple methods like 1:1 matching are straightforward (i.e., 1 if matched and 0 if unmatched), for more complicated scenarios, like full matching, matching with replacement, and variable ratio matching, the weights take on variable values and are critical to include in the analysis of the matched dataset. For example, full matching doesn’t discard any units (by default), but failing to include the matching weights in the estimation of the treatment effect would be like doing no matching at all.&lt;/p&gt;
&lt;p&gt;There is very little guidance in the literature on how to compute matching weights. We have a few clues that have been scattered across different fields, but they have yet to describe a unifying method of computing these weights&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In this post, I’ll describe how matching weights are computed in &lt;code&gt;MatchIt&lt;/code&gt; and how this unifying method relates to the few strategies described in the literature.&lt;/p&gt;
&lt;p&gt;The main theses of this post are that &lt;strong&gt;matching is a nonparametric method for estimating propensity scores&lt;/strong&gt;, and &lt;strong&gt;matching weights are propensity score weights&lt;/strong&gt;. This framework unifies existing approaches for computing weights after matching, applies to all forms of matching (including &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:1 matching, full matching, and stratification), and is straightforward to implement.&lt;/p&gt;
&lt;div id=&#34;matching-as-nonparametric-estimation-of-propensity-scores&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matching as Nonparametric Estimation of Propensity Scores&lt;/h2&gt;
&lt;p&gt;The first step in understanding how matching weights are computed is to consider how matching is a nonparametric estimator of the propensity score. When I talk about matching here, I’m really talking about the assignment of units into pairs, strata, or matched sets &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-greiferMatchingMethodsConfounder2021a&#34; role=&#34;doc-biblioref&#34;&gt;Greifer and Stuart 2021&lt;/a&gt;)&lt;/span&gt;. For example, 1:1 pair matching assigns treated and control units into pairs, each with one treated and one control unit. Optimal full matching assigns all units into matched sets, each with either exactly one treated unit and one or more control units or with exactly one control units and one or more treated units &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hansenOptimalFullMatching2006&#34; role=&#34;doc-biblioref&#34;&gt;Hansen and Klopfer 2006&lt;/a&gt;)&lt;/span&gt;. Propensity score subclassification and coarsened exact matching assign units into strata based on their values of the propensity score or covariates, respectively &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rosenbaumReducingBiasObservational1984&#34; role=&#34;doc-biblioref&#34;&gt;Rosenbaum and Rubin 1984&lt;/a&gt;; &lt;a href=&#34;#ref-iacusCausalInferenceBalance2012&#34; role=&#34;doc-biblioref&#34;&gt;Iacus, King, and Porro 2012&lt;/a&gt;)&lt;/span&gt;. I do want to note that matching with replacement is a slightly different beast, so I will save my discussion of it till later, though how it fits into this framework is straightforward.&lt;/p&gt;
&lt;div id=&#34;computing-stratum-propensity-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Computing stratum propensity scores&lt;/h3&gt;
&lt;p&gt;For matched units, we can compute a new “stratum” propensity score, &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i\)&lt;/span&gt; as &lt;span class=&#34;math display&#34;&gt;\[
\hat{e}^*_i = P(A = 1|S=s_i)
\]&lt;/span&gt;where &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the treatment (0 for control, 1 for treated) and &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is stratum/pair membership indexed by strata &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. Put in words, the &lt;strong&gt;stratum propensity score&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i\)&lt;/span&gt; &lt;strong&gt;for each member of a matched stratum is the proportion of treated units in that stratum&lt;/strong&gt;. We can also write this formula as &lt;span class=&#34;math display&#34;&gt;\[
\hat{e}^*_i = \frac{n_{1s_i}}{n_{s_i}}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n_{1s_i}\)&lt;/span&gt; is the number of treated units in stratum &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{s_i}\)&lt;/span&gt; is the total number of units in stratum &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i\)&lt;/span&gt; is distinct from the usual propensity score, &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}_i = P(A=1|X = x_i)\)&lt;/span&gt;, which is estimated from the treatment and covariates using, e.g., logistic regression or a machine learning model. &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}_i\)&lt;/span&gt; may be used to perform the matching or subclassification, but it is &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i\)&lt;/span&gt;, the subject of this post, that arises &lt;em&gt;from&lt;/em&gt; matching or subclassification. It is critical to keep these two propensity scores distinct; one is used to match (&lt;span class=&#34;math inline&#34;&gt;\(\hat{e}_i\)&lt;/span&gt;), and the other results from matching (&lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;This method of estimating stratum propensity scores is nonparametric in the sense that no model is used and no functional form assumption are made, once units have been assigned into strata. It may be that a model was used to assign units into strata (e.g., when matching or subclassifying based on a propensity score estimated with a model of treatment given the covariates), but, given the matching, this new propensity score is nonparametric. It is agnostic to how the matching was done.&lt;/p&gt;
&lt;p&gt;Intuitively, we can think of stratum membership as a proxy for the covariates that are usually included in a propensity score specification. That is, if all units in a stratum have the same values of the covariates, conditioning on stratum membership is the same as conditioning on the covariates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Examples&lt;/h3&gt;
&lt;p&gt;In full matching, we may have some matched sets that have, for example, 1 treated unit and 7 control units. All units in that stratum would receive a stratum propensity score of &lt;span class=&#34;math inline&#34;&gt;\(1/8\)&lt;/span&gt;. We might have another matched set that has 5 treated units and 1 control unit. All units in that stratum would receive a stratum propensity score of &lt;span class=&#34;math inline&#34;&gt;\(5/6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In 1:1 matching, the situation is more trivial; all matched units, which are each in strata with 1 treated unit and 1 control unit, receive a stratum propensity score of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In propensity score subclassification, we might have more than one unit from each treatment group; for example, a propensity score quintile might contain 56 treated units and 73 control units; all units in that subclass would receive stratum propensity scores of &lt;span class=&#34;math inline&#34;&gt;\(56/129\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matching-weights-as-propensity-score-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matching Weights as Propensity Score Weights&lt;/h2&gt;
&lt;p&gt;To get matching weights from the stratum propensity scores, we can simply apply the usual propensity score weighting formulas that correspond to the desired estimand to these propensity scores. As reminder, the formulas for weights given a generic propensity score &lt;span class=&#34;math inline&#34;&gt;\(e_i\)&lt;/span&gt; are &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
w_{ATE}&amp;amp;=\frac{A_i}{e_i} + \frac{1-A}{1-e_i} \\
w_{ATT}&amp;amp;=A_i + (1-A_i)\frac{e_i}{1-e_i} = e_i \times w_{ATE}\\
w_{ATC}&amp;amp;=A_i \frac{1-e_i}{e_i} + (1-A) = (1-e_i)\times w_{ATE}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(Note: we also sometimes “stabilize” the weights by multiplying them by the stabilization factor &lt;span class=&#34;math inline&#34;&gt;\(A_iP(A+1) + (1-A_i)P(A=0)\)&lt;/span&gt;; this will come up later.) So, we simply feed stratum propensity scores &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i\)&lt;/span&gt; into these formulas, and that’s how we get the matching weights. To my knowledge, this procedure has never been described in the literature. I included it in the &lt;code&gt;MatchIt&lt;/code&gt; documentation once I became its maintainer starting with version 4.0.0.&lt;/p&gt;
&lt;p&gt;Let’s see these weights in action:&lt;/p&gt;
&lt;p&gt;For full matching and propensity score subclassification, one has the choice between the ATE, ATT, or ATC. This choice doesn’t necessarily affect the matching&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, though it does affect how the weights are computed. Each unit receives a stratum propensity score based on their stratum membership, which will likely vary across strata (otherwise the matching is not functioning right). That stratum propensity score is then used to compute the matching weights.&lt;/p&gt;
&lt;p&gt;For the ATT, treated units receive a weight of 1, and control units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{\hat{e}^*_i}{1-\hat{e}^*_i} = \frac{\frac{n_{1s_i}}{n_{s_i}}}{1-\frac{n_{1s_i}}{n_{s_i}}}=\frac{n_{1s_i}}{n_{s_i}-n_{1s_i}}=\frac{n_{1s_i}}{n_{0s_i}}\)&lt;/span&gt;. That is, control units receive a weight equal to ratio of treated units to control units in their stratum.&lt;/p&gt;
&lt;p&gt;For the ATE, treated units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\hat{e}^*_i}=\frac{n_{s_i}}{n_{1s_i}}\)&lt;/span&gt; and control units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{1-\hat{e}^*_i}=\frac{n_{s_i}}{n_{0s_i}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For 1:1 matching for the ATT, the case is much simpler. We expect both treated and matched control units to receive a weight of 1; we’ll see that applying the formulas does indeed yield this result. Remember that for all matched units, &lt;span class=&#34;math inline&#34;&gt;\(\hat{e}^*_i=1/2\)&lt;/span&gt; because each pair has 1 treated unit and 1 control unit. Using the ATT formula, treated units receive a weight of 1 and control units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{\hat{e}^*_i}{1-\hat{e}^*_i}=\frac{\frac{1}{2}}{1-\frac{1}{2}}=1\)&lt;/span&gt;, as we expected. So, even for this simple case, applying the single unifying formula produces the expected results&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;matching-with-replacement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matching with replacement&lt;/h3&gt;
&lt;p&gt;Earlier, I alluded to the fact that matching with replacement works the same way but with some slight variation. In matching with replacement for the ATT, each control unit may be part of more than one matched pair. For example, our resulting matching matrix for 3:1 matching with replacement and a caliper might look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Treated | Control
--------|--------
      A | C D
      B | C E F&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Control unit &lt;code&gt;C&lt;/code&gt; is matched to both treated units &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;, each of which have a different number of matches (e.g., because of a caliper that restricted the number of matches &lt;code&gt;A&lt;/code&gt; could get).&lt;/p&gt;
&lt;p&gt;Each unit receives a stratum propensity score and matching weight for each time it appears in a match. So, unit &lt;code&gt;C&lt;/code&gt; receives a stratum propensity score of &lt;span class=&#34;math inline&#34;&gt;\(1/3\)&lt;/span&gt; from the first matched set and a stratum propensity score of &lt;span class=&#34;math inline&#34;&gt;\(1/4\)&lt;/span&gt; from the second matched set. We apply the usual weighting formula for the ATT to each stratum propensity score, which gives unit &lt;code&gt;C&lt;/code&gt; a matching of weight of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; for the first matched set and a matching weight of &lt;span class=&#34;math inline&#34;&gt;\(1/3\)&lt;/span&gt; for the second matched set. Finally, we add together the weights (&lt;strong&gt;not&lt;/strong&gt; the stratum propensity scores!) for each unit to get their final weight, which gives unit &lt;code&gt;C&lt;/code&gt; a matching weight of &lt;span class=&#34;math inline&#34;&gt;\(1/2 + 1/3 = 5/6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Matching with replacement for the ATE is not available in &lt;code&gt;MatchIt&lt;/code&gt;, but it is in other software such as the &lt;code&gt;Matching&lt;/code&gt; package and Stata &lt;code&gt;teffects nnmatch&lt;/code&gt;. The way this works is each treated unit receives a matched control unit, and, independently, each control unit receives a matched treated unit. So, you would have a matching matrix for 2:1 matching with replacement and a caliper that would look like the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Treated | Control
--------|--------
      A | C D
      B | C E
--------|--------
Control | Treated
--------|--------
      C | A B
      D | A
      E | B
      F | B&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use a slightly different procedure for calculating the ATE weights that relies on the observation that &lt;span class=&#34;math inline&#34;&gt;\(w_{ATE}=w_{ATT}+w_{ATC}\)&lt;/span&gt;&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. For each unit, we compute the weights where the unit is in the focal group (i.e., the group being matched to) and where the unit is in the nonfocal group (i.e., the group being used as matches), and add them up.&lt;/p&gt;
&lt;p&gt;For example, for unit &lt;code&gt;B&lt;/code&gt;, which is a treated unit, we compute the ATT weights when &lt;code&gt;B&lt;/code&gt; is matched to and the ATC weights when &lt;code&gt;B&lt;/code&gt; is used as a match. &lt;code&gt;B&lt;/code&gt; is matched to by &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;E&lt;/code&gt; (top of the table), so it gets an ATT weight of 1. &lt;code&gt;B&lt;/code&gt; is used as a match for &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;E&lt;/code&gt;, and &lt;code&gt;F&lt;/code&gt;, and gets stratum propensity scores of &lt;span class=&#34;math inline&#34;&gt;\(2/3\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;, and weights of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, respectively. Adding up the ATT weight and the ATC weights, we arrive at a final ATE weight of &lt;span class=&#34;math inline&#34;&gt;\(1 + 1/2+1+1 =7/2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For unit &lt;code&gt;C&lt;/code&gt;, which is a control unit, we compute the ATC weights when &lt;code&gt;C&lt;/code&gt; is matched to and the ATT weights when &lt;code&gt;C&lt;/code&gt; is used as a match. &lt;code&gt;C&lt;/code&gt; is matched to by &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; (bottom of the table), so it gets an ATC weight of 1. &lt;code&gt;C&lt;/code&gt; is used as a matched for &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;, and gets stratum propensity scores of &lt;span class=&#34;math inline&#34;&gt;\(1/3\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1/3\)&lt;/span&gt;, and weights of &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1/2\)&lt;/span&gt;, respectively. Adding up the ATC weight and the ATT weights, we arrive at a final ATE weight of &lt;span class=&#34;math inline&#34;&gt;\(1 + 1/2+1/2=2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matching-weights-in-the-literature&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matching weights in the literature&lt;/h2&gt;
&lt;p&gt;There have been some descriptions of methods to compute weights from matching or stratification in the literature. Here we discuss those approaches and demonstrate how they are related to our unified procedure described above.&lt;/p&gt;
&lt;div id=&#34;marginal-mean-weighting-through-stratification-mmws-hong-2010&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marginal Mean Weighting Through Stratification (MMWS; Hong, 2010)&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Hong (&lt;a href=&#34;#ref-hong2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; describes marginal mean weighting through stratification (MMWS), which is a method of computing weights after propensity score stratification for use in estimating marginal treatment effects (i.e., rather than subclass-specific effects). Hong’s formulas for the MMWS weights are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ATT: Control units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{1s_i}}{n_{0s_i}}\frac{1-\text{pr}(A=1)}{\text{pr}(A=1)}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\text{pr}(A=a)\)&lt;/span&gt; is the overall proportion of units in treatment group &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATE: Units in treatment group &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; receive weights of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{s_i}}{n_{a s_i}}\text{pr}(A=a_i)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above formulas are the same as the matching weights formulas we described above except that they include a scaling factor, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1-\text{pr}(A=1)}{\text{pr}(A=1)}\)&lt;/span&gt; for the ATT weights and &lt;span class=&#34;math inline&#34;&gt;\(\text{pr}(A=a_i)\)&lt;/span&gt; for the ATE weights. The ATE scaling factor is equal to the usual stabilization factor for propensity score weighting. In practice, the scaling factors do not affect balance statistics or the weighted outcome means, and so their inclusion here doesn’t change the properties of the weights&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fine-stratification-weights-desai-et-al.-2017&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fine Stratification Weights (Desai et al., 2017)&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Desai et al. (&lt;a href=&#34;#ref-desai2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; describe a method they call “fine stratification”, which is an alternative to traditional propensity score subclassification and which uses many strata (e.g., close to 100 rather than the traditional 5). &lt;span class=&#34;citation&#34;&gt;Desai and Franklin (&lt;a href=&#34;#ref-desai2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; provide the following formulas for fine stratification weights for the ATT and ATE:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ATT: Control units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{1s_i}}{n_1} / \frac{n_{0s_i}}{n_0}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATE: Treated units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{s_i}}{n} / \frac{n_{1s_i}}{n_1}\)&lt;/span&gt;, and control units receive a weight of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{s_i}}{n} / \frac{n_{0s_i}}{n_0}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Doing a little math reveals that the ATE formula is the same as ours except with a scaling factor of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_a}{n}\)&lt;/span&gt; for units in treatment group &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, which is the same scaling factor used by &lt;span class=&#34;citation&#34;&gt;Hong (&lt;a href=&#34;#ref-hong2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;. The formula for the ATT weights is also the same as ours with the same scaling factor, &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_0}{n_1}\)&lt;/span&gt;, used by &lt;span class=&#34;citation&#34;&gt;Hong (&lt;a href=&#34;#ref-hong2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;averaging-means-across-subclasses-lunceford-and-davidian-2004&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Averaging Means across Subclasses (Lunceford and Davidian, 2004)&lt;/h3&gt;
&lt;p&gt;An early approach for computing treatment effects after propensity score subclassification was to compute the subclass-specific means for each treatment group and then compute a weighted average of those means to arrive at a single mean for each treatment group, where the weights were equal to the number of units in each subclass (for the ATE) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lunceford2004&#34; role=&#34;doc-biblioref&#34;&gt;Lunceford and Davidian 2004&lt;/a&gt;)&lt;/span&gt; or the number of treated units in each subclass (for the ATT) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-stuart2010&#34; role=&#34;doc-biblioref&#34;&gt;Stuart 2010&lt;/a&gt;)&lt;/span&gt;. This procedure is also recommended for estimating effects after full matching &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-stuartUsingFullMatching2008&#34; role=&#34;doc-biblioref&#34;&gt;Stuart and Green 2008&lt;/a&gt;; &lt;a href=&#34;#ref-hansenFullMatchingObservational2004&#34; role=&#34;doc-biblioref&#34;&gt;Hansen 2004&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, for the ATT, the estimated potential outcome mean for control units is equal to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{s\in S}{\frac{n_{1s}}{n_1}\left(\frac{1}{n_{0s}}\sum_{i:s_i=s}1\{A_i=0\}Y_i \right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Doing some rearranging, we find this is equal to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{n_1}\sum_{s\in S}{\left(\sum_{i:s_i=s}\frac{n_{1s_i}}{n_{0s_i}}1\{A_i=1\}Y_i \right)} = \frac{1}{n_1}\sum_{i}{\frac{n_{1s_i}}{n_{0s_i}}1\{A_i=0\}Y_i }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is just the Horvitz-Thompson estimator of the potential outcome mean for the control units under treatment using weights of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{1s_i}}{n_{0s_i}}\)&lt;/span&gt;, which are exactly the ATT weights for control units.&lt;/p&gt;
&lt;p&gt;For the ATE, we have that the estimated potential outcome mean under treatment is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{n}\sum_{s\in S}{n_s\left(\frac{1}{n_{1s}}\sum_{i:s_i=s}1\{A_i=1\}Y_i \right)} = \frac{1}{n}\sum_{i}{\frac{n_{s_i}}{n_{1s_i}}1\{A_i=1\}Y_i }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is the Horvitz-Thompson estimator of the potential outcome mean using weights of &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{s_i}}{n_{1s_i}}\)&lt;/span&gt;, which are exactly the ATE weights for treated units. We find an analogous expression for the control units.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k1-matching-weights-austin-2008&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:1 Matching Weights (Austin, 2008)&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Austin (&lt;a href=&#34;#ref-austin2008&#34; role=&#34;doc-biblioref&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; explains how to assess balance on (variable) &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:1 matched samples matched without replacement. The described procedure involves computing matching weights and using those to compute weighted balance statistics. He only provides formulas for the ATT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ATT: Control units receive a weight equal to the reciprocal of the number of control units in their matched set, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n_{0s_i}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our formula for ATT weights is &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_{1s_i}}{n_{0s_i}}\)&lt;/span&gt;, but in &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:1 matching, &lt;span class=&#34;math inline&#34;&gt;\(n_{1s_i}=1\)&lt;/span&gt; (i.e., there is only one treated unit in each matched set), so Austin’s formula is consistent with ours.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matching-imputation-with-replacement-abadie-and-imbens-2006&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matching Imputation with Replacement (Abadie and Imbens, 2006)&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Abadie and Imbens (&lt;a href=&#34;#ref-abadie2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; describe matching imputation, which is generally equivalent to but conceptually distinct from matching as nonparametric preprocessing. Rather than preprocessing the data by forming a matched sample upon which other analyses take place, matching imputation involves imputing the value of each unit’s missing potential outcome using an average of the observed outcomes of its matched units. &lt;span class=&#34;citation&#34;&gt;Abadie and Imbens (&lt;a href=&#34;#ref-abadie2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;, though, do provide weights that can be used to compute a weighted difference in means that is equal to the matching imputation estimator.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ATT: Each control unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; receives a weight equal to &lt;span class=&#34;math inline&#34;&gt;\(K_M (i)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(K_M (i)=\sum_{l=1}^N{1\{i\in J_M(l)\}\frac{1}{\#J_M(l)}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(J_M(l)\)&lt;/span&gt; is the set of units matched to unit &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\#J_M(l)\)&lt;/span&gt; is the size of that set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATE: Each unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; receives a weight equal to &lt;span class=&#34;math inline&#34;&gt;\(1 + K_M(i)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(K_M (i)\)&lt;/span&gt; as defined above.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These equations are hard to parse, but essentially, &lt;span class=&#34;math inline&#34;&gt;\(1\{i\in J_M(l)\}\frac{1}{\#J_M(l)}\)&lt;/span&gt; should be read as the reciprocal of the number of control units matched to the same treated unit control unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is matched to, and this is summed across all treated units &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; for each control unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. The ATT weights are more closely related to our matching weights: using the equivalence described above for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;:1 matching, each time a control unit is matched, it get a weight equal to the reciprocal of the number of control units in its matched set, and then those weights are summed to arrive at a final weight for that unit. The formulas in &lt;span class=&#34;citation&#34;&gt;Abadie and Imbens (&lt;a href=&#34;#ref-abadie2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; describe that method symbolically. The ATE weights for control units in &lt;span class=&#34;citation&#34;&gt;Abadie and Imbens (&lt;a href=&#34;#ref-abadie2006&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; are just 1 plus the ATT weights, and since ATC weights are equal to 1 for control units, these weights are equivalent to our weights. (The analogous connection works for treated units.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I proposed that matching (including stratification) can be seen as a nonparametric method of estimating propensity scores, and those propensity scores can be used with traditional propensity score weighting formulas to arrive at matching weights. These weights are equivalent to other weights described in the literature, though it seems no other authors have made this explicit connection with such generality. There have been some close calls, though: it is clear &lt;span class=&#34;citation&#34;&gt;Hong (&lt;a href=&#34;#ref-hong2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Desai and Franklin (&lt;a href=&#34;#ref-desai2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; were inspired by the formulas for the ATE and ATT when deriving their subclassification weights, perhaps even making the connection themselves, though without explicitly stating these relationships. &lt;span class=&#34;citation&#34;&gt;Lin, Ding, and Han (&lt;a href=&#34;#ref-lin2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; also connect matching to propensity score weighting by noting that &lt;span class=&#34;math inline&#34;&gt;\(K_M(i)\)&lt;/span&gt; as defined above approaches the ATT propensity score weight for control units and the ATC propensity score weight for treated units.&lt;/p&gt;
&lt;p&gt;Beyond just computing matching weights, this framework suggests new extensions of matching to other estimators that involve propensity scores. For example, general weighting formulas for various weighted estimands as described by &lt;span class=&#34;citation&#34;&gt;F. Li, Morgan, and Zaslavsky (&lt;a href=&#34;#ref-li2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; could be used with these matching weights, expanding the estimable estimands of matching methods (especially subclassification and full matching). There has been no research applying the ATO formulas to stratum propensity scores, but these may prove to enhance the precision of matching estimators. This framework also allows matching methods to be used with estimators that involve the propensity score, such as targeted minimum loss-based estimation (TMLE), augmented inverse probability weighting (AIPW), or g-computation with the propensity score as a covariate, all of which have proven to be highly effective methods for estimating treatment effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-abadie2006&#34; class=&#34;csl-entry&#34;&gt;
Abadie, Alberto, and Guido W. Imbens. 2006. &lt;span&gt;“Large Sample Properties of Matching Estimators for Average Treatment Effects.”&lt;/span&gt; &lt;em&gt;Econometrica&lt;/em&gt; 74 (1): 235–67. &lt;a href=&#34;https://doi.org/10.1111/j.1468-0262.2006.00655.x&#34;&gt;https://doi.org/10.1111/j.1468-0262.2006.00655.x&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-austin2008&#34; class=&#34;csl-entry&#34;&gt;
Austin, Peter C. 2008. &lt;span&gt;“Assessing Balance in Measured Baseline Covariates When Using Many-to-One Matching on the Propensity-Score.”&lt;/span&gt; &lt;em&gt;Pharmacoepidemiology and Drug Safety&lt;/em&gt; 17 (12): 1218–25. &lt;a href=&#34;https://doi.org/10.1002/pds.1674&#34;&gt;https://doi.org/10.1002/pds.1674&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-desai2019&#34; class=&#34;csl-entry&#34;&gt;
Desai, Rishi J., and Jessica M. Franklin. 2019. &lt;span&gt;“Alternative Approaches for Confounding Adjustment in Observational Studies Using Weighting Based on the Propensity Score: A Primer for Practitioners.”&lt;/span&gt; &lt;em&gt;BMJ&lt;/em&gt; 367 (October): l5657. &lt;a href=&#34;https://doi.org/10.1136/bmj.l5657&#34;&gt;https://doi.org/10.1136/bmj.l5657&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-desai2017&#34; class=&#34;csl-entry&#34;&gt;
Desai, Rishi J., Kenneth J. Rothman, Brian .T Bateman, Sonia Hernandez-Diaz, and Krista F. Huybrechts. 2017. &lt;span&gt;“A Propensity-Score-Based Fine Stratification Approach for Confounding Adjustment When Exposure Is Infrequent:”&lt;/span&gt; &lt;em&gt;Epidemiology&lt;/em&gt; 28 (2): 249–57. &lt;a href=&#34;https://doi.org/10.1097/EDE.0000000000000595&#34;&gt;https://doi.org/10.1097/EDE.0000000000000595&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-greiferMatchingMethodsConfounder2021a&#34; class=&#34;csl-entry&#34;&gt;
Greifer, Noah, and Elizabeth A Stuart. 2021. &lt;span&gt;“Matching Methods for Confounder Adjustment: An Addition to the Epidemiologist&lt;span&gt;’&lt;/span&gt;s Toolbox.”&lt;/span&gt; &lt;em&gt;Epidemiologic Reviews&lt;/em&gt;, June, mxab003. &lt;a href=&#34;https://doi.org/10.1093/epirev/mxab003&#34;&gt;https://doi.org/10.1093/epirev/mxab003&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hansenFullMatchingObservational2004&#34; class=&#34;csl-entry&#34;&gt;
Hansen, Ben B. 2004. &lt;span&gt;“Full Matching in an Observational Study of Coaching for the SAT.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 99 (467): 609–18. &lt;a href=&#34;https://doi.org/10.1198/016214504000000647&#34;&gt;https://doi.org/10.1198/016214504000000647&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hansenOptimalFullMatching2006&#34; class=&#34;csl-entry&#34;&gt;
Hansen, Ben B, and Stephanie Olsen Klopfer. 2006. &lt;span&gt;“Optimal &lt;span&gt;Full Matching&lt;/span&gt; and &lt;span&gt;Related Designs&lt;/span&gt; via &lt;span&gt;Network Flows&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 15 (3): 609–27. &lt;a href=&#34;https://doi.org/10.1198/106186006X137047&#34;&gt;https://doi.org/10.1198/106186006X137047&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hong2010&#34; class=&#34;csl-entry&#34;&gt;
Hong, Guanglei. 2010. &lt;span&gt;“Marginal Mean Weighting Through Stratification: Adjustment for Selection Bias in Multilevel Data.”&lt;/span&gt; &lt;em&gt;Journal of Educational and Behavioral Statistics&lt;/em&gt; 35 (5): 499–531. &lt;a href=&#34;https://doi.org/10.3102/1076998609359785&#34;&gt;https://doi.org/10.3102/1076998609359785&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-iacusCausalInferenceBalance2012&#34; class=&#34;csl-entry&#34;&gt;
Iacus, Stefano M., Gary King, and Giuseppe Porro. 2012. &lt;span&gt;“Causal &lt;span&gt;Inference&lt;/span&gt; Without &lt;span&gt;Balance Checking&lt;/span&gt;: &lt;span&gt;Coarsened Exact Matching&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Political Analysis&lt;/em&gt; 20 (1): 1–24. &lt;a href=&#34;https://doi.org/10.1093/pan/mpr013&#34;&gt;https://doi.org/10.1093/pan/mpr013&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-li2018&#34; class=&#34;csl-entry&#34;&gt;
Li, Fan, Kari Lock Morgan, and Alan M. Zaslavsky. 2018. &lt;span&gt;“Balancing Covariates via Propensity Score Weighting.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 113 (521): 390–400. &lt;a href=&#34;https://doi.org/10.1080/01621459.2016.1260466&#34;&gt;https://doi.org/10.1080/01621459.2016.1260466&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-liWeightingAnaloguePair2013&#34; class=&#34;csl-entry&#34;&gt;
Li, Liang, and Tom Greene. 2013. &lt;span&gt;“A Weighting Analogue to Pair Matching in Propensity Score Analysis.”&lt;/span&gt; &lt;em&gt;The International Journal of Biostatistics&lt;/em&gt; 9 (2). &lt;a href=&#34;https://doi.org/10.1515/ijb-2012-0030&#34;&gt;https://doi.org/10.1515/ijb-2012-0030&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lin2021&#34; class=&#34;csl-entry&#34;&gt;
Lin, Zhexiao, Peng Ding, and Fang Han. 2021. &lt;span&gt;“Estimation Based on Nearest Neighbor Matching: From Density Ratio to Average Treatment Effect.”&lt;/span&gt; &lt;em&gt;arXiv:2112.13506 [Econ, Math, Stat]&lt;/em&gt;, December. &lt;a href=&#34;http://arxiv.org/abs/2112.13506&#34;&gt;http://arxiv.org/abs/2112.13506&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-lunceford2004&#34; class=&#34;csl-entry&#34;&gt;
Lunceford, Jared K., and Marie Davidian. 2004. &lt;span&gt;“Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects: A Comparative Study.”&lt;/span&gt; &lt;em&gt;Statistics in Medicine&lt;/em&gt; 23 (19): 29372960. &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/sim.1903/full&#34;&gt;http://onlinelibrary.wiley.com/doi/10.1002/sim.1903/full&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rosenbaumReducingBiasObservational1984&#34; class=&#34;csl-entry&#34;&gt;
Rosenbaum, Paul R., and Donald B. Rubin. 1984. &lt;span&gt;“Reducing &lt;span&gt;Bias&lt;/span&gt; in &lt;span&gt;Observational Studies Using Subclassification&lt;/span&gt; on the &lt;span&gt;Propensity Score&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 79 (387): 516–24. &lt;a href=&#34;https://doi.org/10.2307/2288398&#34;&gt;https://doi.org/10.2307/2288398&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuart2010&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A. 2010. &lt;span&gt;“Matching Methods for Causal Inference: A Review and a Look Forward.”&lt;/span&gt; &lt;em&gt;Statistical Science&lt;/em&gt; 25 (1): 1–21. &lt;a href=&#34;https://doi.org/10.1214/09-STS313&#34;&gt;https://doi.org/10.1214/09-STS313&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuartUsingFullMatching2008&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A., and Kerry M. Green. 2008. &lt;span&gt;“Using Full Matching to Estimate Causal Effects in Nonexperimental Studies: Examining the Relationship Between Adolescent Marijuana Use and Adult Outcomes.”&lt;/span&gt; &lt;em&gt;Developmental Psychology&lt;/em&gt;, New methods for new questions in developmental psychology, 44 (2): 395–406. &lt;a href=&#34;https://doi.org/10.1037/0012-1649.44.2.395&#34;&gt;https://doi.org/10.1037/0012-1649.44.2.395&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note: by matching weights, I mean weights resulting from matching, not the matching weights of &lt;span class=&#34;citation&#34;&gt;L. Li and Greene (&lt;a href=&#34;#ref-liWeightingAnaloguePair2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, which will not be discussed here.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Subclassification often tries to balance the number of treated units across subclasses for the ATT, etc., as recommended by &lt;span class=&#34;citation&#34;&gt;Desai et al. (&lt;a href=&#34;#ref-desai2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Note that we need to add the additional statement that unmatched units have an undefined propensity score and receive weight of 0.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;For treated units: &lt;span class=&#34;math inline&#34;&gt;\(w_{ATE}=\frac{1}{\hat{e}^*_i} = \frac{\hat{e}^*_i + 1-\hat{e}^*_i}{\hat{e}^*_i} = 1 + \frac{1-\hat{e}^*_i}{\hat{e}^*_i}=w_{ATT}+w_{ATC}\)&lt;/span&gt;. For control units: &lt;span class=&#34;math inline&#34;&gt;\(w_{ATE}=\frac{1}{1-\hat{e}^*_i} = \frac{\hat{e}^*_i + 1-\hat{e}^*_i}{1-\hat{e}^*_i} = \frac{\hat{e}^*_i}{1-\hat{e}^*_i}+1=w_{ATT}+w_{ATC}\)&lt;/span&gt;. Or, more simply, &lt;span class=&#34;math inline&#34;&gt;\(w_{ATT} + w_{ATC} = e_i \times w_{ATE} + (1-e_i)\times w_{ATE} = w_{ATE}\)&lt;/span&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;The only time the scaling factor affects the effect estimates are when a model that includes covariates but doesn’t fully interact treatment and covariates is used to estimate the treatment effect.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating Treatment Effects After Weighting with Multiply Imputed Data</title>
      <link>https://ngreifer.github.io/blog/treatment-effects-mi/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/treatment-effects-mi/</guid>
      <description>


&lt;p&gt;Multiply imputed data always makes things a little harder. Essentially, you have to perform each step of the analysis in each imputed dataset and then combine the results together in a special way. For basic regression analysis, the &lt;code&gt;mice&lt;/code&gt; package makes fitting models and combining estimates simple. But when we want to do propensity score matching or weighting before fitting our regression models, and when the quantity we want to estimate is not just a coefficient in a regression model, things get a bit harder.&lt;/p&gt;
&lt;p&gt;For doing matching or weighting in multiply imputed data, the R package &lt;code&gt;{MatchThem}&lt;/code&gt; does the job. It essentially provides wrappers for &lt;code&gt;MatchIt::matchit()&lt;/code&gt; and &lt;code&gt;WeightIt::weightit()&lt;/code&gt; for multiply imputed data. It extends &lt;code&gt;{mice}&lt;/code&gt;’s functionality for fitting regression models in multiply imputed data by automatically incorporating the matched or weighted structure into the estimation of the outcome models. It uses &lt;code&gt;mice::pool()&lt;/code&gt; to pool estimates across multiply imputed data.&lt;/p&gt;
&lt;p&gt;But for estimating treatment effects, it’s often not as simple as using a regression coefficient. If we include covariates in our outcome model but want a marginal effect, we need to use an average marginal effects procedure (i.e., g-computation) to compute it within each imputed dataset, and then combine the results afterward. The &lt;code&gt;{marginaleffects}&lt;/code&gt; package provides a wonderful interface for performing g-computation, but for multiply imputed data, it can require some programming by the analyst. In this guide, I’ll show you how to do that programming to combine treatment effect estimates across multiple imputed datasets.&lt;/p&gt;
&lt;p&gt;An alternative to using &lt;code&gt;{marginaleffects}&lt;/code&gt; is to use the &lt;code&gt;{clarify}&lt;/code&gt; package. &lt;code&gt;{clarify}&lt;/code&gt; can also be used to perform g-computation, but it uses simulation-based inference to compute the uncertainty bounds for the estimate. An advantage of simulation-based inference for multiply imputed data is that combining estimates across imputed datasets is much more straightforward. In this guide, I’ll also show you how to use &lt;code&gt;{clarify}&lt;/code&gt; to combine treatment effect estimates across imputed datasets.&lt;/p&gt;
&lt;div id=&#34;packages-well-need&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages we’ll need&lt;/h3&gt;
&lt;p&gt;We will need the following packages for this demonstration: &lt;code&gt;cobalt&lt;/code&gt;, &lt;code&gt;mice&lt;/code&gt;, &lt;code&gt;MatchThem&lt;/code&gt;, &lt;code&gt;WeightIt&lt;/code&gt;, &lt;code&gt;marginaleffects&lt;/code&gt;, and &lt;code&gt;clarify&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data&lt;/h3&gt;
&lt;p&gt;As usual, we’ll be using a version of the &lt;code&gt;lalonde&lt;/code&gt; dataset. Here will use the &lt;code&gt;lalonde_mis&lt;/code&gt; dataset in &lt;code&gt;{cobalt}&lt;/code&gt;, which has missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;lalonde_mis&amp;quot;, package = &amp;quot;cobalt&amp;quot;)

summary(lalonde_mis)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      treat             age             educ           race        married          nodegree           re74              re75              re78        
##  Min.   :0.0000   Min.   :16.00   Min.   : 0.00   black :243   Min.   :0.0000   Min.   :0.0000   Min.   :    0.0   Min.   :    0.0   Min.   :    0.0  
##  1st Qu.:0.0000   1st Qu.:20.00   1st Qu.: 9.00   hispan: 72   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:    0.0   1st Qu.:    0.0   1st Qu.:  238.3  
##  Median :0.0000   Median :25.00   Median :11.00   white :299   Median :0.0000   Median :1.0000   Median :  984.5   Median :  585.4   Median : 4759.0  
##  Mean   :0.3013   Mean   :27.36   Mean   :10.27                Mean   :0.4158   Mean   :0.6303   Mean   : 4420.2   Mean   : 2170.3   Mean   : 6792.8  
##  3rd Qu.:1.0000   3rd Qu.:32.00   3rd Qu.:12.00                3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 7626.9   3rd Qu.: 3202.0   3rd Qu.:10893.6  
##  Max.   :1.0000   Max.   :55.00   Max.   :18.00                Max.   :1.0000   Max.   :1.0000   Max.   :35040.1   Max.   :25142.2   Max.   :60307.9  
##                                                                NA&amp;#39;s   :20                        NA&amp;#39;s   :40        NA&amp;#39;s   :39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see there are some missing values in &lt;code&gt;married&lt;/code&gt;, &lt;code&gt;re74&lt;/code&gt;, and &lt;code&gt;re75&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;imputing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Imputing the data&lt;/h3&gt;
&lt;p&gt;Here, we’ll use &lt;code&gt;{mice}&lt;/code&gt; to impute the data. Although typically something like 20 imputation is sufficient, for the method &lt;code&gt;{clarify}&lt;/code&gt; uses, it needs way more, so we’ll use 50. We’ll use the default settings, but you should tailor the imputation to fit the needs of your dataset. (I always like to use a machine learning method for my imputations). We’ll also set a seed to ensure replicability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;mice&amp;quot;)
set.seed(12345)
imp &amp;lt;- mice(lalonde_mis, m = 50, printFlag = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mice()&lt;/code&gt; returns a &lt;code&gt;mids&lt;/code&gt; object, which contains the imputed datasets. Although we could extract the datasets using &lt;code&gt;complete()&lt;/code&gt;, we’ll supply this object directly to our function for estimating the propensity score weights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighting-the-imputed-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weighting the imputed data&lt;/h3&gt;
&lt;p&gt;We’ll use &lt;code&gt;MatchThem::weightthem()&lt;/code&gt; to estimate propensity score weights in the imputed datasets. We could also use &lt;code&gt;MatchThem::matchthem()&lt;/code&gt; to do matching; the process is basically identical&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Here we’ll use logistic regression (🤢) to estimate ATT weights to keep things quick and simple.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;MatchThem&amp;quot;)
w.imp &amp;lt;- weightthem(treat ~ age + educ + race + married + nodegree +
                      re74 + re75, data = imp, method = &amp;quot;ps&amp;quot;,
                    estimand = &amp;quot;ATT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s assess balance using &lt;code&gt;{cobalt}&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;cobalt&amp;quot;)
bal.tab(w.imp, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;), abs = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance summary across all imputations
##                 Type Mean.Diff.Adj Max.Diff.Adj Mean.KS.Adj Max.KS.Adj
## prop.score  Distance        0.0235       0.0379      0.1166     0.1327
## age          Contin.        0.1120       0.1343      0.3053     0.3146
## educ         Contin.        0.0352       0.0485      0.0369     0.0412
## race_black    Binary        0.0024       0.0036      0.0024     0.0036
## race_hispan   Binary        0.0003       0.0007      0.0003     0.0007
## race_white    Binary        0.0022       0.0030      0.0022     0.0030
## married       Binary        0.0168       0.0236      0.0168     0.0236
## nodegree      Binary        0.0191       0.0250      0.0191     0.0250
## re74         Contin.        0.0097       0.0281      0.2027     0.2261
## re75         Contin.        0.0075       0.0286      0.1388     0.1648
## 
## Average effective sample sizes across imputations
##                 0   1
## Unadjusted 429.   185
## Adjusted   100.19 185&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Balance could be a bit better on &lt;code&gt;age&lt;/code&gt;, but we’re going to move on because we have things to do.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-outcome-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitting the outcome models&lt;/h3&gt;
&lt;p&gt;Our next step is to fit the outcome model in each imputed dataset. Here, our outcome will be &lt;code&gt;re78 == 0&lt;/code&gt;, i.e., whether a unit’s earnings in 1978 were 0. Ideally, treatment reduces this risk. Although our estimand will be a risk ratio, because we’re doing g-computation, we can fit a model for the outcome that actually makes sense rather than choosing one based on the convenient interpretation of its coefficients. So, we’ll fit a probit outcome model to really hit home that we need a post-estimation method to estimate our quantity of interest and can’t rely on our model coefficients.&lt;/p&gt;
&lt;p&gt;Although &lt;code&gt;{MatchThem}&lt;/code&gt; has functionality for fitting models to the imputed datasets that incorporate the weights, for our purposes, it is better to extract the imputed datasets and fit each model manually in a loop. We’ll use &lt;code&gt;glm()&lt;/code&gt; to do so, though the &lt;code&gt;{MatchThem}&lt;/code&gt; and &lt;code&gt;{WeightIt}&lt;/code&gt; documentation may recommend &lt;code&gt;survey::svyglm()&lt;/code&gt; because it correctly computes the robust standard errors. We’ll do that later using &lt;code&gt;{marginaleffects}&lt;/code&gt; and &lt;code&gt;{clarify}&lt;/code&gt; functions so it’s okay that we don’t do it now. We’ll use a quasi-binomial model because we have weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fits &amp;lt;- lapply(complete(w.imp, &amp;quot;all&amp;quot;), function(d) {
  glm(I(re78 == 0) ~ treat + age + educ + married + race +
        nodegree + re74 + re75, data = d,
      weights = weights, family = quasibinomial(&amp;quot;probit&amp;quot;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to interpret the pooled coefficients from our outcome model (and we had included correct estimation of the standard errors, which we didn’t here), we could use &lt;code&gt;pool(fits) |&amp;gt; summary()&lt;/code&gt; to get them. But none of that is true here so we’ll move on and save the pooling till after we estimate the quantity of interest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-marginaleffects-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;{marginaleffects}&lt;/code&gt; workflow&lt;/h2&gt;
&lt;p&gt;Now we have our list of models. Our next step is to estimate the ATT risk ratio in each one (with the correct standard error) and pool the results. If the only quantity we want is the treatment effect, this is easy. We can use &lt;code&gt;marginaleffects::avg_comparisons()&lt;/code&gt; on each model and then use &lt;code&gt;mice::pool()&lt;/code&gt; to pool the results. In our call to &lt;code&gt;avg_comparisons()&lt;/code&gt;, we need to subset the data used to fit each model to just the treated units and supply this to &lt;code&gt;newdata&lt;/code&gt;, supply the name of the variable containing the weights to &lt;code&gt;wts&lt;/code&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, supply the robust standard error type (HC3) to &lt;code&gt;vcov&lt;/code&gt;, and specify that we want the log risk ratio of the average estimated potential outcomes by supplying &lt;code&gt;&#34;lnratioavg&#34;&lt;/code&gt; to &lt;code&gt;transform_pre&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;marginaleffects&amp;quot;)
comp.imp &amp;lt;- lapply(fits, function(fit) {
  avg_comparisons(fit, newdata = subset(fit$data, treat == 1),
                  variables = &amp;quot;treat&amp;quot;, wts = &amp;quot;weights&amp;quot;, vcov = &amp;quot;HC3&amp;quot;,
                  transform_pre = &amp;quot;lnratioavg&amp;quot;)
})

pooled.comp &amp;lt;- mice::pool(comp.imp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can use &lt;code&gt;summary()&lt;/code&gt; on the resulting object, adding the arguments &lt;code&gt;conf.int = TRUE&lt;/code&gt; to request confidence intervals and &lt;code&gt;exponentiate = TRUE&lt;/code&gt; to get the risk ratio from the log risk ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pooled.comp, conf.int = TRUE,
        exponentiate = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    term              contrast  estimate std.error  statistic       df  p.value    2.5 %   97.5 %
## 1 treat ln(mean(1) / mean(0)) 0.9321569 0.2097534 -0.3349366 610.5055 0.737788 0.617436 1.407298&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find a risk ratio of approximately 0.932, 95% CI: [0.617, 1.407], indicating that in our sample, the risk of having zero earnings in 1978 decreased slightly for those who received treatment, but we don’t have strong evidence for such an effect in the population.&lt;/p&gt;
&lt;p&gt;Although this is nice and simple, things get a bit more complicated when we want to estimate multiple comparisons at the same time, estimate the marginal risks, or perform a more complex analysis. Additional programming is required to make &lt;code&gt;mice::pool()&lt;/code&gt; compatible with these more complex quantities. We’ll demonstrate how to hack &lt;code&gt;{marginaleffects}&lt;/code&gt; to make it work using the instructions in the &lt;code&gt;{marginaleffects}&lt;/code&gt; &lt;a href=&#34;https://vincentarelbundock.github.io/marginaleffects/articles/multiple_imputation.html&#34;&gt;vignette on multiple imputation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We’ll be using &lt;code&gt;avg_predictions()&lt;/code&gt; on each model to compute the marginal risks under each treatment level, which uses a similar syntax to &lt;code&gt;comparisons()&lt;/code&gt;. The challenge comes in that &lt;code&gt;avg_predictions()&lt;/code&gt; produces two rows of output (one for each treatment level), which are not correctly distinguished by &lt;code&gt;mice::pool()&lt;/code&gt;. So, we’ll have to create a new custom class and write a new &lt;code&gt;tidy()&lt;/code&gt; method for our class.&lt;/p&gt;
&lt;p&gt;First, we’ll generate our marginal risks and assign the output our new class, which is arbitrary but which I will call &lt;code&gt;&#34;pred_imp_custom&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred.imp &amp;lt;- lapply(fits, function(fit) {
  out &amp;lt;- avg_predictions(fit, newdata = subset(fit$data, treat == 1),
                         variables = &amp;quot;treat&amp;quot;, wts = &amp;quot;weights&amp;quot;,
                         vcov = &amp;quot;HC3&amp;quot;, by = &amp;quot;treat&amp;quot;)
  
  # the next line assigns our custom class
  class(out) &amp;lt;- c(&amp;quot;pred_imp_custom&amp;quot;, class(out))
  return(out)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll write our new &lt;code&gt;tidy()&lt;/code&gt; method. (Make sure to replace &lt;code&gt;treat&lt;/code&gt; everywhere you see it with the name of your treatment variable.) We won’t actually be using this function at all; it is called internally by &lt;code&gt;mice::pool()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy.pred_imp_custom &amp;lt;- function(x, ...) {
    out &amp;lt;- marginaleffects:::tidy.predictions(x, ...)
    out$term &amp;lt;- paste(&amp;quot;treat =&amp;quot;, out$treat)
    return(out)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can use &lt;code&gt;mice::pool()&lt;/code&gt; and &lt;code&gt;summary()&lt;/code&gt; to get our marginal risks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mice::pool(pred.imp) |&amp;gt; summary(conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        term  estimate  std.error statistic       df      p.value     2.5 %    97.5 %
## 1 treat = 0 0.2607090 0.04264062  6.114100 609.4350 1.734761e-09 0.1769686 0.3444494
## 2 treat = 1 0.2430092 0.03197686  7.599534 611.9484 1.120645e-13 0.1802115 0.3058069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking the ratio of these risks gives us the risk ratio we computed above.&lt;/p&gt;
&lt;p&gt;Note that you have to customize the &lt;code&gt;tidy()&lt;/code&gt; method in a slightly different way when you are estimating treatment effects in subgroups. I’ll leave that as an exercise to the reader, or you can hire me to do it for you :)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-clarify-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;{clarify}&lt;/code&gt; workflow&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;{clarify}&lt;/code&gt; workflow for multiply imputed data is very similar to its workflow for regular data. How simulation-based inference works broadly is that sets of parameters are drawn from a distribution after fitting the model; this distribution is often assumed to be multivariate normal with the mean vector equal to the estimated coefficients and the covariance equal to the asymptotic covariance matrix of the coefficients. Many (e.g., 1000) sets of coefficients are drawn, and a quantity of interest is computed using each set, forming a “posterior” distribution of the quantity of interest. This posterior is then used for inference: its standard deviation can be used as the quantity’s standard error, and its quantiles can be used as confidence intervals. For more information on this methodology, see the &lt;code&gt;{clarify}&lt;/code&gt; &lt;a href=&#34;https://iqss.github.io/clarify/&#34;&gt;website&lt;/a&gt; and its references.&lt;/p&gt;
&lt;p&gt;With multiply imputed data, this process is done for the model fit to each imputed dataset, and then the distributions of the quantities of interest are simply combined to form a single distribution, which is used for inference. In Bayesian terms, this would be called “mixing draws”. The variance of this mixture distribution approaches the variance of the estimate computed using Rubin’s rules when the number of imputations is high.&lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;{clarify}&lt;/code&gt;, we supply the list of fitted models to &lt;code&gt;clarify::misim()&lt;/code&gt;, which draws the coefficients from their implied distributions from each model. We also need to specify the method for computing the covariance matrix (here, using the same HC3 robust covariance we used with &lt;code&gt;{marginaleffects}&lt;/code&gt; to account for the weights). We will only request 200 replications per fitted model since we have 50 imputations, which gives us 10,000 replicates (likely more than enough for stable inference).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;clarify&amp;quot;)

sim.imp &amp;lt;- misim(fits, n = 200, vcov = &amp;quot;HC3&amp;quot;)
sim.imp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A `clarify_misim` object
##  - 10 coefficients, 50 imputations with 200 simulated values each
##  - sampled distributions: multivariate t(604)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note: because we used a quasi-binomial model, a scaled t-distribution was used to draw the coefficients. In practice this will give similar draws to a normal distribution.)&lt;/p&gt;
&lt;p&gt;The output of &lt;code&gt;misim()&lt;/code&gt; is then fed to a function for computing the quantity of interest in each draw; here, we’ll be using &lt;code&gt;clarify::sim_ame()&lt;/code&gt;, which is appropriate for computing marginal risks in a subset of the data (i.e., the ATT risk ratio). We supply the treatment variable to &lt;code&gt;var&lt;/code&gt; and subset the data to just the treated units using &lt;code&gt;subset&lt;/code&gt; to request the ATT. Although we can use the &lt;code&gt;contrast&lt;/code&gt; argument to request the (log) risk ratio, we can compute that afterward quickly from the marginal risks. (Using &lt;code&gt;cl = 3&lt;/code&gt; uses parallel computing with 3 cores but only if you are on a Mac. See the &lt;code&gt;sim_ame()&lt;/code&gt; documentation for more information on how to use the &lt;code&gt;cl&lt;/code&gt; argument.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.att &amp;lt;- sim_ame(sim.imp, var = &amp;quot;treat&amp;quot;,
                   subset = treat == 1, cl = 3,
                   verbose = FALSE)
sim.att&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A `clarify_est` object (from `sim_ame()`)
##  - Average marginal effect of `treat`
##  - 10000 simulated values
##  - 2 quantities estimated:                  
##  E[Y(0)] 0.2605322
##  E[Y(1)] 0.2428401&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute the risk ratio, we can use &lt;code&gt;transform()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.att &amp;lt;- transform(sim.att, RR = `E[Y(1)]`/`E[Y(0)]`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can compute out confidence intervals and p-values around the estimated marginal risks and risk ratio using &lt;code&gt;summary()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sim.att, null = c(RR = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Estimate 2.5 % 97.5 % P-value
## E[Y(0)]    0.261 0.187  0.354       .
## E[Y(1)]    0.243 0.188  0.313       .
## RR         0.932 0.630  1.421    0.76&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we find a risk ratio of approximately 0.932, 95% CI: [0.63, 1.421]. The estimates, confidence intervals, and p-values we get from the two methods line up well.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;{clarify}&lt;/code&gt; uses quantile-based confidence intervals and computes the p-values by inverting them (i.e., finding the largest confidence level that yields an interval that excludes the null value and computing the p-value as one minus that level). Wald confidence intervals and p-values can also be request by setting &lt;code&gt;method = &#34;wald&#34;&lt;/code&gt; in the call to &lt;code&gt;summary()&lt;/code&gt;, but these are only recommended if the quantity has a normal distribution (which the risk ratio does not).&lt;/p&gt;
&lt;div id=&#34;explaining-differences-between-the-approaches&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explaining differences between the approaches&lt;/h3&gt;
&lt;p&gt;Both the delta method- and simulation-based inference approaches are valid, but sometimes you will get results that disagree. The estimates of the quantities of interest may disagree because of how &lt;code&gt;mice::pool()&lt;/code&gt; and &lt;code&gt;clarify::sim_ame()&lt;/code&gt; combine estimates across imputations.&lt;/p&gt;
&lt;p&gt;Rubin’s rules involve simply taking the mean of the estimates across imputations. This works well when the quantity is collapsible, linear, or has a symmetric (ideally normal) distribution. If the quantity of interest is none of those but can be transformed from a quantity that does have those properties, Rubin’s rules can be apply to this intermediate quantity before transforming the estimate to get the final results. This is exactly what we did in the &lt;code&gt;{marginaleffects}&lt;/code&gt; workflow when we computed the log risk ratio before pooling and then exponentiating the pooled log risk ratio to arrive at the risk ratio. If we had gone straight into pooling the risk ratio, the resulting estimate might not have been consistent.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{clarify}&lt;/code&gt; works by first using Rubin’s pooling rules on the model coefficients, which we assume to be normally distributed, and then computing the quantity of interest in each imputed dataset using draws from the pooled coefficients. A benefit of this strategy is that we don’t have to wonder whether the quantity of interest satisfies the above properties. The resulting estimates will be consistent because no pooling is done on them; the pooling happens only in the first step.&lt;/p&gt;
&lt;p&gt;Confidence intervals may differ slightly between the two methods, and this could be due to two reasons: 1) the delta method and simulation-based inferences naturally compute confidence intervals in different ways, with the delta method using a first-order Taylor series approximation and assuming normality of the quantity of interest, and simulation-based inference using simulation to generate a “posterior” for the quantity of interest and using its quantiles as the interval; and 2) simulation-based inference requires many imputations for the variance of the posterior to equal the variance of the Rubin’s rules pooled estimate. More imputations is always better for both methods, so do as many as you can.&lt;/p&gt;
&lt;p&gt;How should you choose between the delta method and simulation-based inference? Use whichever will get you published, of course! (Just kidding.) Use the one you find most trustworthy, that your audience will find the most trustworthy, and that balances the assumptions you are willing to make with the desired precision of the estimate. You might also use the one that seems more natural to you, either conceptually or in terms of usability. Frankly, I find &lt;code&gt;{clarify}&lt;/code&gt; to be easier to use when the quantity of interest is more complicated than a single comparison (e.g., for subgroup analysis or for computing average marginal risks), but &lt;code&gt;{marginaleffects}&lt;/code&gt; can be faster, doesn’t rely on a stochastic process, and is better-backed by statistical theory. Confirming you get similar results with both methods is always a good idea, and the plotting diagnostics in &lt;code&gt;{clarify}&lt;/code&gt; can be used to determine whether any difference might be due to the failure of the delta method due to violation of one of its assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The key differences is that pair membership needs to be accounted for in estimation of the variance of the outcome model coefficients; this is usually as simply as specifying &lt;code&gt;vcov = ~subclass&lt;/code&gt; to functions in &lt;code&gt;{marginaleffects}&lt;/code&gt; or &lt;code&gt;{clarify}&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This actually isn’t necessary for the ATT but it’s walys good practice.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Note: we need the log risk ratio because Rubin’s pooling rules don’t apply to the risk ratio but do to the log risk ratio. We will exponentiate the log risk ratio and its confidence interval after pooling.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Genetic Matching, from the Ground Up</title>
      <link>https://ngreifer.github.io/blog/genetic-matching/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/genetic-matching/</guid>
      <description>


&lt;p&gt;Genetic matching sounds cool and science-y, something we social scientists love because nobody thinks what we do is “real” science. And genetic matching is cool and science-y, but not because it has anything to do with genes or DNA. Genetic matching is a method of adjusting for confounding in observational studies; it is a close relative of propensity score matching and Mahalanobis distance matching and serves exactly the same purpose. &lt;span class=&#34;citation&#34;&gt;Sekhon (&lt;a href=&#34;#ref-sekhonMultivariatePropensityScore2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; describe genetic matching, but I’ll explain it here in simple terms and with an emphasis on its generality, which is undersold by its implementations.&lt;/p&gt;
&lt;p&gt;This post won’t make any sense if you don’t know what matching in general is. Go read &lt;span class=&#34;citation&#34;&gt;Stuart (&lt;a href=&#34;#ref-stuartMatchingMethodsCausal2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Greifer and Stuart (&lt;a href=&#34;#ref-greiferMatchingMethodsConfounder2021a&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, and the &lt;code&gt;MatchIt&lt;/code&gt; &lt;a href=&#34;https://kosukeimai.github.io/MatchIt/articles/matching-methods.html&#34;&gt;vignette&lt;/a&gt; on matching methods to learn about them. The focus here will be on &lt;em&gt;pair matching&lt;/em&gt;, which involves assigning units to pairs or strata based on the distances between them, then discarding unpaired units.&lt;/p&gt;
&lt;p&gt;The goal of matching is balanced samples, i.e., samples where the distribution of covariates in the treated and control groups is the same so that an estimated treatment effect cannot be said to be due to differences in the covariate distributions. Why, then, do we make pairs? Close pairs create balance, in theory. How do we compute how close units are to each other? There are several ways; a common one is the Mahalanobis distance, as described for matching in &lt;span class=&#34;citation&#34;&gt;Rubin (&lt;a href=&#34;#ref-rubinBiasReductionUsing1980&#34; role=&#34;doc-biblioref&#34;&gt;1980&lt;/a&gt;)&lt;/span&gt;, and which I’ll describe here.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Mahalanobis distance&lt;/strong&gt; between two units &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\delta^{md}_{i,j}=\sqrt{(\mathbf{x}_i-\mathbf{x}_j)\Sigma^{-1}(\mathbf{x}_i-\mathbf{x}_j)&amp;#39;}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i\)&lt;/span&gt; is the vector of covariates for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (i.e., that unit’s row in the dataset) and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is the covariance matrix of the covariates&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Equivalently, the Mahalanobis distance is the Euclidean distance (i.e., the regular distance) computed on the standardized principal components. The Mahalanobis distance is an improvement over the Euclidean distance of the covariates because it standardizes the covariates to be on the same scale and adjusts for correlations between covariates (so two highly correlated variables only count once). A great description of the Mahalanobis distance is &lt;a href=&#34;https://stats.stackexchange.com/a/62147/116195&#34;&gt;here&lt;/a&gt; (though there it is not described in the context of matching).&lt;/p&gt;
&lt;p&gt;Genetic matching concerns a generalization of the Mahalanobis distance, called the &lt;strong&gt;generalized Mahalanobis distance&lt;/strong&gt;, which additionally involves a weight matrix. The generalized Mahalanobis distance is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\delta^{gmd}_{i,j}(W)=\sqrt{(\mathbf{x}_i-\mathbf{x}_j)&amp;#39;\left(\Sigma^{-\frac{1}{2}}\right)&amp;#39; W\Sigma^{-\frac{1}{2}}(\mathbf{x}_i-\mathbf{x}_j)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^{-\frac{1}{2}}\)&lt;/span&gt; is the “square root” of the inverse of the covariance matrix (e.g., the Cholesky decomposition), and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is a symmetric weight matrix that can contain anything but in most cases is a diagonal matrix with a scalar weight for each covariate in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; (not weights for each unit like in propensity score weighting; a weight for each &lt;em&gt;covariate&lt;/em&gt;), i.e., &lt;span class=&#34;math inline&#34;&gt;\(W = \text{diag}(\begin{bmatrix} w_1 &amp;amp; \dots &amp;amp; w_p \end{bmatrix})\)&lt;/span&gt;. The generalized Mahalanobis distance is equal to the usual Mahalanobis distance when &lt;span class=&#34;math inline&#34;&gt;\(W=I\)&lt;/span&gt;, the identity matrix.&lt;/p&gt;
&lt;p&gt;What does any of this have to do with genetic matching? Well, “genetic matching” is a bit of a misnomer; it’s not a matching method. It’s a method of estimating &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Genetic matching finds the &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; that, when incorporated in a generalized Mahalanobis distance used to match treated and control units, yields the best balance. Once you have found &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, you then do a regular round of matching, and that is your matched sample.&lt;/p&gt;
&lt;p&gt;To put it slightly more formally, consider a function &lt;span class=&#34;math inline&#34;&gt;\(\text{match}(\delta)\)&lt;/span&gt;, which takes in a distance matrix &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and produces a matched set of treated and control units, characterized by a set of matching weights (e.g., 1 if matched, 0 if unmatched) and pair membership for each unit. Consider a function &lt;span class=&#34;math inline&#34;&gt;\(\text{imbalance}(m)\)&lt;/span&gt;, which takes in the output of a &lt;span class=&#34;math inline&#34;&gt;\(\text{match}(\delta)\)&lt;/span&gt; and returns a scalar imbalance metric (e.g., the largest absolute standardized mean difference among all the covariates). We can then write the genetic matching problem as the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\underset{W}{\operatorname{arg\,min}} \, \text{imbalance}(\text{match}(\delta^{gmd}(W)))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Genetic matching is very general; there are many ways to do the matching (i.e., many ways to specify the &lt;span class=&#34;math inline&#34;&gt;\(\text{match}()\)&lt;/span&gt; function) and many ways to characterize imbalance (i.e., many ways to specify the &lt;span class=&#34;math inline&#34;&gt;\(\text{imbalance}()\)&lt;/span&gt; function) (and even several ways to specific &lt;span class=&#34;math inline&#34;&gt;\(\delta()\)&lt;/span&gt;!). Although nearest neighbor matching is often used for &lt;span class=&#34;math inline&#34;&gt;\(\text{match}()\)&lt;/span&gt;, any matching method that uses a distance matrix could be as well. A specific imbalance measure (which I’ll explain in more detail later) is most often used for &lt;span class=&#34;math inline&#34;&gt;\(\text{imbalance}()\)&lt;/span&gt; because it is the default in the software that implements genetic matching, but any imbalance measure could be used, and there has been research that indicates that alternative measures may work better.&lt;/p&gt;
&lt;p&gt;You may be wondering where the “genetic” part of “genetic matching” comes in. “Genetic” comes from the name of the optimization algorithm that is used to solve the genetic matching problem stated above, which is just called the genetic algorithm. In principle, though, any optimization routine could be used; the genetic algorithm was chosen specifically because it deals well with nonsmooth surfaces, which the objective function above surely is. But other optimization methods that do not rely on derivatives do as well, such as “particle swarm optimization” (we’re really doing &lt;em&gt;science&lt;/em&gt; here). I don’t really understand these methods deeply, but we don’t have to to understand what genetic matching is doing&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. In order to understand how to tune the algorithm, though, there are some bits worth knowing about, which I’ll briefly cover in the Implementation section below.&lt;/p&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;p&gt;Genetic matching is implemented in the &lt;code&gt;{Matching}&lt;/code&gt; package in R, which performs genetic matching to estimate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, performs nearest neighbor matching using &lt;span class=&#34;math inline&#34;&gt;\(\delta^{gmd}(W)\)&lt;/span&gt; or another distance matrix, and then estimates the treatment effect&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. The &lt;code&gt;GenMatch()&lt;/code&gt; function estimates &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, and the &lt;code&gt;Match()&lt;/code&gt; function does the matching on the resulting output&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Genetic matching is also available in &lt;code&gt;{MatchIt}&lt;/code&gt; by setting &lt;code&gt;method = &#34;genetic&#34;&lt;/code&gt; in the call to &lt;code&gt;matchit()&lt;/code&gt;, but it just calls &lt;code&gt;GenMatch()&lt;/code&gt; and &lt;code&gt;Match()&lt;/code&gt; from &lt;code&gt;{Matching}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;GenMatch()&lt;/code&gt; relies on &lt;code&gt;rgenoud::genoud()&lt;/code&gt;, one implementation of the genetic algorithm in R. There are a few tuning parameters worth understanding to use genetic matching to its full potential. The most important one is the population size (i.e., the number of candidates in each generation of the genetic algorithm), controlled by the &lt;code&gt;pop.size&lt;/code&gt; argument. All you need to know is that high values are better and slower. Another one perhaps worth knowing about is the number of generations that have to pass with no improvement in the objective function before the algorithm halts and returns the best candidate it has found, controlled by the &lt;code&gt;wait.generations&lt;/code&gt; argument. Here, too, higher values are better and slower.&lt;/p&gt;
&lt;p&gt;A detail I haven’t emphasized is that the matching method used to to produce the final matched sample using the estimated &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; should be the same one used in estimating $W$, because the estimated &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; are tailored to that matching method (i.e., they only optimize balance when supplied to that &lt;span class=&#34;math inline&#34;&gt;\(\text{match}()\)&lt;/span&gt; function)&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. The matching methods available in &lt;code&gt;{Matching}&lt;/code&gt; are nearest neighbor matching with or without replacement, with or without calipers or exact matching constraints, and with &lt;span class=&#34;math inline&#34;&gt;\(1:1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(k:1\)&lt;/span&gt; matching. This is a pretty broad set of matching options, though it is not complete (e.g., optimal and full matching are not available). One thing about genetic matching is that it is &lt;em&gt;slow&lt;/em&gt;, so using a fast matching method is useful for not spending forever to get your matches. &lt;code&gt;{Matching}&lt;/code&gt; uses a fast implementation of nearest neighbor matching programmed in C, which makes it fairly fast, though still quite slow for even moderately sized problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-imbalance-measure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Imbalance Measure&lt;/h3&gt;
&lt;p&gt;The imbalance measure used in genetic matching is critical to its success as a method. Seeking balance using a poor metric means the resulting matched sample will not be able to reduce bias well, even if the optimal values of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; that minimize that imbalance measure have been found. One challenge is that there is no clear best imbalance measure to use. Ideally, it should incorporate balance on all covariates, and not just on their means but on their full distributions, and not just the marginal distributions but the joint distributions. The best imbalance measure depends heavily on the true outcome-generating model, which is inherently unknowable (otherwise we wouldn’t be doing matching in the first place), though there has been some research into it.&lt;/p&gt;
&lt;p&gt;By default, the imbalance measure &lt;code&gt;GenMatch()&lt;/code&gt; uses is the smallest p-value among the sets of two-sample t-tests and Kolmogorov-Smirnov (KS) tests for each covariate. This is a bit of a strange imbalance measure that doesn’t really show up anywhere else in the literature. &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; justify the use of p-values (which are typically disregarded as methods to assess balance) by arguing that here they are simply used to put the mean differences and KS statistic on a uniform scale rather than to be interpreted as p-values to be used in a hypothesis test. However, there has been research into other balance criteria that might perform better. &lt;span class=&#34;citation&#34;&gt;Oyenubi and Wittenberg (&lt;a href=&#34;#ref-oyenubiDoesChoiceBalancemeasure2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; find that the largest value of a univariate balance measure called the “entropic distance”, which is a relative of the KS statistic, performs well as an imbalance measure. &lt;span class=&#34;citation&#34;&gt;Zhu, Savage, and Ghosh (&lt;a href=&#34;#ref-zhuKernelBasedMetricBalance2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; find that a multivariate imbalance measure called the “kernel distance” does well; this measure takes into account the full, joint covariate distribution, unlike the other methods which do not consider the joint distribution, explaining its effectiveness. I am partial to the energy distance &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rizzoEnergyDistance2016&#34; role=&#34;doc-biblioref&#34;&gt;Rizzo and Székely 2016&lt;/a&gt;; &lt;a href=&#34;#ref-hulingEnergyBalancingCovariate2022&#34; role=&#34;doc-biblioref&#34;&gt;Huling and Mak 2022&lt;/a&gt;)&lt;/span&gt;, which is demonstrated to have nice properties and is easy to explain and calculate. Simple balance measures can be effective as well, though; &lt;span class=&#34;citation&#34;&gt;Oyenubi and Wittenberg (&lt;a href=&#34;#ref-oyenubiDoesChoiceBalancemeasure2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Stuart, Lee, and Leacy (&lt;a href=&#34;#ref-stuartPrognosticScorebasedBalance2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; find that standardized mean differences can be effective in assessing balance, even though they only take into account the covariate means and do not consider the joint distribution of the covariates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-covariates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Covariates&lt;/h3&gt;
&lt;p&gt;The generalized Mahalanobis distance depends on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;–the covariates, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;–the “scaling” matrix (usually the covariance matrix), and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;–the weights matrix. These, of course, can all be specified in a variety of ways. &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; should contain the covariates one would like balance on, though in principle it doesn’t have to, as long as those covariates are included in the imbalance measure. For example, one might only include 3 of the most important covariates in the calculation of the distance and weights, but optimize balance on all 10 covariates included in the analysis. &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; recommend including the propensity score in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;, as close pairs on the propensity score tends to yield well-balanced samples (which is the motivation behind propensity score matching in the first place). On the other hand, &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; recommend against including the propensity score if balance can be achieved without it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Below are some examples of genetic matching. First we’ll use &lt;code&gt;{Matching}&lt;/code&gt;, which gives us a bit more insight into how the process goes, and then we’ll perform the same analysis using &lt;code&gt;{MatchIt}&lt;/code&gt; to demonstrate how much easier it is. We’ll use the &lt;code&gt;lalonde&lt;/code&gt; dataset in &lt;code&gt;{MatchIt}&lt;/code&gt; for this analysis&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;using-matching&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using &lt;code&gt;Matching&lt;/code&gt;&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;lalonde&amp;quot;, package = &amp;quot;MatchIt&amp;quot;)

covs &amp;lt;- lalonde |&amp;gt; subset(select = c(age, educ, married,
                                     race, nodegree,
                                     re74, re75))
treat &amp;lt;- lalonde$treat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a factor variable (&lt;code&gt;race&lt;/code&gt;) among our covariates, so we need to turn it into a set of dummy variables for &lt;code&gt;{Matching}&lt;/code&gt; . The &lt;code&gt;{cobalt}&lt;/code&gt; function &lt;code&gt;splitfactor()&lt;/code&gt; makes this easy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covs &amp;lt;- covs |&amp;gt; cobalt::splitfactor(drop.first = FALSE)

head(covs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age educ married race_black race_hispan race_white nodegree re74 re75
## NSW1  37   11       1          1           0          0        1    0    0
## NSW2  22    9       0          0           1          0        1    0    0
## NSW3  30   12       0          1           0          0        0    0    0
## NSW4  27   11       0          1           0          0        1    0    0
## NSW5  33    8       0          1           0          0        1    0    0
## NSW6  22    9       0          1           0          0        1    0    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll estimate a propensity score to include among the covariates, as recommended by &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Logistic regression PS
ps &amp;lt;- glm(treat ~ age + educ + married + race +
            nodegree + re74 + re75, data = lalonde,
          family = binomial) |&amp;gt;
  fitted()

## Append the PS to the covariates
covs_ps &amp;lt;- cbind(ps, covs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, now we’re finally ready to use functions in &lt;code&gt;{Matching}&lt;/code&gt; to perform genetic matching. The first step is to use &lt;code&gt;GenMatch()&lt;/code&gt; to compute &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, and after that we will use &lt;code&gt;Match()&lt;/code&gt; to perform the matching using the &lt;code&gt;GenMatch()&lt;/code&gt; output. To use &lt;code&gt;GenMatch()&lt;/code&gt;, we have to know what kind of matching we eventually want to do. In this example, we’ll do 2:1 matching with replacement for the ATT. &lt;code&gt;{Matching}&lt;/code&gt; has a few extra quirks that need to be addressed to make the matching work as intended, which I’ll include in the code below without much explanation (since my recommendation is to use &lt;code&gt;{MatchIt}&lt;/code&gt; anyway, which takes care of these automatically).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matching)
# Set seed for reproducibility; genetic matching has a random
# component
set.seed(333)
Gen_out &amp;lt;- GenMatch(
  Tr = treat,             #Treatment
  X = covs_ps,            #Covariates to match on
  BalanceMatrix = covs,   #Covariance to balance
  estimand = &amp;quot;ATT&amp;quot;,       #Estimand
  M = 2,                  #2:1 matching
  replace = TRUE,         #With replacement
  ties = FALSE,           #No ties
  distance.tolerance = 0, #Use precise values
  print.level = 0,        #Don&amp;#39;t print output
  pop.size = 200          #Genetic population size; bigger is better
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important part of the &lt;code&gt;GenMatch()&lt;/code&gt; output is the &lt;code&gt;Weight.matrix&lt;/code&gt;, which corresponds to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. It’s not really worth interpreting the weights; they are just whatever values happened to yield the best balance and don’t actually tell you anything about how important any covariate is to the treatment. We can supply the weights to the &lt;code&gt;Match()&lt;/code&gt; function to do a final round of matching. All the arguments related to matching (e.g., &lt;code&gt;estimand&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;replace&lt;/code&gt;, etc.) should be the same between &lt;code&gt;GenMatch()&lt;/code&gt; and &lt;code&gt;Match()&lt;/code&gt;. We call &lt;code&gt;Match()&lt;/code&gt; below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Match_out &amp;lt;- Match(
  Tr = treat,             #Treatment
  X = covs_ps,            #Covariates to match on
  estimand = &amp;quot;ATT&amp;quot;,       #Estimand
  M = 2,                  #2:1 matching
  replace = TRUE,         #With replacement
  ties = FALSE,           #No ties
  distance.tolerance = 0, #Use precise values
  Weight.matrix = Gen_out$Weight.matrix,
  Weight = 3              #Tell Match() we&amp;#39;re using Weight.matrix
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can take a look at the balance using &lt;code&gt;cobalt::bal.tab()&lt;/code&gt;. Here, we check balance not only on the means but also on the KS statistics, since those are part of what is being optimized by the genetic optimization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cobalt::bal.tab(Match_out, treat ~ age + educ + married + race +
                  nodegree + re74 + re75, data = lalonde,
                stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance Measures
##                Type Diff.Adj KS.Adj
## age         Contin.  -0.0178 0.1378
## educ        Contin.   0.0686 0.0459
## married      Binary   0.0000 0.0000
## race_black   Binary   0.0054 0.0054
## race_hispan  Binary   0.0000 0.0000
## race_white   Binary  -0.0054 0.0054
## nodegree     Binary   0.0135 0.0135
## re74        Contin.   0.0305 0.1270
## re75        Contin.   0.0923 0.0919
## 
## Sample sizes
##                      Control Treated
## All                    429.      185
## Matched (ESS)           42.1     185
## Matched (Unweighted)   121.      185
## Unmatched              308.        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we’ll use &lt;code&gt;MatchIt&lt;/code&gt;, which does everything (adjusting the covariate matrix, estimating propensity scores, optimizing &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, and matching on the new distance matrix) all at once.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-matchit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using &lt;code&gt;MatchIt&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;All we need to do is supply the usual arguments to &lt;code&gt;matchit()&lt;/code&gt; and set &lt;code&gt;method = &#34;genetic&#34;&lt;/code&gt;. See the &lt;code&gt;MatchIt&lt;/code&gt; &lt;a href=&#34;https://kosukeimai.github.io/MatchIt/articles/MatchIt.html&#34;&gt;vignettes&lt;/a&gt; for information on the basic use of &lt;code&gt;matchit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(888)
matchit_out &amp;lt;- MatchIt::matchit(
  treat ~ age + educ + married + race +
                  nodegree + re74 + re75,
  data = lalonde,
  method = &amp;quot;genetic&amp;quot;,
  estimand = &amp;quot;ATT&amp;quot;,
  ratio = 2,
  replace = TRUE,
  pop.size = 200
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, &lt;code&gt;matchit()&lt;/code&gt; estimates a propensity score using logistic regression and includes it in the matching covariates (but not the covariates on which balance is optimized), just as we did manually using &lt;code&gt;GenMatch()&lt;/code&gt; above. If you want to use difference variables to balance on from those used to match, use the &lt;code&gt;mahvars&lt;/code&gt; argument, which is explained in the documentation for genetic matching (accessible using &lt;code&gt;help(&#34;method_genetic&#34;, package = &#34;MatchIt&#34;)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can assess balance using &lt;code&gt;summary()&lt;/code&gt; or using &lt;code&gt;bal.tab()&lt;/code&gt;. We’ll do the latter below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cobalt::bal.tab(matchit_out, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance Measures
##                 Type Diff.Adj KS.Adj
## distance    Distance   0.0337 0.1000
## age          Contin.  -0.0238 0.1514
## educ         Contin.   0.0712 0.0324
## married       Binary  -0.0027 0.0027
## race_black    Binary   0.0081 0.0081
## race_hispan   Binary   0.0000 0.0000
## race_white    Binary  -0.0081 0.0081
## nodegree      Binary   0.0054 0.0054
## re74         Contin.   0.0356 0.1514
## re75         Contin.   0.0689 0.0730
## 
## Sample sizes
##                      Control Treated
## All                    429.      185
## Matched (ESS)           45.6     185
## Matched (Unweighted)   123.      185
## Unmatched              306.        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results will differ due to slight differences in how the two functions process their inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-genetic-matching-yourself&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Programming Genetic Matching Yourself&lt;/h2&gt;
&lt;p&gt;Perhaps surprisingly, it’s fairly easy to program genetic matching yourself. You only need the following ingredients:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A function that creates a distance matrix from a set of weights &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;A function that performs matching on a given distance matrix&lt;/li&gt;
&lt;li&gt;A function that evaluates balance on a given matched sample&lt;/li&gt;
&lt;li&gt;A function that performs the genetic optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are (fairly) easy to come by, and I’ll show you how to write each of them.&lt;/p&gt;
&lt;p&gt;For the first function, we can use &lt;code&gt;MatchIt::mahalanobis_dist()&lt;/code&gt; if we want &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; to be the full covariance matrix of the covariates, but it’s actually quite a bit simpler to use &lt;code&gt;MatchIt::scaled_euclidean_dist()&lt;/code&gt; to just use the variances of the covariates, which is what &lt;code&gt;GenMatch()&lt;/code&gt; (and therefore &lt;code&gt;matchit()&lt;/code&gt;) does anyway. This is because we can supply to &lt;code&gt;scaled_euclidean_dist()&lt;/code&gt; a vector of variances, which we will simply divide by the weights. So, our function for creating the distance matrix given the set of weights will be the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist_from_W &amp;lt;- function(W, dist_covs) {
  variances &amp;lt;- apply(dist_covs, 2, var)
  MatchIt::scaled_euclidean_dist(data = dist_covs, var = variances / W)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, there are many ways we could make this more efficient. I just want to demonstrate how easy it is to program genetic matching. Programming it &lt;em&gt;well&lt;/em&gt; is another story.&lt;/p&gt;
&lt;p&gt;Next, we need a function that performs matching on covariates given a distance matrix. We could use &lt;code&gt;optmatch::fullmatch()&lt;/code&gt; for full matching, but &lt;code&gt;matchit()&lt;/code&gt; provides a nice, general interface for many matching methods. We can supply the distance matrix to the &lt;code&gt;distance&lt;/code&gt; argument of &lt;code&gt;matchit()&lt;/code&gt;. A function that takes in a distance matrix and returns a &lt;code&gt;matchit&lt;/code&gt; object containing the matched sample and matching weights is the following&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_matching_with_dist &amp;lt;- function(dist) {
  MatchIt::matchit(treat ~ 1, data = lalonde, distance = dist,
                   method = &amp;quot;nearest&amp;quot;, ratio = 2, replace = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need a function that takes in a &lt;code&gt;matchit&lt;/code&gt; object and computes a scalar balance statistic. You can use your favorite balance statistic, but here I’ll use the maximum absolute standardized mean difference (ASMD) of all the covariates in the matched sample&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. This measure can be easily computed using &lt;code&gt;cobalt::col_w_smd()&lt;/code&gt;, which takes in a matrix of covariates, a treatment vector, and a weights vector and returns the weighted ASMDs for each covariate. We will allow the set of covariates to be different from those used to compute the distance measure. We implement this below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_balance &amp;lt;- function(m, bal_covs, treat) {
  weights &amp;lt;- cobalt::get.w(m)
  max(cobalt::col_w_smd(bal_covs, treat, weights,
                        s.d.denom = &amp;quot;treated&amp;quot;,
                        abs = TRUE))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay! We have the key ingredients for our objective function, which takes in a set of covariates weights &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and returns a balance statistic that we want to optimize. Let’s put everything together into a single function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;objective &amp;lt;- function(W_, dist_covs, bal_covs, treat) {
  W &amp;lt;- exp(c(0, W_))
  
  dist_from_W(W, dist_covs) |&amp;gt;
    do_matching_with_dist() |&amp;gt;
    compute_balance(bal_covs, treat)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line of the function needs explaining. Instead of optimizing over the weights directly, we’re going to optimize over the log of the weights. This ensures the weights can prioritize and de-prioritize variables in a symmetric way&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. To get back to the weights &lt;code&gt;W&lt;/code&gt; used in the distance measure, we need to exponentiate the optimized log-weights &lt;code&gt;W_&lt;/code&gt;. Also, instead of optimizing over all the weights, we are going to fix one weight to 1 (i.e., fix one log-weight to 0). This is because the matches are invariant to multiplying all the weights by a constant&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;. So, we can identify the weights by choosing an arbitrary weight to set to 1&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can give this function a try to see balance when when the log-weights are all set to 0 (i.e., so all weights are equal to 1), which corresponds to matching using the standard scaled Euclidean distance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_test &amp;lt;- rep(0, ncol(covs_ps) - 1)
objective(W_test, dist_covs = covs_ps, bal_covs = covs,
          treat = treat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1280539&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can supply this to a function that performs the genetic algorithm to optimize our objective function. &lt;code&gt;GenMatch()&lt;/code&gt; uses &lt;code&gt;rgenoud::genoud()&lt;/code&gt;, but there is a more modern interface in the R package &lt;code&gt;{GA}&lt;/code&gt;, which we’ll use instead just to demonstrate that the method is software-independent. We’ll use &lt;code&gt;GA::ga()&lt;/code&gt;, which implements the standard genetic algorithm, though other functions are available for more sophisticated methods.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ga()&lt;/code&gt; can only maximize functions, but we want to minimize our imbalance, so we just have to create a new objective function that is the negative of our original.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Need negative objective to minimize imbalance
neg_objective &amp;lt;- function(...) -objective(...)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at the &lt;code&gt;GA::ga()&lt;/code&gt; call below. We specify &lt;code&gt;type = &#34;real-valued&#34;&lt;/code&gt; because our weights are real numbers, we supply the negative of our objective function to &lt;code&gt;fitness&lt;/code&gt;, and we supply the additional argument to our functions (&lt;code&gt;dist_covs&lt;/code&gt;, the covariates used in the distance matrix and the weights of which we are optimizing over; &lt;code&gt;bal_covs&lt;/code&gt;, the covariates used to compute the balance statistic that is our criterion; and &lt;code&gt;treat&lt;/code&gt;, the treatment vector). We need to provide lower and upper bounds for the weights, and here I’ve supplied -7 and 7, which correspond to weights of &lt;span class=&#34;math inline&#34;&gt;\(\exp(-7)=.0009\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\exp(7)=1096.6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The next arguments control the speed and performance of the optimization process. I’ve already described &lt;code&gt;popSize&lt;/code&gt;, the population size (called &lt;code&gt;pop.size&lt;/code&gt; in &lt;code&gt;GenMatch()&lt;/code&gt;). We are going to let the algorithm run for 500 generations (&lt;code&gt;maxiter&lt;/code&gt;, called &lt;code&gt;max.generations&lt;/code&gt; in &lt;code&gt;GenMatch()&lt;/code&gt;/&lt;code&gt;genoud()&lt;/code&gt;) but stop if there is no improvement in balance after 100 iterations (&lt;code&gt;run&lt;/code&gt;, called &lt;code&gt;wait.generations&lt;/code&gt; in &lt;code&gt;GenMatch()&lt;/code&gt;/&lt;code&gt;genoud()&lt;/code&gt;). I’m going to request parallel processing using 4 cores to speed it up, and suppress printing of output&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opt_out &amp;lt;- GA::ga(
  type = &amp;quot;real-valued&amp;quot;,
  fitness = neg_objective,
  dist_covs = covs_ps,
  bal_covs = covs,
  treat = treat,
  lower = rep(-7, ncol(covs_ps) - 1),
  upper = rep(7, ncol(covs_ps) - 1),
  popSize = 200, 
  maxiter = 500,
  run = 100,
  parallel = 4,
  seed = 567, #set seed here if using parallelization
  monitor = NULL
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes my computer about 3 minutes to run. We can run some summaries on the output object to examine the results of the optimization:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(opt_out)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Genetic Algorithm ─────────────────── 
## 
## GA settings: 
## Type                  =  real-valued 
## Population size       =  200 
## Number of generations =  500 
## Elitism               =  10 
## Crossover probability =  0.8 
## Mutation probability  =  0.1 
## Search domain = 
##       x1 x2 x3 x4 x5 x6 x7 x8 x9
## lower -7 -7 -7 -7 -7 -7 -7 -7 -7
## upper  7  7  7  7  7  7  7  7  7
## 
## GA results: 
## Iterations             = 205 
## Fitness function value = -0.02735878 
## Solutions = 
##             x1        x2         x3       x4       x5        x6        x7        x8       x9
## [1,] -2.390895 -3.260254 -0.3561188 1.771811 1.873141 0.4049403 0.5014592 -1.137037 2.931112
## [2,] -2.376927 -3.258679 -0.3536636 1.525004 1.885433 0.3003448 0.5020003 -1.143757 2.929666
## [3,] -2.390895 -3.260254 -0.3561188 1.319585 1.873141 0.4049403 0.5014592 -1.137037 2.931112
## [4,] -2.390895 -3.260254 -0.3561188 1.542400 1.873141 0.4049403 0.5014592 -1.137037 2.931112&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that our final value for the criterion was about -0.0274 and this was achieved by each of the sets of log weights displayed. We can just focus on the first row. It’s not worth over-interpreting these values since their purpose is just to achieve balance and they don’t reveal anything about the causal or statistical relevance of the covariates. But we can see that &lt;code&gt;x9&lt;/code&gt; (i.e., &lt;code&gt;re75&lt;/code&gt;) was the most important covariate in the distance measure, and &lt;code&gt;x2&lt;/code&gt; (i.e., &lt;code&gt;educ&lt;/code&gt;) was the least important.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(opt_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ngreifer.github.io/blog/genetic-matching/index.en_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
We can also see from the plot that close to the best balance was reached pretty quickly in fewer than 50 generations, and refinements after that were very minor. This suggests that if you’re in a rush or just want to test out genetic matching without committing to it, you can wait just a few generations (fewer than 100, which is the default in &lt;code&gt;GenMatch()&lt;/code&gt;) to get a good sense of its performance.&lt;/p&gt;
&lt;p&gt;Finally, let’s perform a final round of matching using the found matching weights and assess balance on each covariate in our matched sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Extract weights by transforming log weights from output
W &amp;lt;- exp(c(0, opt_out@solution[1,]))

#Compute distance measure from weights and do matching
m.out &amp;lt;- dist_from_W(W, covs_ps) |&amp;gt;
  do_matching_with_dist()

m.out&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A matchit object
##  - method: 2:1 nearest neighbor matching with replacement
##  - distance: User-defined (matrix)
##  - number of obs.: 614 (original), 305 (matched)
##  - target estimand: ATT&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Assess balance. See ?bal.tab for info on the arguments
cobalt::bal.tab(treat ~ age + educ + married + race +
                  nodegree + re74 + re75,
                data = lalonde, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;), 
                binary = &amp;quot;std&amp;quot;, un = TRUE,
                weights = cobalt::get.w(m.out),
                method = &amp;quot;matching&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance Measures
##                Type Diff.Un  KS.Un Diff.Adj KS.Adj
## age         Contin. -0.3094 0.1577   0.0249 0.2514
## educ        Contin.  0.0550 0.1114   0.0242 0.0189
## married      Binary -0.8263 0.3236  -0.0207 0.0081
## race_black   Binary  1.7615 0.6404   0.0223 0.0081
## race_hispan  Binary -0.3498 0.0827   0.0000 0.0000
## race_white   Binary -1.8819 0.5577  -0.0274 0.0081
## nodegree     Binary  0.2450 0.1114   0.0238 0.0108
## re74        Contin. -0.7211 0.4470  -0.0270 0.1649
## re75        Contin. -0.2903 0.2876   0.0225 0.0378
## 
## Sample sizes
##                      Control Treated
## All                    429.      185
## Matched (ESS)           51.5     185
## Matched (Unweighted)   120.      185
## Unmatched              309.        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that after matching, the largest standardized mean difference is indeed 0.0274, well below the usual criterion of .1. That doesn’t mean the sample is fully balanced, though; some KS statistics are a bit high, suggesting that an imbalance measure that accounts for the full distribution of the covariates beyond the means might be more effective. Finally, once satisfactory balance has been found, you can estimate the treatment effect using the methods described in &lt;code&gt;vignette(&#34;estimating-effects&#34;, package = &#34;MatchIt&#34;)&lt;/code&gt;. I’ve gone on long enough so I won’t do that here.&lt;/p&gt;
&lt;p&gt;Congratulations! You’ve just done genetic matching, three ways!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-diamondGeneticMatchingEstimating2013&#34; class=&#34;csl-entry&#34;&gt;
Diamond, Alexis, and Jasjeet S. Sekhon. 2013. &lt;span&gt;“Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.”&lt;/span&gt; &lt;em&gt;Review of Economics and Statistics&lt;/em&gt; 95 (3): 932945. &lt;a href=&#34;https://doi.org/10.1162/REST_a_00318&#34;&gt;https://doi.org/10.1162/REST_a_00318&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-greiferMatchingMethodsConfounder2021a&#34; class=&#34;csl-entry&#34;&gt;
Greifer, Noah, and Elizabeth A Stuart. 2021. &lt;span&gt;“Matching Methods for Confounder Adjustment: An Addition to the Epidemiologist&lt;span&gt;’&lt;/span&gt;s Toolbox.”&lt;/span&gt; &lt;em&gt;Epidemiologic Reviews&lt;/em&gt;, June, mxab003. &lt;a href=&#34;https://doi.org/10.1093/epirev/mxab003&#34;&gt;https://doi.org/10.1093/epirev/mxab003&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hulingEnergyBalancingCovariate2022&#34; class=&#34;csl-entry&#34;&gt;
Huling, Jared D., and Simon Mak. 2022. &lt;span&gt;“Energy &lt;span&gt;Balancing&lt;/span&gt; of &lt;span&gt;Covariate Distributions&lt;/span&gt;.”&lt;/span&gt; &lt;span&gt;arXiv&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2004.13962&#34;&gt;https://doi.org/10.48550/arXiv.2004.13962&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kingWhyPropensityScores2019&#34; class=&#34;csl-entry&#34;&gt;
King, Gary, and Richard Nielsen. 2019. &lt;span&gt;“Why Propensity Scores Should Not Be Used for Matching.”&lt;/span&gt; &lt;em&gt;Political Analysis&lt;/em&gt;, May, 1–20. &lt;a href=&#34;https://doi.org/10.1017/pan.2019.11&#34;&gt;https://doi.org/10.1017/pan.2019.11&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-oyenubiDoesChoiceBalancemeasure2020&#34; class=&#34;csl-entry&#34;&gt;
Oyenubi, Adeola, and Martin Wittenberg. 2020. &lt;span&gt;“Does the Choice of Balance-Measure Matter Under Genetic Matching?”&lt;/span&gt; &lt;em&gt;Empirical Economics&lt;/em&gt;, May. &lt;a href=&#34;https://doi.org/10.1007/s00181-020-01873-9&#34;&gt;https://doi.org/10.1007/s00181-020-01873-9&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rizzoEnergyDistance2016&#34; class=&#34;csl-entry&#34;&gt;
Rizzo, Maria L., and Gábor J. Székely. 2016. &lt;span&gt;“Energy Distance.”&lt;/span&gt; &lt;em&gt;WIREs Computational Statistics&lt;/em&gt; 8 (1): 27–38. &lt;a href=&#34;https://doi.org/10.1002/wics.1375&#34;&gt;https://doi.org/10.1002/wics.1375&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rubinBiasReductionUsing1980&#34; class=&#34;csl-entry&#34;&gt;
Rubin, Donald B. 1980. &lt;span&gt;“Bias Reduction Using Mahalanobis-Metric Matching.”&lt;/span&gt; &lt;em&gt;Biometrics&lt;/em&gt; 36 (2): 293–98. &lt;a href=&#34;https://doi.org/10.2307/2529981&#34;&gt;https://doi.org/10.2307/2529981&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sekhonMultivariatePropensityScore2011&#34; class=&#34;csl-entry&#34;&gt;
Sekhon, Jasjeet S. 2011. &lt;span&gt;“Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 42 (1): 1–52. &lt;a href=&#34;https://doi.org/10.18637/jss.v042.i07&#34;&gt;https://doi.org/10.18637/jss.v042.i07&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuartMatchingMethodsCausal2010&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A. 2010. &lt;span&gt;“Matching Methods for Causal Inference: A Review and a Look Forward.”&lt;/span&gt; &lt;em&gt;Statistical Science&lt;/em&gt; 25 (1): 1–21. &lt;a href=&#34;https://doi.org/10.1214/09-STS313&#34;&gt;https://doi.org/10.1214/09-STS313&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuartPrognosticScorebasedBalance2013&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A., Brian K. Lee, and Finbarr P. Leacy. 2013. &lt;span&gt;“Prognostic Score-Based Balance Measures Can Be a Useful Diagnostic for Propensity Score Methods in Comparative Effectiveness Research.”&lt;/span&gt; &lt;em&gt;Journal of Clinical Epidemiology&lt;/em&gt; 66 (8): S84. &lt;a href=&#34;https://doi.org/10.1016/j.jclinepi.2013.01.013&#34;&gt;https://doi.org/10.1016/j.jclinepi.2013.01.013&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-zhuKernelBasedMetricBalance2018&#34; class=&#34;csl-entry&#34;&gt;
Zhu, Yeying, Jennifer S. Savage, and Debashis Ghosh. 2018. &lt;span&gt;“A Kernel-Based Metric for Balance Assessment.”&lt;/span&gt; &lt;em&gt;Journal of Causal Inference&lt;/em&gt; 6 (2). &lt;a href=&#34;https://doi.org/10.1515/jci-2016-0029&#34;&gt;https://doi.org/10.1515/jci-2016-0029&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;There are several possible ways to compute &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;; for example, &lt;span class=&#34;citation&#34;&gt;Rubin (&lt;a href=&#34;#ref-rubinBiasReductionUsing1980&#34; role=&#34;doc-biblioref&#34;&gt;1980&lt;/a&gt;)&lt;/span&gt; uses the “pooled” covariance matrix, which is a weighted average of the within-group covariances.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Basically, they work by proposing a population of guesses of the parameters to be estimated (e.g., 50 sets of candidate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;s), removing the candidates with the worst imbalance, and reproducing and perturbing the remaining candidates slightly (like a genetic mutation), then doing this over and over again so that only the best candidates remain. This is a type of “evolutionary algorithm” because it works a bit like natural selection, where the fittest creatures remain to reproduce but with slight variation, and the least fit die off, improving the overall fitness of the species.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;code&gt;{Matching}&lt;/code&gt; uses matching imputation to estimate the treatment effect, which is different from running an outcome regression in the matched sample. See my answer &lt;a href=&#34;https://stats.stackexchange.com/a/566981/116195&#34;&gt;here&lt;/a&gt; for some additional details on this distinction and its implications.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;It’s maybe worth knowing that &lt;code&gt;GenMatch()&lt;/code&gt; actually uses &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; with all the off-diagonal elements set to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. This is not described in its documentation or in the papers describing the method. In practice, this likely makes little difference to the overall matching performance. A benefit of this approach is that you get a nice interpretation of the resulting &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; as importance of each variable in the match, though this interpretation serves little use in practice.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Using a different matching method for the final match than you did in estimating &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is possible, but not advised.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Be careful! There’s a &lt;code&gt;lalonde&lt;/code&gt; dataset in &lt;code&gt;{Matching}&lt;/code&gt;, too, which is different.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Here is seems like we aren’t matching on any covariates by supplying &lt;code&gt;treat ~ 1&lt;/code&gt; as the model formula; we are supplying the distance matrix ourselves, so the covariates play no role in the matching beyond that. To speed up the evaluation and prevent &lt;code&gt;matchit()&lt;/code&gt; from having to process a whole data frame of covariates, we omit the covariates.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;This same balance statistic can be used in &lt;code&gt;WeightIt&lt;/code&gt; and &lt;code&gt;twang&lt;/code&gt; for generalized boosted modeling and other methods that involve optimizing a user-supplied criterion.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;That is, so a weight of 2 is as easy to find as a weight of 1/2, as these have the same “magnitude”; they correspond to log-weights of .69 and -.69, respectively.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;That is, the exact same matches found for a given set of weights would be found if all those weights were multiplied by, e.g., 100.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;It doesn’t matter which one you choose, but I like to make the propensity score have the scaling weight to assess how much more or less important the covariates are than the propensity score for achieving balance.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;If you’re following along at home, try setting &lt;code&gt;monitor = plot&lt;/code&gt; to see a neat plot of the progress of the optimization! We’ll also view this plot after the optimization has finished.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Subgroup Analysis After Propensity Score Matching Using R</title>
      <link>https://ngreifer.github.io/blog/subgroup-analysis-psm/</link>
      <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/subgroup-analysis-psm/</guid>
      <description>


&lt;p&gt;Today I’m going to demonstrate performing a subgroup analysis after propensity score matching using R. Subgroup analysis, also known as moderation analysis or the analysis of effect modification, concerns the estimation of treatment effects within subgroups of a pre-treatment covariate. This post assumes you understand how to do propensity score matching. For a general introduction to propensity score matching, I recommend &lt;span class=&#34;citation&#34;&gt;Austin (&lt;a href=&#34;#ref-austinIntroductionPropensityScore2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; and the &lt;code&gt;{MatchIt}&lt;/code&gt; &lt;a href=&#34;https://kosukeimai.github.io/MatchIt/articles/MatchIt.html&#34;&gt;introductory vignette&lt;/a&gt;. If you understand inverse probability weighting but aren’t too familiar with matching, I recommend my article with Liz Stuart &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-greiferMatchingMethodsConfounder2021a&#34; role=&#34;doc-biblioref&#34;&gt;Greifer and Stuart 2021&lt;/a&gt;)&lt;/span&gt;. For an introduction to subgroup analysis with propensity scores, you can also check out &lt;span class=&#34;citation&#34;&gt;Green and Stuart (&lt;a href=&#34;#ref-greenExaminingModerationAnalyses2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. Here, I’ll mainly try to get to the point.&lt;/p&gt;
&lt;p&gt;The dataset we’ll use today is the famous Lalonde dataset, investigating the effect of a job training program on earnings. We’ll use the version of this dataset that comes with the &lt;code&gt;{MatchIt}&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;lalonde&amp;quot;, package = &amp;quot;MatchIt&amp;quot;)
head(lalonde)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      treat age educ   race married nodegree re74 re75       re78
## NSW1     1  37   11  black       1        1    0    0  9930.0460
## NSW2     1  22    9 hispan       0        1    0    0  3595.8940
## NSW3     1  30   12  black       0        0    0    0 24909.4500
## NSW4     1  27   11  black       0        1    0    0  7506.1460
## NSW5     1  33    8  black       0        1    0    0   289.7899
## NSW6     1  22    9  black       0        1    0    0  4056.4940&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The treatment is &lt;code&gt;treat&lt;/code&gt;, the outcome in the original study was &lt;code&gt;re78&lt;/code&gt; (1978 earnings), and the other variables are pretreatment covariates that we want to adjust for using propensity score matching. In this example, I’ll actually be using a different outcome, &lt;code&gt;re78_0&lt;/code&gt;, which is whether the participant’s 1978 earnings were equal to 0 or not, because I want to demonstrate the procedure for a binary outcome. So, we hope the treatment effect is negative, i.e., the risk of 0 earnings decreases for those in the treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lalonde$re78_0 &amp;lt;- as.numeric(lalonde$re78 == 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our moderator will be &lt;code&gt;race&lt;/code&gt;, a 3-category factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;with(lalonde, table(race))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## race
##  black hispan  white 
##    243     72    299&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our estimand will be the subgroup-specific and marginal average treatment effect on the treated (ATT), using the risk difference as our effect measure.&lt;/p&gt;
&lt;div id=&#34;packages-youll-need&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Packages You’ll Need&lt;/h3&gt;
&lt;p&gt;We’ll need a few R packages for this analysis. We’ll need &lt;code&gt;{MatchIt}&lt;/code&gt; and &lt;code&gt;{optmatch}&lt;/code&gt; for the matching, &lt;code&gt;{cobalt}&lt;/code&gt; for the balance assessment, &lt;code&gt;{marginaleffects}&lt;/code&gt; for estimating the treatment effects, and &lt;code&gt;{sandwich}&lt;/code&gt; for computing the standard errors. You can install those using the code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;MatchIt&amp;quot;, &amp;quot;optmatch&amp;quot;, &amp;quot;cobalt&amp;quot;,
                   &amp;quot;marginaleffects&amp;quot;, &amp;quot;sandwich&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s get into it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-subgroup-matching&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Subgroup Matching&lt;/h2&gt;
&lt;p&gt;Our first step is to perform the matching. Although there are a few strategies for performing matching for subgroup analysis, in general subgroup-specific matching tends to work best, though it requires a little extra work.&lt;/p&gt;
&lt;p&gt;We’ll do this by splitting the dataset by &lt;code&gt;race&lt;/code&gt; and performing a separate matching analysis within each one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Splitting the data
lalonde_b &amp;lt;- subset(lalonde, race == &amp;quot;black&amp;quot;)
lalonde_h &amp;lt;- subset(lalonde, race == &amp;quot;hispan&amp;quot;)
lalonde_w &amp;lt;- subset(lalonde, race == &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we’ll use full matching because 1:1 matching without replacement, the most common (but worst) way to do propensity score matching, doesn’t work well in this dataset. The process described below works &lt;em&gt;exactly&lt;/em&gt; the same for 1:1 and most other kinds of matching as it does for full matching. We’ll estimate propensity scores in each subgroup, here using probit regression, which happens to yield better balance than logistic regression does.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;MatchIt&amp;quot;)

#Matching in race == &amp;quot;black&amp;quot;
m.out_b &amp;lt;- matchit(treat ~ age + educ + married + nodegree + re74 + re75,
                   data = lalonde_b, method = &amp;quot;full&amp;quot;, estimand = &amp;quot;ATT&amp;quot;,
                   link = &amp;quot;probit&amp;quot;)

#Matching in race == &amp;quot;hispan&amp;quot;
m.out_h &amp;lt;- matchit(treat ~ age + educ + married + nodegree + re74 + re75,
                   data = lalonde_h, method = &amp;quot;full&amp;quot;, estimand = &amp;quot;ATT&amp;quot;,
                   link = &amp;quot;probit&amp;quot;)

#Matching in race == &amp;quot;black&amp;quot;
m.out_w &amp;lt;- matchit(treat ~ age + educ + married + nodegree + re74 + re75,
                   data = lalonde_w, method = &amp;quot;full&amp;quot;, estimand = &amp;quot;ATT&amp;quot;,
                   link = &amp;quot;probit&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-assessing-balance-within-subgroups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Assessing Balance within Subgroups&lt;/h2&gt;
&lt;p&gt;We need to assess subgroup balance; we can do that using &lt;code&gt;summary()&lt;/code&gt; on each &lt;code&gt;matchit&lt;/code&gt; object, or we can use functions from &lt;code&gt;{cobalt}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Below are examples of using &lt;code&gt;summary()&lt;/code&gt; and &lt;code&gt;cobalt::bal.tab()&lt;/code&gt; on one &lt;code&gt;matchit&lt;/code&gt; object at a time&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m.out_b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## matchit(formula = treat ~ age + educ + married + nodegree + re74 + 
##     re75, data = lalonde_b, method = &amp;quot;full&amp;quot;, link = &amp;quot;probit&amp;quot;, 
##     estimand = &amp;quot;ATT&amp;quot;)
## 
## Summary of Balance for All Data:
##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max
## distance        0.6587        0.6121          0.4851     0.7278    0.1134   0.1972
## age            25.9808       26.0690         -0.0121     0.4511    0.0902   0.2378
## educ           10.3141       10.0920          0.1079     0.5436    0.0336   0.0807
## married         0.1859        0.2874         -0.2608          .    0.1015   0.1015
## nodegree        0.7244        0.6437          0.1806          .    0.0807   0.0807
## re74         2155.0132     3117.0584         -0.1881     0.9436    0.0890   0.2863
## re75         1490.7221     1834.4220         -0.1043     1.0667    0.0480   0.1441
## 
## Summary of Balance for Matched Data:
##          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.
## distance        0.6587        0.6577          0.0096     1.0403    0.0095   0.0705          0.0374
## age            25.9808       27.6538         -0.2292     0.3644    0.1148   0.2073          1.3764
## educ           10.3141       10.1368          0.0861     0.6552    0.0228   0.0684          1.0485
## married         0.1859        0.1822          0.0096          .    0.0037   0.0037          0.6236
## nodegree        0.7244        0.7286         -0.0096          .    0.0043   0.0043          0.7548
## re74         2155.0132     2998.6538         -0.1650     0.7590    0.0513   0.2025          0.7256
## re75         1490.7221     2120.7862         -0.1911     0.8819    0.0798   0.1912          0.8430
## 
## Sample Sizes:
##               Control Treated
## All             87.       156
## Matched (ESS)   36.04     156
## Matched         87.       156
## Unmatched        0.         0
## Discarded        0.         0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;cobalt&amp;quot;)
bal.tab(m.out_b, un = TRUE, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance Measures
##              Type Diff.Un  KS.Un Diff.Adj KS.Adj
## distance Distance  0.4851 0.1972   0.0096 0.0705
## age       Contin. -0.0121 0.2378  -0.2292 0.2073
## educ      Contin.  0.1079 0.0807   0.0861 0.0684
## married    Binary -0.1015 0.1015   0.0037 0.0037
## nodegree   Binary  0.0807 0.0807  -0.0043 0.0043
## re74      Contin. -0.1881 0.2863  -0.1650 0.2025
## re75      Contin. -0.1043 0.1441  -0.1911 0.1912
## 
## Sample sizes
##                      Control Treated
## All                    87.       156
## Matched (ESS)          36.04     156
## Matched (Unweighted)   87.       156&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also get a clearer sense of balance overall using &lt;code&gt;bal.tab()&lt;/code&gt; by directly supplying the matching weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Initialize the weights
fm_weights &amp;lt;- numeric(nrow(lalonde))

#Assign the weights based on the subgroup
fm_weights[lalonde$race == &amp;quot;black&amp;quot;] &amp;lt;- m.out_b$weights
fm_weights[lalonde$race == &amp;quot;hispan&amp;quot;] &amp;lt;- m.out_h$weights
fm_weights[lalonde$race == &amp;quot;white&amp;quot;] &amp;lt;- m.out_w$weights

bal.tab(treat ~ age + educ + married + nodegree + re74 + re75,
        data = lalonde, weights = fm_weights, cluster = &amp;quot;race&amp;quot;,
        stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;), abs = TRUE, cluster.summary = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance by cluster
## 
##  - - - Cluster: black - - - 
## Balance Measures
##             Type Diff.Adj KS.Adj
## age      Contin.   0.2292 0.2073
## educ     Contin.   0.0861 0.0684
## married   Binary   0.0037 0.0037
## nodegree  Binary   0.0043 0.0043
## re74     Contin.   0.1650 0.2025
## re75     Contin.   0.1911 0.1912
## 
## Effective sample sizes
##                0   1
## Unadjusted 87.   156
## Adjusted   36.04 156
## 
##  - - - Cluster: hispan - - - 
## Balance Measures
##             Type Diff.Adj KS.Adj
## age      Contin.   0.2298 0.1848
## educ     Contin.   0.2888 0.2762
## married   Binary   0.0604 0.0604
## nodegree  Binary   0.1024 0.1024
## re74     Contin.   0.1323 0.3188
## re75     Contin.   0.1220 0.2351
## 
## Effective sample sizes
##                0  1
## Unadjusted 61.   11
## Adjusted   26.24 11
## 
##  - - - Cluster: white - - - 
## Balance Measures
##             Type Diff.Adj KS.Adj
## age      Contin.   0.4137 0.2126
## educ     Contin.   0.4246 0.1840
## married   Binary   0.0025 0.0025
## nodegree  Binary   0.1653 0.1653
## re74     Contin.   0.2846 0.4165
## re75     Contin.   0.0825 0.1444
## 
## Effective sample sizes
##                 0  1
## Unadjusted 281.   18
## Adjusted    49.49 18
##  - - - - - - - - - - - - - - 
## 
## Balance summary across all clusters
##             Type Mean.Diff.Adj Max.Diff.Adj Mean.KS.Adj Max.KS.Adj
## age      Contin.        0.2909       0.4137      0.2016     0.2126
## educ     Contin.        0.2665       0.4246      0.1762     0.2762
## married   Binary        0.0222       0.0604      0.0222     0.0604
## nodegree  Binary        0.0907       0.1653      0.0907     0.1653
## re74     Contin.        0.1940       0.2846      0.3126     0.4165
## re75     Contin.        0.1319       0.1911      0.1902     0.2351
## 
## Total effective sample sizes across clusters
##                 0   1
## Unadjusted 429.   185
## Adjusted   111.77 185&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;cluster&lt;/code&gt; argument produces balance tables in each subgroup and, because we specified &lt;code&gt;cluster.summary = TRUE&lt;/code&gt;, a balance table summarizing across subgroups. To suppress display of the subgroup-specific balance tables (which may be useful if you have many subgroups), you can specify &lt;code&gt;which.cluster = .none&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To make a plot displaying the balance statistics visually, we can use &lt;code&gt;cobalt::love.plot()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;love.plot(treat ~ age + educ + married + nodegree + re74 + re75,
        data = lalonde, weights = fm_weights, cluster = &amp;quot;race&amp;quot;,
        stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;), abs = TRUE,
        which.cluster = .none, agg.fun = &amp;quot;max&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Standardized mean differences and raw mean differences are present in the same plot. 
## Use the &amp;#39;stars&amp;#39; argument to distinguish between them and appropriately label the x-axis.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ngreifer.github.io/blog/subgroup-analysis-psm/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;See the &lt;code&gt;{cobalt}&lt;/code&gt; &lt;a href=&#34;https://ngreifer.github.io/cobalt/articles/cobalt_A4_love.plot.html&#34;&gt;vignette on customizing &lt;code&gt;love.plot()&lt;/code&gt;&lt;/a&gt; to see how to finely control the appearance of the plot.&lt;/p&gt;
&lt;p&gt;From this output, we can see that balance is actually pretty bad; the greatest standardized mean difference (SMD) across subgroups after matching is around .46, which is way too big. In a realistic scenario, we would try different matching methods, maybe resorting to weighting, until we found good balance across the subgroups. In order to validly interpret the subgroup-specific effects and tests for moderation, we need to achieve balance in each subgroup, not just overall. We didn’t get good balance here, but to stay focused on the rest of the procedure, we’ll move forward as if we did.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-fitting-the-outcome-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Fitting the Outcome Model&lt;/h2&gt;
&lt;p&gt;Next, we’ll fit the outcome model. It’s important to remember that the outcome model is an intermediate step for estimating the treatment effect; no quantity estimated by the model needs to correspond to the treatment effect directly. We’ll be using a marginal effects procedure to estimate the treatment effects in the next section.&lt;/p&gt;
&lt;p&gt;First, we’ll extract the matched datasets from the &lt;code&gt;matchit&lt;/code&gt; objects. We can’t just use the matching weights we extracted earlier because we also need subclass (i.e., pair) membership. We’ll use &lt;code&gt;match.data()&lt;/code&gt; from &lt;code&gt;{MatchIt}&lt;/code&gt; to extract the matched datasets, which contain the matching weights and subclass membership in the &lt;code&gt;weights&lt;/code&gt; and &lt;code&gt;subclass&lt;/code&gt; columns, respectively, and use &lt;code&gt;rbind()&lt;/code&gt; to bind them into a single combined dataset&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Extract the matched datasets
matched_data_b &amp;lt;- match.data(m.out_b)
matched_data_h &amp;lt;- match.data(m.out_h)
matched_data_w &amp;lt;- match.data(m.out_w)

#Combine them using rbind()
matched_data &amp;lt;- rbind(matched_data_b,
                      matched_data_h,
                      matched_data_w)

names(matched_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;treat&amp;quot;    &amp;quot;age&amp;quot;      &amp;quot;educ&amp;quot;     &amp;quot;race&amp;quot;     &amp;quot;married&amp;quot;  &amp;quot;nodegree&amp;quot; &amp;quot;re74&amp;quot;     &amp;quot;re75&amp;quot;     &amp;quot;re78&amp;quot;     &amp;quot;re78_0&amp;quot;   &amp;quot;distance&amp;quot; &amp;quot;weights&amp;quot;  &amp;quot;subclass&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we can fit the outcome model. The choice of which model to fit should depend primarily on the best model for the outcome; because we have a binary outcome, we’ll use logistic regression.&lt;/p&gt;
&lt;p&gt;It’s usually a good idea to include covariates in the outcome model. It’s also usually a good idea to allow the treatment to interact with the covariates in the outcome model. It’s also usually a good idea to fit separate models within each subgroup. Combining this all yields a pretty complicated model, which is why it will be so important to use a marginal effects procedure rather than trying to interpret the model’s coefficients. Here’s how we fit this model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glm(re78_0 ~ race * (treat * (age + educ + married + nodegree +
                                       re74 + re75)),
           data = matched_data, weights = weights,
           family = &amp;quot;quasibinomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re not even going to look at the output of this model, which has 42 parameters. If the model doesn’t fit with your dataset, you can remove interactions between the treatment and some covariates or remove the covariates altogether.&lt;/p&gt;
&lt;p&gt;For a linear model, you can use &lt;code&gt;lm()&lt;/code&gt; and remove the &lt;code&gt;family&lt;/code&gt; argument. We used &lt;code&gt;family = &#34;quasibinomial&#34;&lt;/code&gt; because we want logistic regression for our binary outcome but we are using the matching weights, which otherwise create a (harmless but annoying) warning when run with &lt;code&gt;family = &#34;binomial&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-estimate-the-treatment-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Estimate the Treatment Effects&lt;/h2&gt;
&lt;p&gt;Finally, we can estimate the treatment effects. To do so, we’ll use an average marginal effects procedure as implemented in &lt;code&gt;{marginaleffects}&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. First, we’ll estimate the average marginal effect overall, averaging across the subgroups. Again, we’re hoping for a negative treatment effect, which indicates the risk of having zero income decreased among those who received the treatment. Because we are estimating the ATT, we need to subset the data for which the average marginal effects are computed to just the treated units, which we do using the &lt;code&gt;newdata&lt;/code&gt; argument (which can be omitted when the ATE is the target estimand). We also need to supply pair membership to ensure the standard errors are correctly computed, which we do by supplying the &lt;code&gt;subclass&lt;/code&gt; variable containing pair membership to the &lt;code&gt;vcov&lt;/code&gt; argument. In general, we need to supply the weights to the &lt;code&gt;wts&lt;/code&gt; argument of &lt;code&gt;avg_comparisons()&lt;/code&gt; as well (though, in this case, because we are estimating the ATT and all weights are 1 for the treated group, it doesn’t make a difference).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;marginaleffects&amp;quot;)

#Estimate the overall ATT
avg_comparisons(fit, variables = &amp;quot;treat&amp;quot;,
                newdata = subset(matched_data, treat == 1),
                vcov = ~subclass, wts = &amp;quot;weights&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Term Contrast Estimate Std. Error      z Pr(&amp;gt;|z|)  2.5 % 97.5 %
##  treat    1 - 0  0.03434    0.04405 0.7795  0.43566 -0.052 0.1207
## 
## Prediction type:  response 
## Columns: type, term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimated risk difference is 0.02305 with a high p-value and a confidence interval containing 0, indicating no evidence of an effect overall. (Note: this doesn’t mean there is no effect! The data are compatible with effects anywhere within the confidence interval, which includes negative and positive effects of a moderate size!)&lt;/p&gt;
&lt;p&gt;New, let’s estimate the subgroup-specific effects by supplying the subgrouping variable, &lt;code&gt;&#34;race&#34;&lt;/code&gt;, to the &lt;code&gt;by&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg_comparisons(fit, variables = &amp;quot;treat&amp;quot;,
                newdata = subset(matched_data, treat == 1),
                vcov = ~subclass, wts = &amp;quot;weights&amp;quot;,
                by = &amp;quot;race&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   Term          Contrast   race Estimate Std. Error      z  Pr(&amp;gt;|z|)    2.5 %   97.5 %
##  treat mean(1) - mean(0)  black  0.06985    0.05168  1.352 0.1764930 -0.03144  0.17114
##  treat mean(1) - mean(0) hispan -0.18744    0.07293 -2.570 0.0101667 -0.33038 -0.04450
##  treat mean(1) - mean(0)  white -0.13790    0.04886 -2.822 0.0047678 -0.23367 -0.04214
## 
## Prediction type:  response 
## Columns: type, term, contrast, race, estimate, std.error, statistic, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we see that actually there is evidence of treatment effects within subgroups! In the subgroups &lt;code&gt;hispan&lt;/code&gt; and &lt;code&gt;white&lt;/code&gt;, we see moderately sized negative effects with small p-values and confidence intervals excluding 0, suggesting that there treatment effects in these subgroups.&lt;/p&gt;
&lt;p&gt;We can also test whether the treatment effects differ between groups using the &lt;code&gt;hypothesis&lt;/code&gt; argument of &lt;code&gt;avg_comparisons()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg_comparisons(fit, variables = &amp;quot;treat&amp;quot;,
                newdata = subset(matched_data, treat == 1),
                vcov = ~subclass, wts = &amp;quot;weights&amp;quot;,
                by = &amp;quot;race&amp;quot;, hypothesis = &amp;quot;pairwise&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##                                                                Term Estimate Std. Error       z  Pr(&amp;gt;|z|)    2.5 % 97.5 %
##  (black,treat,mean(1) - mean(0)) - (hispan,treat,mean(1) - mean(0))  0.25729    0.08939  2.8785 0.0039961  0.08210 0.4325
##   (black,treat,mean(1) - mean(0)) - (white,treat,mean(1) - mean(0))  0.20775    0.07112  2.9211 0.0034877  0.06836 0.3471
##  (hispan,treat,mean(1) - mean(0)) - (white,treat,mean(1) - mean(0)) -0.04954    0.08779 -0.5643 0.5725398 -0.22160 0.1225
## 
## Prediction type:  response 
## Columns: type, term, estimate, std.error, statistic, p.value, conf.low, conf.high&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see evidence that the treatment effect differs between the &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;hispan&lt;/code&gt; groups, and between the &lt;code&gt;black&lt;/code&gt; and &lt;code&gt;white&lt;/code&gt; groups. With many subgroups, it might be useful to adjust your p-values for multiple comparisons, which we can do using &lt;code&gt;p.adjust()&lt;/code&gt;, e.g.,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p.adjust(comp$p.value, method = &amp;quot;holm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if &lt;code&gt;comp&lt;/code&gt; contained the &lt;code&gt;avg_comparisons()&lt;/code&gt; output above.&lt;/p&gt;
&lt;p&gt;Congratulations! You’ve done a subgroup analysis!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-reporting-your-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5: Reporting Your Results&lt;/h2&gt;
&lt;p&gt;A fair bit needs to be included when reporting your results to ensure your analysis is replicable and can be correctly interpreted by your audience. The key things to report are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The method of estimating the propensity score and performing the matching (noting that these were done within subgroups), including the estimand targeted and whether that estimand was respected by the procedure (using, e.g., a caliper changes the estimand from the one you specify). This should also include the packages used and, even better, the functions used. If you’re using &lt;code&gt;{MatchIt}&lt;/code&gt;, the documentation should also tell you which papers to cite.&lt;/li&gt;
&lt;li&gt;A quick summary of other methods you might have tried and why you went with the one you went with (i.e., because it yielded better balance, a greater effective sample size, etc.).&lt;/li&gt;
&lt;li&gt;Covariate balance, measured broadly; this can include a balance table, a balance plot (like one produced by &lt;code&gt;cobalt::love.plot()&lt;/code&gt;), or a summary of balance (like providing the largest SMD and KS statistic observed across subgroups). Make sure your description of balance reflects the subgroups, e.g., by having separate tables or plots for each subgroup or clarifying that the statistics presented are averages or the worst case across subgroups.&lt;/li&gt;
&lt;li&gt;The outcome model you used, especially specifying the form of the model used and how/whether covariates entered the model. Also mention the method used to compute the standard errors (e.g., cluster-robust standard errors with pair membership as the clustering variable).&lt;/li&gt;
&lt;li&gt;Details of the marginal effects procedure used, including the package used, and the method to compute the standard errors (in this case, the delta method, which is the only method available in &lt;code&gt;{marginaleffects}&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;The treatment effect estimates along with their p-values and confidence intervals, both overall and within subgroups.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-austinIntroductionPropensityScore2011&#34; class=&#34;csl-entry&#34;&gt;
Austin, Peter C. 2011. &lt;span&gt;“An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.”&lt;/span&gt; &lt;em&gt;Multivariate Behavioral Research&lt;/em&gt; 46 (3): 399–424. &lt;a href=&#34;https://doi.org/10.1080/00273171.2011.568786&#34;&gt;https://doi.org/10.1080/00273171.2011.568786&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-greenExaminingModerationAnalyses2014&#34; class=&#34;csl-entry&#34;&gt;
Green, Kerry M., and Elizabeth A. Stuart. 2014. &lt;span&gt;“Examining Moderation Analyses in Propensity Score Methods: &lt;span&gt;Application&lt;/span&gt; to Depression and Substance Use.”&lt;/span&gt; &lt;em&gt;Journal of Consulting and Clinical Psychology&lt;/em&gt;, Advances in &lt;span&gt;Data Analytic Methods&lt;/span&gt;, 82 (5): 773–83. &lt;a href=&#34;https://doi.org/10.1037/a0036515&#34;&gt;https://doi.org/10.1037/a0036515&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-greiferMatchingMethodsConfounder2021a&#34; class=&#34;csl-entry&#34;&gt;
Greifer, Noah, and Elizabeth A Stuart. 2021. &lt;span&gt;“Matching &lt;span&gt;Methods&lt;/span&gt; for &lt;span&gt;Confounder Adjustment&lt;/span&gt;: &lt;span&gt;An Addition&lt;/span&gt; to the &lt;span&gt;Epidemiologist&lt;/span&gt;’s &lt;span&gt;Toolbox&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Epidemiologic Reviews&lt;/em&gt;, June, mxab003. &lt;a href=&#34;https://doi.org/10.1093/epirev/mxab003&#34;&gt;https://doi.org/10.1093/epirev/mxab003&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You might notices the mean differences for binary variables differ between the two outputs; that’s because &lt;code&gt;summary()&lt;/code&gt; standardizes the mean differences whereas &lt;code&gt;bal.tab()&lt;/code&gt; does not for binary variables. If you want standardized mean differences for binary variables from &lt;code&gt;bal.tab()&lt;/code&gt;, just add the argument &lt;code&gt;binary = &#34;std&#34;&lt;/code&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note: &lt;code&gt;rbind()&lt;/code&gt; must be used for this; functions from other packages, like &lt;code&gt;dplyr::bind_rows()&lt;/code&gt;, will not correctly preserve the subclass structure.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;This requires version 0.9.0 ore greater of &lt;code&gt;{marginaleffects}&lt;/code&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://ngreifer.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>Software</title>
      <link>https://ngreifer.github.io/software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/software/</guid>
      <description>&lt;p&gt;This page documents the software packages I have worked on. If you have any questions about them, please submit your question to their GitHub issues page rather than emailing me. You are also welcome to ask a question on &lt;a href=&#34;https://stackoverflow.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackOverflow&lt;/a&gt; or &lt;a href=&#34;https://stats.stackexchange.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrossValidated&lt;/a&gt;, which I check often.&lt;/p&gt;
&lt;h2 id=&#34;r-packages&#34;&gt;R packages&lt;/h2&gt;
&lt;p&gt;These packages are ones that I am a primary author on and have expertise on the methods implemented. I consider these packages to be &amp;ldquo;mine&amp;rdquo;, at least partly, in the sense that I can speak not only on the implementation but on the methods as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cobalt&lt;/code&gt;: Covariate Balance Tables and Plots
&lt;ul&gt;
&lt;li&gt;Noah Greifer | &lt;a href=&#34;https://ngreifer.github.io/cobalt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cran.r-project.org/package=cobalt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/ngreifer/cobalt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WeightIt&lt;/code&gt;: Weighting for Covariate Balance in Observational Studies
&lt;ul&gt;
&lt;li&gt;Noah Greifer | &lt;a href=&#34;https://ngreifer.github.io/WeightIt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cran.r-project.org/package=WeightIt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/ngreifer/WeightIt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MatchIt&lt;/code&gt;: Nonparametric Preprocessing for Parametric Causal Inference
&lt;ul&gt;
&lt;li&gt;Daniel Ho, Kosuke Imai, Gary King, Elizabeth Stuart, and Noah Greifer | &lt;a href=&#34;https://kosukeimai.github.io/MatchIt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cran.r-project.org/package=MatchIt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/kosukeimai/MatchIt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MatchThem&lt;/code&gt;: Matching and Weighting Multiply Imputed Datasets
&lt;ul&gt;
&lt;li&gt;Farhad Pishgar and Noah Greifer | &lt;a href=&#34;https://cran.r-project.org/package=MatchThem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/FarhadPishgar/MatchThem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;optweight&lt;/code&gt;: Targeted Stable Balancing Weights Using Optimization
&lt;ul&gt;
&lt;li&gt;Noah Greifer | &lt;a href=&#34;https://cran.r-project.org/package=optweight&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/ngreifer/optweight&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MatchingFrontier&lt;/code&gt;: Computation of the Balance-Sample Size Frontier in Matching Methods for Causal Inference
&lt;ul&gt;
&lt;li&gt;Gary King, Christopher Lucas, Richard Nielsen, and Noah Greifer | &lt;a href=&#34;https://iqss.github.io/MatchingFrontier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://github.com/iqss/MatchingFrontier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fwb&lt;/code&gt;: Fractional Weighted Bootstrap
&lt;ul&gt;
&lt;li&gt;Noah Greifer | &lt;a href=&#34;https://ngreifer.github.io/fwb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cran.r-project.org/package=fwb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/ngreifer/fwb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clarify&lt;/code&gt;: Simulation-Based Inference for Regression Models
&lt;ul&gt;
&lt;li&gt;Noah Greifer, Steven Worthington, Stefano Iacus, and Gary King | &lt;a href=&#34;https://iqss.github.io/clarify/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cloud.r-project.org/package=clarify&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/iqss/clarify/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These packages are ones that I have developed as part of my job but which I don&amp;rsquo;t consider &amp;ldquo;mine&amp;rdquo; in the sense that I am not the primary maintainer and I don&amp;rsquo;t have expertise on the methods implemented. Please do not contact me about these packages.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;netlit&lt;/code&gt;: Augment a literature review with network analysis statistics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Devin Judge-Lord and Noah Greifer | &lt;a href=&#34;https://judgelord.github.io/netlit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://github.com/judgelord/netlit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EvoPhylo&lt;/code&gt;: Pre- And Postprocessing of Morphological Data from Relaxed Clock Bayesian Phylogenetics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tiago Simões, Noah Greifer, and Stephanie Pierce | &lt;a href=&#34;https://tiago-simoes.github.io/EvoPhylo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cran.r-project.org/package=EvoPhylo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/tiago-simoes/EvoPhylo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Morphoscape&lt;/code&gt;: Computation and Visualization of Adaptive Landscapes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blake Dickson, Stephanie Pierce, and Noah Greifer | &lt;a href=&#34;https://blakedickson.github.io/Morphoscape/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt; | &lt;a href=&#34;https://cran.r-project.org/package=Morphoscape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt; | &lt;a href=&#34;https://github.com/blakedickson/Morphoscape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
