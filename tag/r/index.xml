<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Noah Greifer</title>
    <link>https://ngreifer.github.io/tag/r/</link>
      <atom:link href="https://ngreifer.github.io/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 08 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://ngreifer.github.io/media/sharing.jpg</url>
      <title>R</title>
      <link>https://ngreifer.github.io/tag/r/</link>
    </image>
    
    <item>
      <title>Genetic Matching, from the Bottom Up</title>
      <link>https://ngreifer.github.io/blog/genetic-matching/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://ngreifer.github.io/blog/genetic-matching/</guid>
      <description>


&lt;p&gt;Genetic matching sounds cool and science-y, something we social scientists love because nobody thinks what we do is “real” science. And genetic matching is cool and science-y, but not because it has anything to do with genes or DNA. Genetic matching is a method of adjusting for confounding in observational studies; it is a close relative of propensity score matching and Mahalanobis distance matching and serves exactly the same purpose. &lt;span class=&#34;citation&#34;&gt;Sekhon (&lt;a href=&#34;#ref-sekhonMultivariatePropensityScore2011&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; describe genetic matching, but I’ll explain it here in simple terms and with an emphasis on its generality, which is undersold by its implementations.&lt;/p&gt;
&lt;p&gt;This post won’t make any sense if you don’t know what matching in general is. Go read &lt;span class=&#34;citation&#34;&gt;Stuart (&lt;a href=&#34;#ref-stuartMatchingMethodsCausal2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Greifer and Stuart (&lt;a href=&#34;#ref-greiferMatchingMethodsConfounder2021a&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt;, and the &lt;code&gt;MatchIt&lt;/code&gt; &lt;a href=&#34;https://kosukeimai.github.io/MatchIt/articles/matching-methods.html&#34;&gt;vignette&lt;/a&gt; on matching methods to learn about them. The focus here will be on &lt;em&gt;pair matching&lt;/em&gt;, which involves assigning units to pairs or strata based on the distances between them, then discarding unpaired units.&lt;/p&gt;
&lt;p&gt;The goal of matching is balanced samples, i.e., samples where the distribution of covariates in the treated and control groups is the same so that an estimated treatment effect cannot be said to be due to differences in the covariate distributions. Why, then, do we make pairs? Close pairs create balance, in theory. How do we compute how close units are to each other? There are several ways; a common one is the Mahalanobis distance, as described for matching in &lt;span class=&#34;citation&#34;&gt;Rubin (&lt;a href=&#34;#ref-rubinBiasReductionUsing1980&#34; role=&#34;doc-biblioref&#34;&gt;1980&lt;/a&gt;)&lt;/span&gt;, and which I’ll describe here.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Mahalanobis distance&lt;/strong&gt; between two units &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\delta^{md}_{i,j}=\sqrt{(\mathbf{x}_i-\mathbf{x}_j)\Sigma^{-1}(\mathbf{x}_i-\mathbf{x}_j)&amp;#39;}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i\)&lt;/span&gt; is the vector of covariates for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (i.e., that unit’s row in the dataset) and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; is the covariance matrix of the covariates&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Equivalently, the Mahalanobis distance is the Euclidean distance (i.e., the regular distance) computed on the standardized principal components. The Mahalanobis distance is an improvement over the Euclidean distance of the covariates because it standardizes the covariates to be on the same scale and adjusts for correlations between covariates (so two highly correlated variables only count once). A great description of the Mahalanobis distance is &lt;a href=&#34;https://stats.stackexchange.com/a/62147/116195&#34;&gt;here&lt;/a&gt; (though there it is not described in the context of matching).&lt;/p&gt;
&lt;p&gt;Genetic matching concerns a generalization of the Mahalanobis distance, called the &lt;strong&gt;generalized Mahalanobis distance&lt;/strong&gt;, which additionally involves a weight matrix. The generalized Mahalanobis distance is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\delta^{gmd}_{i,j}(W)=\sqrt{(\mathbf{x}_i-\mathbf{x}_j)&amp;#39;\left(\Sigma^{-\frac{1}{2}}\right)&amp;#39; W\Sigma^{-\frac{1}{2}}(\mathbf{x}_i-\mathbf{x}_j)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^{-\frac{1}{2}}\)&lt;/span&gt; is the “square root” of the inverse of the covariance matrix (e.g., the Cholesky decomposition), and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is a symmetric weight matrix that can contain anything but in most cases is a diagonal matrix with a scalar weight for each covariate in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; (not weights for each unit like in propensity score weighting; a weight for each &lt;em&gt;covariate&lt;/em&gt;), i.e., &lt;span class=&#34;math inline&#34;&gt;\(W = \text{diag}(\begin{bmatrix} w_1 &amp;amp; \dots &amp;amp; w_p \end{bmatrix})\)&lt;/span&gt;. The generalized Mahalanobis distance is equal to the usual Mahalanobis distance when &lt;span class=&#34;math inline&#34;&gt;\(W=I\)&lt;/span&gt;, the identity matrix.&lt;/p&gt;
&lt;p&gt;What does any of this have to do with genetic matching? Well, “genetic matching” is a bit of a misnomer; it’s not a matching method. It’s a method of estimating &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. Genetic matching finds the &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; that, when incorporated in a generalized Mahalanobis distance used to match treated and control units, yields the best balance. Once you have found &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, you then do a regular round of matching, and that is your matched sample.&lt;/p&gt;
&lt;p&gt;To put it slightly more formally, consider a function &lt;span class=&#34;math inline&#34;&gt;\(\text{match}(\delta)\)&lt;/span&gt;, which takes in a distance matrix &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; and produces a matched set of treated and control units, characterized by a set of matching weights (e.g., 1 if matched, 0 if unmatched) and pair membership for each unit. Consider a function &lt;span class=&#34;math inline&#34;&gt;\(\text{imbalance}(m)\)&lt;/span&gt;, which takes in the output of a &lt;span class=&#34;math inline&#34;&gt;\(\text{match}(\delta)\)&lt;/span&gt; and returns a scalar imbalance metric (e.g., the largest absolute standardized mean difference among all the covariates). We can then write the genetic matching problem as the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\underset{W}{\operatorname{arg\,min}} \, \text{imbalance}(\text{match}(\delta^{gmd}(W)))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Genetic matching is very general; there are many ways to do the matching (i.e., many ways to specify the &lt;span class=&#34;math inline&#34;&gt;\(\text{match}()\)&lt;/span&gt; function) and many ways to characterize imbalance (i.e., many ways to specify the &lt;span class=&#34;math inline&#34;&gt;\(\text{imbalance}()\)&lt;/span&gt; function) (and even several ways to specific &lt;span class=&#34;math inline&#34;&gt;\(\delta()\)&lt;/span&gt;!). Although nearest neighbor matching is often used for &lt;span class=&#34;math inline&#34;&gt;\(\text{match}()\)&lt;/span&gt;, any matching method that uses a distance matrix could be as well. A specific imbalance measure (which I’ll explain in more detail later) is most often used for &lt;span class=&#34;math inline&#34;&gt;\(\text{imbalance}()\)&lt;/span&gt; because it is the default in the software that implements genetic matching, but any imbalance measure could be used, and there has been research that indicates that alternative measures may work better.&lt;/p&gt;
&lt;p&gt;You may be wondering where the “genetic” part of “genetic matching” comes in. “Genetic” comes from the name of the optimization algorithm that is used to solve the genetic matching problem stated above, which is just called the genetic algorithm. In principle, though, any optimization routine could be used; the genetic algorithm was chosen specifically because it deals well with nonsmooth surfaces, which the objective function above surely is. But other optimization methods that do not rely on derivatives do as well, such as “particle swarm optimization” (we’re really doing &lt;em&gt;science&lt;/em&gt; here). I don’t really understand these methods deeply, but we don’t have to to understand what genetic matching is doing&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. In order to understand how to tune the algorithm, though, there are some bits worth knowing about, which I’ll briefly cover in the Implementation section below.&lt;/p&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;p&gt;Genetic matching is implemented in the &lt;code&gt;{Matching}&lt;/code&gt; package in R, which performs genetic matching to estimate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, performs nearest neighbor matching using &lt;span class=&#34;math inline&#34;&gt;\(\delta^{gmd}(W)\)&lt;/span&gt; or another distance matrix, and then estimates the treatment effect&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. The &lt;code&gt;GenMatch()&lt;/code&gt; function estimates &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, and the &lt;code&gt;Match()&lt;/code&gt; function does the matching on the resulting output&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Genetic matching is also available in &lt;code&gt;{MatchIt}&lt;/code&gt; by setting &lt;code&gt;method = &#34;genetic&#34;&lt;/code&gt; in the call to &lt;code&gt;matchit()&lt;/code&gt;, but it just calls &lt;code&gt;GenMatch()&lt;/code&gt; and &lt;code&gt;Match()&lt;/code&gt; from &lt;code&gt;{Matching}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;GenMatch()&lt;/code&gt; relies on &lt;code&gt;rgenoud::genoud()&lt;/code&gt;, one implementation of the genetic algorithm in R. There are a few tuning parameters worth understanding to use genetic matching to its full potential. The most important one is the population size (i.e., the number of candidates in each generation of the genetic algorithm), controlled by the &lt;code&gt;pop.size&lt;/code&gt; argument. All you need to know is that high values are better and slower. Another one perhaps worth knowing about is the number of generations that have to pass with no improvement in the objective function before the algorithm halts and returns the best candidate it has found, controlled by the &lt;code&gt;wait.generations&lt;/code&gt; argument. Here, too, higher values are better and slower.&lt;/p&gt;
&lt;p&gt;A detail I haven’t emphasized is that the matching method used to to produce the final matched sample using the estimated &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; should be the same one used in estimating $W$, because the estimated &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; are tailored to that matching method (i.e., they only optimize balance when supplied to that &lt;span class=&#34;math inline&#34;&gt;\(\text{match}()\)&lt;/span&gt; function)&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. The matching methods available in &lt;code&gt;{Matching}&lt;/code&gt; are nearest neighbor matching with or without replacement, with or without calipers or exact matching constraints, and with &lt;span class=&#34;math inline&#34;&gt;\(1:1\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(k:1\)&lt;/span&gt; matching. This is a pretty broad set of matching options, though it is not complete (e.g., optimal and full matching are not available). One thing about genetic matching is that it is &lt;em&gt;slow&lt;/em&gt;, so using a fast matching method is useful for not spending forever to get your matches. &lt;code&gt;{Matching}&lt;/code&gt; uses a fast implementation of nearest neighbor matching programmed in C, which makes it fairly fast, though still quite slow for even moderately sized problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-imbalance-measure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Imbalance Measure&lt;/h3&gt;
&lt;p&gt;The imbalance measure used in genetic matching is critical to its success as a method. Seeking balance using a poor metric means the resulting matched sample will not be able to reduce bias well, even if the optimal values of &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; that minimize that imbalance measure have been found. One challenge is that there is no clear best imbalance measure to use. Ideally, it should incorporate balance on all covariates, and not just on their means but on their full distributions, and not just the marginal distributions but the joint distributions. The best imbalance measure depends heavily on the true outcome-generating model, which is inherently unknowable (otherwise we wouldn’t be doing matching in the first place), though there has been some research into it.&lt;/p&gt;
&lt;p&gt;By default, the imbalance measure &lt;code&gt;GenMatch()&lt;/code&gt; uses is the smallest p-value among the sets of two-sample t-tests and Kolmogorov-Smirnov (KS) tests for each covariate. This is a bit of a strange imbalance measure that doesn’t really show up anywhere else in the literature. &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; justify the use of p-values (which are typically disregarded as methods to assess balance) by arguing that here they are simply used to put the mean differences and KS statistic on a uniform scale rather than to be interpreted as p-values to be used in a hypothesis test. However, there has been research into other balance criteria that might perform better. &lt;span class=&#34;citation&#34;&gt;Oyenubi and Wittenberg (&lt;a href=&#34;#ref-oyenubiDoesChoiceBalancemeasure2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; find that the largest value of a univariate balance measure called the “entropic distance”, which is a relative of the KS statistic, performs well as an imbalance measure. &lt;span class=&#34;citation&#34;&gt;Zhu, Savage, and Ghosh (&lt;a href=&#34;#ref-zhuKernelBasedMetricBalance2018&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; find that a multivariate imbalance measure called the “kernel distance” does well; this measure takes into account the full, joint covariate distribution, unlike the other methods which do not consider the joint distribution, explaining its effectiveness. I am partial to the energy distance &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rizzoEnergyDistance2016&#34; role=&#34;doc-biblioref&#34;&gt;Rizzo and Székely 2016&lt;/a&gt;; &lt;a href=&#34;#ref-hulingEnergyBalancingCovariate2022&#34; role=&#34;doc-biblioref&#34;&gt;Huling and Mak 2022&lt;/a&gt;)&lt;/span&gt;, which is demonstrated to have nice properties and is easy to explain and calculate. Simple balance measures can be effective as well, though; &lt;span class=&#34;citation&#34;&gt;Oyenubi and Wittenberg (&lt;a href=&#34;#ref-oyenubiDoesChoiceBalancemeasure2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Stuart, Lee, and Leacy (&lt;a href=&#34;#ref-stuartPrognosticScorebasedBalance2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; find that standardized mean differences can be effective in assessing balance, even though they only take into account the covariate means and do not consider the joint distribution of the covariates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-covariates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Covariates&lt;/h3&gt;
&lt;p&gt;The generalized Mahalanobis distance depends on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;–the covariates, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;–the “scaling” matrix (usually the covariance matrix), and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;–the weights matrix. These, of course, can all be specified in a variety of ways. &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt; should contain the covariates one would like balance on, though in principle it doesn’t have to, as long as those covariates are included in the imbalance measure. For example, one might only include 3 of the most important covariates in the calculation of the distance and weights, but optimize balance on all 10 covariates included in the analysis. &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; recommend including the propensity score in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;, as close pairs on the propensity score tends to yield well-balanced samples (which is the motivation behind propensity score matching in the first place). On the other hand, &lt;span class=&#34;citation&#34;&gt;King and Nielsen (&lt;a href=&#34;#ref-kingWhyPropensityScores2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; recommend against including the propensity score if balance can be achieved without it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Below are some examples of genetic matching. First we’ll use &lt;code&gt;{Matching}&lt;/code&gt;, which gives us a bit more insight into how the process goes, and then we’ll perform the same analysis using &lt;code&gt;{MatchIt}&lt;/code&gt; to demonstrate how much easier it is. We’ll use the &lt;code&gt;lalonde&lt;/code&gt; dataset in &lt;code&gt;{MatchIt}&lt;/code&gt; for this analysis&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;using-matching&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using &lt;code&gt;Matching&lt;/code&gt;&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;lalonde&amp;quot;, package = &amp;quot;MatchIt&amp;quot;)

covs &amp;lt;- lalonde |&amp;gt; subset(select = c(age, educ, married,
                                     race, nodegree,
                                     re74, re75))
treat &amp;lt;- lalonde$treat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a factor variable (&lt;code&gt;race&lt;/code&gt;) among our covariates, so we need to turn it into a set of dummy variables for &lt;code&gt;{Matching}&lt;/code&gt; . The &lt;code&gt;{cobalt}&lt;/code&gt; function &lt;code&gt;splitfactor()&lt;/code&gt; makes this easy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;covs &amp;lt;- covs |&amp;gt; cobalt::splitfactor(drop.first = FALSE)

head(covs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      age educ married race_black race_hispan race_white nodegree re74 re75
## NSW1  37   11       1          1           0          0        1    0    0
## NSW2  22    9       0          0           1          0        1    0    0
## NSW3  30   12       0          1           0          0        0    0    0
## NSW4  27   11       0          1           0          0        1    0    0
## NSW5  33    8       0          1           0          0        1    0    0
## NSW6  22    9       0          1           0          0        1    0    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll estimate a propensity score to include among the covariates, as recommended by &lt;span class=&#34;citation&#34;&gt;Diamond and Sekhon (&lt;a href=&#34;#ref-diamondGeneticMatchingEstimating2013&#34; role=&#34;doc-biblioref&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Logistic regression PS
ps &amp;lt;- glm(treat ~ age + educ + married + race +
            nodegree + re74 + re75, data = lalonde,
          family = binomial) |&amp;gt;
  fitted()

## Append the PS to the covariates
covs_ps &amp;lt;- cbind(ps, covs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay, now we’re finally ready to use functions in &lt;code&gt;{Matching}&lt;/code&gt; to perform genetic matching. The first step is to use &lt;code&gt;GenMatch()&lt;/code&gt; to compute &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, and after that we will use &lt;code&gt;Match()&lt;/code&gt; to perform the matching using the &lt;code&gt;GenMatch()&lt;/code&gt; output. To use &lt;code&gt;GenMatch()&lt;/code&gt;, we have to know what kind of matching we eventually want to do. In this example, we’ll do 2:1 matching with replacement for the ATT. &lt;code&gt;{Matching}&lt;/code&gt; has a few extra quirks that need to be addressed to make the matching work as intended, which I’ll include in the code below without much explanation (since my recommendation is to use &lt;code&gt;{MatchIt}&lt;/code&gt; anyway, which takes care of these automatically).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matching)
# Set seed for reproducibility; genetic matching has a random
# component
set.seed(333)
Gen_out &amp;lt;- GenMatch(
  Tr = treat,             #Treatment
  X = covs_ps,            #Covariates to match on
  BalanceMatrix = covs,   #Covariance to balance
  estimand = &amp;quot;ATT&amp;quot;,       #Estimand
  M = 2,                  #2:1 matching
  replace = TRUE,         #With replacement
  ties = FALSE,           #No ties
  distance.tolerance = 0, #Use precise values
  print.level = 0,        #Don&amp;#39;t print output
  pop.size = 200          #Genetic population size; bigger is better
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important part of the &lt;code&gt;GenMatch()&lt;/code&gt; output is the &lt;code&gt;Weight.matrix&lt;/code&gt;, which corresponds to &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;. It’s not really worth interpreting the weights; they are just whatever values happened to yield the best balance and don’t actually tell you anything about how important any covariate is to the treatment. We can supply the weights to the &lt;code&gt;Match()&lt;/code&gt; function to do a final round of matching. All the arguments related to matching (e.g., &lt;code&gt;estimand&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;replace&lt;/code&gt;, etc.) should be the same between &lt;code&gt;GenMatch()&lt;/code&gt; and &lt;code&gt;Match()&lt;/code&gt;. We call &lt;code&gt;Match()&lt;/code&gt; below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Match_out &amp;lt;- Match(
  Tr = treat,             #Treatment
  X = covs_ps,            #Covariates to match on
  estimand = &amp;quot;ATT&amp;quot;,       #Estimand
  M = 2,                  #2:1 matching
  replace = TRUE,         #With replacement
  ties = FALSE,           #No ties
  distance.tolerance = 0, #Use precise values
  Weight.matrix = Gen_out$Weight.matrix,
  Weight = 3              #Tell Match() we&amp;#39;re using Weight.matrix
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can take a look at the balance using &lt;code&gt;cobalt::bal.tab()&lt;/code&gt;. Here, we check balance not only on the means but also on the KS statistics, since those are part of what is being optimized by the genetic optimization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cobalt::bal.tab(Match_out, treat ~ age + educ + married + race +
                  nodegree + re74 + re75, data = lalonde,
                stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance Measures
##                Type Diff.Adj KS.Adj
## age         Contin.  -0.0178 0.1378
## educ        Contin.   0.0686 0.0459
## married      Binary   0.0000 0.0000
## race_black   Binary   0.0054 0.0054
## race_hispan  Binary   0.0000 0.0000
## race_white   Binary  -0.0054 0.0054
## nodegree     Binary   0.0135 0.0135
## re74        Contin.   0.0305 0.1270
## re75        Contin.   0.0923 0.0919
## 
## Sample sizes
##                      Control Treated
## All                    429.      185
## Matched (ESS)           42.1     185
## Matched (Unweighted)   121.      185
## Unmatched              308.        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below we’ll use &lt;code&gt;MatchIt&lt;/code&gt;, which does everything (adjusting the covariate matrix, estimating propensity scores, optimizing &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, and matching on the new distance matrix) all at once.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-matchit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using &lt;code&gt;MatchIt&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;All we need to do is supply the usual arguments to &lt;code&gt;matchit()&lt;/code&gt; and set &lt;code&gt;method = &#34;genetic&#34;&lt;/code&gt;. See the &lt;code&gt;MatchIt&lt;/code&gt; &lt;a href=&#34;https://kosukeimai.github.io/MatchIt/articles/MatchIt.html&#34;&gt;vignettes&lt;/a&gt; for information on the basic use of &lt;code&gt;matchit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(888)
matchit_out &amp;lt;- MatchIt::matchit(
  treat ~ age + educ + married + race +
                  nodegree + re74 + re75,
  data = lalonde,
  method = &amp;quot;genetic&amp;quot;,
  estimand = &amp;quot;ATT&amp;quot;,
  ratio = 2,
  replace = TRUE,
  pop.size = 200
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, &lt;code&gt;matchit()&lt;/code&gt; estimates a propensity score using logistic regression and includes it in the matching covariates (but not the covariates on which balance is optimized), just as we did manually using &lt;code&gt;GenMatch()&lt;/code&gt; above. If you want to use difference variables to balance on from those used to match, use the &lt;code&gt;mahvars&lt;/code&gt; argument, which is explained in the documentation for genetic matching (accessible using &lt;code&gt;help(&#34;method_genetic&#34;, package = &#34;MatchIt&#34;)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can assess balance using &lt;code&gt;summary()&lt;/code&gt; or using &lt;code&gt;bal.tab()&lt;/code&gt;. We’ll do the latter below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cobalt::bal.tab(matchit_out, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call
##  MatchIt::matchit(formula = treat ~ age + educ + married + race + 
##     nodegree + re74 + re75, data = lalonde, method = &amp;quot;genetic&amp;quot;, 
##     estimand = &amp;quot;ATT&amp;quot;, replace = TRUE, ratio = 2, pop.size = 200)
## 
## Balance Measures
##                 Type Diff.Adj KS.Adj
## distance    Distance   0.0337 0.1000
## age          Contin.  -0.0238 0.1514
## educ         Contin.   0.0712 0.0324
## married       Binary  -0.0027 0.0027
## race_black    Binary   0.0081 0.0081
## race_hispan   Binary   0.0000 0.0000
## race_white    Binary  -0.0081 0.0081
## nodegree      Binary   0.0054 0.0054
## re74         Contin.   0.0356 0.1514
## re75         Contin.   0.0689 0.0730
## 
## Sample sizes
##                      Control Treated
## All                    429.      185
## Matched (ESS)           45.6     185
## Matched (Unweighted)   123.      185
## Unmatched              306.        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results will differ due to slight differences in how the two functions process their inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-genetic-matching-yourself&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Programming Genetic Matching Yourself&lt;/h2&gt;
&lt;p&gt;Perhaps surprisingly, it’s fairly easy to program genetic matching yourself. You only need the following ingredients:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A function that creates a distance matrix from a set of weights &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;A function that performs matching on a given distance matrix&lt;/li&gt;
&lt;li&gt;A function that evaluates balance on a given matched sample&lt;/li&gt;
&lt;li&gt;A function that performs the genetic optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are (fairly) easy to come by, and I’ll show you how to write each of them.&lt;/p&gt;
&lt;p&gt;For the first function, we can use &lt;code&gt;MatchIt::mahalanobis_dist()&lt;/code&gt; if we want &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; to be the full covariance matrix of the covariates, but it’s actually quite a bit simpler to use &lt;code&gt;MatchIt::scaled_euclidean_dist()&lt;/code&gt; to just use the variances of the covariates, which is what &lt;code&gt;GenMatch()&lt;/code&gt; (and therefore &lt;code&gt;matchit()&lt;/code&gt;) does anyway. This is because we can supply to &lt;code&gt;scaled_euclidean_dist()&lt;/code&gt; a vector of variances, which we will simply divide by the weights. So, our function for creating the distance matrix given the set of weights will be the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist_from_W &amp;lt;- function(W, dist_covs) {
  variances &amp;lt;- apply(dist_covs, 2, var)
  MatchIt::scaled_euclidean_dist(data = dist_covs, var = variances / W)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, there are many ways we could make this more efficient. I just want to demonstrate how easy it is to program genetic matching. Programming it &lt;em&gt;well&lt;/em&gt; is another story.&lt;/p&gt;
&lt;p&gt;Next, we need a function that performs matching on covariates given a distance matrix. We could use &lt;code&gt;optmatch::fullmatch()&lt;/code&gt; for full matching, but &lt;code&gt;matchit()&lt;/code&gt; provides a nice, general interface for many matching methods. We can supply the distance matrix to the &lt;code&gt;distance&lt;/code&gt; argument of &lt;code&gt;matchit()&lt;/code&gt;. A function that takes in a distance matrix and returns a &lt;code&gt;matchit&lt;/code&gt; object containing the matched sample and matching weights is the following&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;do_matching_with_dist &amp;lt;- function(dist) {
  MatchIt::matchit(treat ~ 1, data = lalonde, distance = dist,
                   method = &amp;quot;nearest&amp;quot;, ratio = 2, replace = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need a function that takes in a &lt;code&gt;matchit&lt;/code&gt; object and computes a scalar balance statistic. You can use your favorite balance statistic, but here I’ll use the maximum absolute standardized mean difference (ASMD) of all the covariates in the matched sample&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. This measure can be easily computed using &lt;code&gt;cobalt::col_w_smd()&lt;/code&gt;, which takes in a matrix of covariates, a treatment vector, and a weights vector and returns the weighted ASMDs for each covariate. We will allow the set of covariates to be different from those used to compute the distance measure. We implement this below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_balance &amp;lt;- function(m, bal_covs, treat) {
  weights &amp;lt;- cobalt::get.w(m)
  max(cobalt::col_w_smd(bal_covs, treat, weights,
                        s.d.denom = &amp;quot;treated&amp;quot;,
                        abs = TRUE))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Okay! We have the key ingredients for our objective function, which takes in a set of covariates weights &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; and returns a balance statistic that we want to optimize. Let’s put everything together into a single function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;objective &amp;lt;- function(W_, dist_covs, bal_covs, treat) {
  W &amp;lt;- exp(c(0, W_))
  
  dist_from_W(W, dist_covs) |&amp;gt;
    do_matching_with_dist() |&amp;gt;
    compute_balance(bal_covs, treat)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line of the function needs explaining. Instead of optimizing over the weights directly, we’re going to optimize over the log of the weights. This ensures the weights can prioritize and de-prioritize variables in a symmetric way&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. To get back to the weights &lt;code&gt;W&lt;/code&gt; used in the distance measure, we need to exponentiate the optimized log-weights &lt;code&gt;W_&lt;/code&gt;. Also, instead of optimizing over all the weights, we are going to fix one weight to 1 (i.e., fix one log-weight to 0). This is because the matches are invariant to multiplying all the weights by a constant&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;. So, we can identify the weights by choosing an arbitrary weight to set to 1&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can give this function a try to see balance when when the log-weights are all set to 0 (i.e., so all weights are equal to 1), which corresponds to matching using the standard scaled Euclidean distance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_test &amp;lt;- rep(0, ncol(covs_ps) - 1)
objective(W_test, dist_covs = covs_ps, bal_covs = covs,
          treat = treat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1280539&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can supply this to a function that performs the genetic algorithm to optimize our objective function. &lt;code&gt;GenMatch()&lt;/code&gt; uses &lt;code&gt;rgenoud::genoud()&lt;/code&gt;, but there is a more modern interface in the R package &lt;code&gt;{GA}&lt;/code&gt;, which we’ll use instead just to demonstrate that the method is software-independent. We’ll use &lt;code&gt;GA::ga()&lt;/code&gt;, which implements the standard genetic algorithm, though other functions are available for more sophisticated methods.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ga()&lt;/code&gt; can only maximize functions, but we want to minimize our imbalance, so we just have to create a new objective function that is the negative of our original.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Need negative objective to minimize imbalance
neg_objective &amp;lt;- function(...) -objective(...)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at the &lt;code&gt;GA::ga()&lt;/code&gt; call below. We specify &lt;code&gt;type = &#34;real-valued&#34;&lt;/code&gt; because our weights are real numbers, we supply the negative of our objective function to &lt;code&gt;fitness&lt;/code&gt;, and we supply the additional argument to our functions (&lt;code&gt;dist_covs&lt;/code&gt;, the covariates used in the distance matrix and the weights of which we are optimizing over; &lt;code&gt;bal_covs&lt;/code&gt;, the covariates used to compute the balance statistic that is our criterion; and &lt;code&gt;treat&lt;/code&gt;, the treatment vector). We need to provide lower and upper bounds for the weights, and here I’ve supplied -7 and 7, which correspond to weights of &lt;span class=&#34;math inline&#34;&gt;\(\exp(-7)=.0009\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\exp(7)=1096.6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The next arguments control the speed and performance of the optimization process. I’ve already described &lt;code&gt;popSize&lt;/code&gt;, the population size (called &lt;code&gt;pop.size&lt;/code&gt; in &lt;code&gt;GenMatch()&lt;/code&gt;). We are going to let the algorithm run for 500 generations (&lt;code&gt;maxiter&lt;/code&gt;, called &lt;code&gt;max.generations&lt;/code&gt; in &lt;code&gt;GenMatch()&lt;/code&gt;/&lt;code&gt;genoud()&lt;/code&gt;) but stop if there is no improvement in balance after 100 iterations (&lt;code&gt;run&lt;/code&gt;, called &lt;code&gt;wait.generations&lt;/code&gt; in &lt;code&gt;GenMatch()&lt;/code&gt;/&lt;code&gt;genoud()&lt;/code&gt;). I’m going to request parallel processing using 4 cores to speed it up, and suppress printing of output&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opt_out &amp;lt;- GA::ga(
  type = &amp;quot;real-valued&amp;quot;,
  fitness = neg_objective,
  dist_covs = covs_ps,
  bal_covs = covs,
  treat = treat,
  lower = rep(-7, ncol(covs_ps) - 1),
  upper = rep(7, ncol(covs_ps) - 1),
  popSize = 200, 
  maxiter = 500,
  run = 100,
  parallel = 4,
  seed = 567, #set seed here if using parallelization
  monitor = NULL
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes my computer about 3 minutes to run. We can run some summaries on the output object to examine the results of the optimization:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(opt_out)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Genetic Algorithm ─────────────────── 
## 
## GA settings: 
## Type                  =  real-valued 
## Population size       =  200 
## Number of generations =  500 
## Elitism               =  10 
## Crossover probability =  0.8 
## Mutation probability  =  0.1 
## Search domain = 
##       x1 x2 x3 x4 x5 x6 x7 x8 x9
## lower -7 -7 -7 -7 -7 -7 -7 -7 -7
## upper  7  7  7  7  7  7  7  7  7
## 
## GA results: 
## Iterations             = 251 
## Fitness function value = -0.02735878 
## Solutions = 
##              x1        x2         x3       x4        x5          x6        x7          x8       x9
##  [1,] -2.131722 -3.211259 0.07716908 2.014075 0.8820362 -0.56233539 0.8603869 -0.11499679 3.247619
##  [2,] -2.143106 -3.214016 0.04484056 1.883541 0.7790500 -0.23702373 0.8119892 -0.13855416 3.239199
##  [3,] -2.134344 -3.215796 0.06373645 2.096751 0.9507463 -0.51305525 0.8511466 -0.13408798 3.240825
##  [4,] -2.126203 -3.218519 0.06842050 2.054350 0.8760389  0.14654690 0.8123135 -0.13585342 3.240511
##  [5,] -2.139156 -3.215859 0.05880775 2.075352 0.8094103 -0.55428742 0.8385978 -0.14236686 3.231219
##  [6,] -2.126643 -3.215275 0.07452945 2.085683 0.9246621  0.03924881 0.8309840 -0.13213708 3.233923
##  [7,] -2.144081 -3.217624 0.07350998 2.083049 0.6266973 -0.19835280 0.8437496 -0.13188036 3.230876
##  [8,] -2.140938 -3.218686 0.08261494 2.046125 0.8239281 -0.74804769 0.8537672 -0.12327868 3.263523
##  [9,] -2.126126 -3.208634 0.06744410 1.963189 0.9050594 -0.37299052 0.8754860 -0.06291793 3.241593
## [10,] -2.127889 -3.198998 0.05221517 1.678228 0.8716645  0.10282196 0.8418134 -0.13623726 3.241613
## [11,] -2.144941 -3.215945 0.05434804 2.057280 0.8117419 -0.96995541 0.8475345 -0.13000397 3.307741
## [12,] -2.140205 -3.205996 0.06436092 2.048975 0.7902906 -1.87554753 0.8307355 -0.13913023 3.239275&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that our final value for the criterion was about -0.0274 and this was achieved by each of the sets of log weights displayed. We can just focus on the first row. It’s not worth over-interpreting these values since their purpose is just to achieve balance and they don’t reveal anything about the causal or statistical relevance of the covariates. But we can see that &lt;code&gt;x9&lt;/code&gt; (i.e., &lt;code&gt;re75&lt;/code&gt;) was the most important covariate in the distance measure, and &lt;code&gt;x2&lt;/code&gt; (i.e., &lt;code&gt;educ&lt;/code&gt;) was the least important.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(opt_out)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ngreifer.github.io/blog/genetic-matching/index.en_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
We can also see from the plot that close to the best balance was reached pretty quickly in fewer than 50 generations, and refinements after that were very minor. This suggests that if you’re in a rush or just want to test out genetic matching without committing to it, you can wait just a few generations (fewer than 100, which is the default in &lt;code&gt;GenMatch()&lt;/code&gt;) to get a good sense of its performance.&lt;/p&gt;
&lt;p&gt;Finally, let’s perform a final round of matching using the found matching weights and assess balance on each covariate in our matched sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Extract weights by transforming log weights from output
W &amp;lt;- exp(c(0, opt_out@solution[1,]))

#Compute distance measure from weights and do matching
m.out &amp;lt;- dist_from_W(W, covs_ps) |&amp;gt;
  do_matching_with_dist()

m.out&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## A matchit object
##  - method: 2:1 nearest neighbor matching with replacement
##  - distance: User-defined (matrix)
##  - number of obs.: 614 (original), 306 (matched)
##  - target estimand: ATT&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Assess balance. See ?bal.tab for info on the arguments
cobalt::bal.tab(treat ~ age + educ + married + race +
                  nodegree + re74 + re75,
                data = lalonde, stats = c(&amp;quot;m&amp;quot;, &amp;quot;ks&amp;quot;), 
                binary = &amp;quot;std&amp;quot;, un = TRUE,
                weights = cobalt::get.w(m.out),
                method = &amp;quot;matching&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Balance Measures
##                Type Diff.Un  KS.Un Diff.Adj KS.Adj
## age         Contin. -0.3094 0.1577   0.0181 0.2541
## educ        Contin.  0.0550 0.1114   0.0255 0.0243
## married      Binary -0.8263 0.3236  -0.0207 0.0081
## race_black   Binary  1.7615 0.6404   0.0223 0.0081
## race_hispan  Binary -0.3498 0.0827   0.0000 0.0000
## race_white   Binary -1.8819 0.5577  -0.0274 0.0081
## nodegree     Binary  0.2450 0.1114   0.0178 0.0081
## re74        Contin. -0.7211 0.4470  -0.0273 0.1405
## re75        Contin. -0.2903 0.2876   0.0271 0.0405
## 
## Sample sizes
##                      Control Treated
## All                   429.       185
## Matched (ESS)          49.93     185
## Matched (Unweighted)  121.       185
## Unmatched             308.         0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that after matching, the largest standardized mean difference is indeed 0.0274, well below the usual criterion of .1. That doesn’t mean the sample is fully balanced, though; some KS statistics are a bit high, suggesting that an imbalance measure that accounts for the full distribution of the covariates beyond the means might be more effective. Finally, once satisfactory balance has been found, you can estimate the treatment effect using the methods described in &lt;code&gt;vignette(&#34;estimating-effects&#34;, package = &#34;MatchIt&#34;)&lt;/code&gt;. I’ve gone on long enough so I won’t do that here.&lt;/p&gt;
&lt;p&gt;Congratulations! You’ve just done genetic matching, three ways!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-diamondGeneticMatchingEstimating2013&#34; class=&#34;csl-entry&#34;&gt;
Diamond, Alexis, and Jasjeet S. Sekhon. 2013. &lt;span&gt;“Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.”&lt;/span&gt; &lt;em&gt;Review of Economics and Statistics&lt;/em&gt; 95 (3): 932945. &lt;a href=&#34;https://doi.org/10.1162/REST_a_00318&#34;&gt;https://doi.org/10.1162/REST_a_00318&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-greiferMatchingMethodsConfounder2021a&#34; class=&#34;csl-entry&#34;&gt;
Greifer, Noah, and Elizabeth A Stuart. 2021. &lt;span&gt;“Matching Methods for Confounder Adjustment: An Addition to the Epidemiologist&lt;span&gt;’&lt;/span&gt;s Toolbox.”&lt;/span&gt; &lt;em&gt;Epidemiologic Reviews&lt;/em&gt;, June, mxab003. &lt;a href=&#34;https://doi.org/10.1093/epirev/mxab003&#34;&gt;https://doi.org/10.1093/epirev/mxab003&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hulingEnergyBalancingCovariate2022&#34; class=&#34;csl-entry&#34;&gt;
Huling, Jared D., and Simon Mak. 2022. &lt;span&gt;“Energy &lt;span&gt;Balancing&lt;/span&gt; of &lt;span&gt;Covariate Distributions&lt;/span&gt;.”&lt;/span&gt; &lt;span&gt;arXiv&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.48550/arXiv.2004.13962&#34;&gt;https://doi.org/10.48550/arXiv.2004.13962&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kingWhyPropensityScores2019&#34; class=&#34;csl-entry&#34;&gt;
King, Gary, and Richard Nielsen. 2019. &lt;span&gt;“Why Propensity Scores Should Not Be Used for Matching.”&lt;/span&gt; &lt;em&gt;Political Analysis&lt;/em&gt;, May, 1–20. &lt;a href=&#34;https://doi.org/10.1017/pan.2019.11&#34;&gt;https://doi.org/10.1017/pan.2019.11&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-oyenubiDoesChoiceBalancemeasure2020&#34; class=&#34;csl-entry&#34;&gt;
Oyenubi, Adeola, and Martin Wittenberg. 2020. &lt;span&gt;“Does the Choice of Balance-Measure Matter Under Genetic Matching?”&lt;/span&gt; &lt;em&gt;Empirical Economics&lt;/em&gt;, May. &lt;a href=&#34;https://doi.org/10.1007/s00181-020-01873-9&#34;&gt;https://doi.org/10.1007/s00181-020-01873-9&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rizzoEnergyDistance2016&#34; class=&#34;csl-entry&#34;&gt;
Rizzo, Maria L., and Gábor J. Székely. 2016. &lt;span&gt;“Energy Distance.”&lt;/span&gt; &lt;em&gt;WIREs Computational Statistics&lt;/em&gt; 8 (1): 27–38. &lt;a href=&#34;https://doi.org/10.1002/wics.1375&#34;&gt;https://doi.org/10.1002/wics.1375&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-rubinBiasReductionUsing1980&#34; class=&#34;csl-entry&#34;&gt;
Rubin, Donald B. 1980. &lt;span&gt;“Bias Reduction Using Mahalanobis-Metric Matching.”&lt;/span&gt; &lt;em&gt;Biometrics&lt;/em&gt; 36 (2): 293–98. &lt;a href=&#34;https://doi.org/10.2307/2529981&#34;&gt;https://doi.org/10.2307/2529981&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-sekhonMultivariatePropensityScore2011&#34; class=&#34;csl-entry&#34;&gt;
Sekhon, Jasjeet S. 2011. &lt;span&gt;“Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R.”&lt;/span&gt; &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 42 (1): 1–52. &lt;a href=&#34;https://doi.org/10.18637/jss.v042.i07&#34;&gt;https://doi.org/10.18637/jss.v042.i07&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuartMatchingMethodsCausal2010&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A. 2010. &lt;span&gt;“Matching Methods for Causal Inference: A Review and a Look Forward.”&lt;/span&gt; &lt;em&gt;Statistical Science&lt;/em&gt; 25 (1): 1–21. &lt;a href=&#34;https://doi.org/10.1214/09-STS313&#34;&gt;https://doi.org/10.1214/09-STS313&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-stuartPrognosticScorebasedBalance2013&#34; class=&#34;csl-entry&#34;&gt;
Stuart, Elizabeth A., Brian K. Lee, and Finbarr P. Leacy. 2013. &lt;span&gt;“Prognostic Score-Based Balance Measures Can Be a Useful Diagnostic for Propensity Score Methods in Comparative Effectiveness Research.”&lt;/span&gt; &lt;em&gt;Journal of Clinical Epidemiology&lt;/em&gt; 66 (8): S84. &lt;a href=&#34;https://doi.org/10.1016/j.jclinepi.2013.01.013&#34;&gt;https://doi.org/10.1016/j.jclinepi.2013.01.013&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-zhuKernelBasedMetricBalance2018&#34; class=&#34;csl-entry&#34;&gt;
Zhu, Yeying, Jennifer S. Savage, and Debashis Ghosh. 2018. &lt;span&gt;“A Kernel-Based Metric for Balance Assessment.”&lt;/span&gt; &lt;em&gt;Journal of Causal Inference&lt;/em&gt; 6 (2). &lt;a href=&#34;https://doi.org/10.1515/jci-2016-0029&#34;&gt;https://doi.org/10.1515/jci-2016-0029&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;There are several possible ways to compute &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;; for example, &lt;span class=&#34;citation&#34;&gt;Rubin (&lt;a href=&#34;#ref-rubinBiasReductionUsing1980&#34; role=&#34;doc-biblioref&#34;&gt;1980&lt;/a&gt;)&lt;/span&gt; uses the “pooled” covariance matrix, which is a weighted average of the within-group covariances.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Basically, they work by proposing a population of guesses of the parameters to be estimated (e.g., 50 sets of candidate &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;s), removing the candidates with the worst imbalance, and reproducing and perturbing the remaining candidates slightly (like a genetic mutation), then doing this over and over again so that only the best candidates remain. This is a type of “evolutionary algorithm” because it works a bit like natural selection, where the fittest creatures remain to reproduce but with slight variation, and the least fit die off, improving the overall fitness of the species.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;code&gt;{Matching}&lt;/code&gt; uses matching imputation to estimate the treatment effect, which is different from running an outcome regression in the matched sample. See my answer &lt;a href=&#34;https://stats.stackexchange.com/a/566981/116195&#34;&gt;here&lt;/a&gt; for some additional details on this distinction and its implications.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;It’s maybe worth knowing that &lt;code&gt;GenMatch()&lt;/code&gt; actually uses &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; with all the off-diagonal elements set to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;. This is not described in its documentation or in the papers describing the method. In practice, this likely makes little difference to the overall matching performance. A benefit of this approach is that you get a nice interpretation of the resulting &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; as importance of each variable in the match, though this interpretation serves little use in practice.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Using a different matching method for the final match than you did in estimating &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; is possible, but not advised.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Be careful! There’s a &lt;code&gt;lalonde&lt;/code&gt; dataset in &lt;code&gt;{Matching}&lt;/code&gt;, too, which is different.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Here is seems like we aren’t matching on any covariates by supplying &lt;code&gt;treat ~ 1&lt;/code&gt; as the model formula; we are supplying the distance matrix ourselves, so the covariates play no role in the matching beyond that. To speed up the evaluation and prevent &lt;code&gt;matchit()&lt;/code&gt; from having to process a whole data frame of covariates, we omit the covariates.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;This same balance statistic can be used in &lt;code&gt;WeightIt&lt;/code&gt; and &lt;code&gt;twang&lt;/code&gt; for generalized boosted modeling and other methods that involve optimizing a user-supplied criterion.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;That is, so a weight of 2 is as easy to find as a weight of 1/2, as these have the same “magnitude”; they correspond to log-weights of .69 and -.69, respectively.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;That is, the exact same matches found for a given set of weights would be found if all those weights were multiplied by, e.g., 100.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;It doesn’t matter which one you choose, but I like to make the propensity score have the scaling weight to assess how much more or less important the covariates are than the propensity score for achieving balance.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;If you’re following along at home, try setting &lt;code&gt;monitor = plot&lt;/code&gt; to see a neat plot of the progress of the optimization! We’ll also view this plot after the optimization has finished.&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
