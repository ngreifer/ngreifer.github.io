---
title: 'Why "Why Propensity Scores Should Not Be Used for Matching" Should Not Be Used to Dismiss Propensity Score Matching'
author: Noah Greifer
date: '2023-07-08'
slug: in-defense-of-propensity-scores
categories: []
tags:
  - propensity-scores
  - matching
subtitle: ''
summary: ''
authors: []
lastmod: '2023-07-08T14:06:53-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: true
bibliography: references.bib
---

Propensity scores get a lot of hate. They get hate on Twitter, on CrossValidated, and in the literature. They are hated by trialists, epidemiologists, and economists. Among the many papers hating on propensity scores, perhaps the most famous is @kingWhyPropensityScores2019, "Why Propensity Scores Should Not Be Used for Matching", a paper with a title so clear, it seems hardly necessary to read the actual paper, right?

But there are plenty of people who love propensity scores. Medical researchers, especially those with little statistical sophistication, love them. I have a Google Scholar alert for "propensity score" and everyday I am bombarded by medical studies that used propensity score matching to estimate the effect of robot assisted surgery vs being thrown down a well among the subpopulation of men aged 45-47 with pancreatic cancer and HIV. Some of the most influential statisticians of the day have written positively about propensity scores, including legendary statistician Don Rubin (who, with observational studies guru Paul Rosenbaum, invented/discovered them in @rosenbaumCentralRolePropensity1983), famed clear writer, acclaimed biostatistician, and wonderful person Liz Stuart, political methodologist extraordinaire Kosuke Imai, and genre-defining clinician-turned-epidemiologist-savant Jamie Robins.

I, your humble narrator, have been on both sides of the debate, defending propensity scores online, critiquing the critiques, and insulting the fools who think methodological development stopped in 1983. I am the author and maintainer of several pieces of software that facilitate using propensity scores, but those same pieces of software also offer and emphasize alternatives to propensity scores. I was invited to a [statistics podcast](https://quantitudepod.org/s3e27-propensity-scores/) to talk about propensity scores and refused to define them until 22 minutes into the episode.

In this post, I'm going to clarify some of the arguments against propensity scores and respond to them. A theme I want to highlight is that almost none of these arguments are against propensity scores themselves. They are about study designs, data analysis strategies and philosophies, and classes of statistical estimators. To start off, I want to clarify the role propensity scores take in observational studies (and it's not as central as @rosenbaumCentralRolePropensity1983 "The Central Role of Propensity Scores in Observational Studies" suggests).

First, I want to highlight where propensity scores fall into the study design/analysis methods hierarchy. Some people seem to think the heirarchy looks like this:

```{r}
#Flowchart 1
```

That is, you're either doing a beautiful, perfect, scientific randomized trial, or you're using propensity scores. Which means any critique you can throw against any design/method that isn't a randomized trial is an argument against propensity scores. In fact, and as I explain in this CV answer and my Quantitude episode, the hierarchy looks more like this:

```{r}
#FLowchart 2
```

The hierarchy is explained well in @matthayAlternativeCausalInference2020. At the top we have the distinction between randomized trials, in which participants are randomly assigned to treatment condition, and observational studies, in which assignment to treatment condition is done by the clinician, participant, or an unknown but nonignorable force. Among methods to analyze observational studies, there are several that can be used depending on the assumptions one can meet. If an instrumental variable is available that meets the required exclusion criteria, instrumental variable analysis can be used. If a pre-treatment measure of the outcome is available and the parallel trends assumptions is met, difference-in-differences (DiD) can be used. If you have collected a sufficient set of variables to eliminate confounding, then covariate adjustment methods can be used.

Among covariate adjustment methods, there are those that are considered "analysis-based" (i.e., model the outcome), "design-based" (i.e., manipulate the sample without involving the outcome), or doubly-robust (which do both, or at least involve modeling both the treatment and outcome). Among design-based methods are matching (including stratification, subset selection, and pair matching) and weighting. And among matching and weighting methods, there are variations that use propensity scores and those that don't.

Critiques of propensity scores exist at all levels of the hierarchy. But these critiques are distinct and should not be muddled. It's true that any critique at any level of this hierarchy is a critique of propensity score methods, though perhaps only indirectly. For example, one may say they dislike propensity score methods because they require you to meet the assumption of strong ignorability (a.k.a., conditional exchangeability, satisfaction of the backdoor criterion, selection on observables). But all covariate adjustment methods share this same critique. It's not that the critique is invalid, but citing @kingWhyPropensityScores2019 in support of this argument makes no sense given that @kingWhyPropensityScores2019 is an argument about the very lowest level of the hierarchy, that is, between matching methods that use propensity scores and matching methods that don't.

In what follows, I will evaluate 5 arguments about why propensity scores should not be used. These arguments are levied against propensity scores at each of the levels of the hierarchy, as if propensity scores represent the entire body of causal inference methodology for observational studies. In many cases, the arguments are not wrong, but they are often misdirected, with naive combatants citing irrelevant facts and papers to attempt to support their point.

### 1. Propensity scores can't be used to establish causality; only a randomized trial can

This argument is a direct reaction to the frequent use of propensity scores to estimate causal effects in observational studies. The argument goes as follows: given that observational studies lack randomization, their ability to estimate causal effects requires untenable assumptions (no matter what those assumptions are), and the only design that can validly be used to assess causality involves double-blind randomization. Observational studies are neither double-blind (participants know what treatment they receive) nor randomized (participants receive treatment based on an impermeable cloud of complex mechanisms). This is not an argument against propensity scores; it is an argument against all observational studies and the methods used to analyze them.

I agree that observational studies require very strong assumptions, many of which cannot even be hoped to be met in practice except in a few highly specific cases. But this argument has nothing to do with propensity scores. Propensity scores are so far from the concepts actually being discussed in this argument. They are a specific implementation of a specific method of a specific class of methods that require a specific assumption, not an embodiment of the analysis of all observational studies. If you want to make this claim, you can't use propensity scores as a punching bag, and you can't include critiques aimed specifically at propensity scores to bolster your point. You have to argue against all applications of instrumental variables, difference-in-differences, and covariate adjustment methods, including those that involve and don't involve propensity scores. (It's also known that propensity scores can be used to validly estimate effects in randomized trials [@williamsonVarianceReductionRandomised2014].)

Another problem I and many others have with this argument is it means that so little science can actually get done. Randomized trials are are a tiny subset of the data that is available to us. Discarding all of it simply because it doesn't satisfy the strict requirements of randomized treatment assignment would be a tremendous waste, not just because so much money is spent collecting observational data (e.g., in longitudinal surveys or passively in healthcare databases), but also because there actually is useful information to be gleaned from observational studies, even if that information is not assumption free.

One critique goes that observational studies should not be run in medical research because it is unethical to use information laden with such strict assumptions to treat patients; I don't like this argument because it is the fault of the clinician for using such information, not the researcher for producing it. That is, it might actually be unethical for a clinician to use observational data as evidence to treat patients. But what isn't unethical is a researcher collecting observational data, estimating a conditional association using propensity scores or another method, and interpreting their estimate as a possibly biased estimate of a causal effect.

Most of all, this argument has nothing to do with the claims if @kingWhyPropensityScores2019, who accept the premise that observational studies can provide useful results, strong ignorability is a plausible assumption, design-based analyses are useful, and matching is a valid effect estimator.

### 2. Propensity scores shouldn't be used because strong ignorability never holds in practice, but IV or DiD assumptions do

This argument is implicit in much economic research. Even though some of the biggest names in econometrics have written fruitfully about propensity score analysis, it is still rarely used in the field, and instrumental variable methods and difference-in-difference methods are given far greater attention. Most work on propensity score analysis in economics has been done by Abadie and Imbens (yes, Nobel-prize-sharing Imbens), with some recent work done by Martin Huber (no, not robust standard error Huber). Otherwise, propensity score analysis receives little attention, except when disguised as synthetic controls (which is a propensity score weighting method) or when used alongside modern DiD methods (like the weighting methods used in Calloway and Santanna's TWFE-alternative estimators).

As someone coming from psychology and biostatistics, it is absurd to me to think that the assumptions underlying instrumental variable methods or DiD could ever be more tenable than strong ignorability. Instrumental variables require the exclusion restriction, no confounding of the instrument, relevance of the instrument to the treatment, and monotonicity. They extrapolate from the data, are imprecise, and yield a mostly uninterpretable estimand. The burden of proof for these assumptions is as strong as it is for strong ignorability.

Similarly, DiD requires two assumptions that to me seem highly unrealistic: that pre-treatment outcomes don't cause selection into treatment and that the outcome trends in the absence of treatment are the same between the treatment groups. If the outcome is anything more than just a epiphenomenon and is actually somewhat involved in who receives treatment (which of course it would be, because treatments meant to affect the outcome would be given to units based on need as determined by prior outcomes), this assumptions is violated. And (conditional) parallel trends is one step away from strong ignorbaility; if two groups are different, why would their trends be the same? Conditional parallel trends requires adjusting for all common causes of treatment and trends, which sounds a lot like adjusting for all causes of treatment and the outcome. How these could be considered weaker assumptions than strong ignorability is beyond me.

### 3. Propensity scores have no advantages over outcome regression, and therefore shouldn't be used

This is a fairly common argument in the circles I live in, often instigated by a naive student wondering when to use propensity scores vs. regression. To be fair, there has been very little stated explicitly in the literature about this point. One of my most popular answers on CrossValidated [addresses](https://stats.stackexchange.com/a/544958/116195) this question specifically. I'll summarize some of the arguments here, but I'll also address some arguments I have seen on Twitter and elsewhere. Importantly, though, this is not really a debate about propensity scores vs regression; it is a debate bout analysis-based methods vs. designed-based methods. And of course the resolution is clear: combine them.

This argument actually is mostly true. The best performing methods are those that flexibly model the outcome, as these methods not only eliminate bias due to confounding on the observed covariates but also reduce the unexplained variability in the outcome, which is what provides precision in the effect estimate. Methods like BART dramatically outperform designed-based methods in simulations [@dorieAutomatedDoityourselfMethods2019; @huEstimationCausalEffects2020a], and AIPW and TMLE are among the only methods that have solid guarantees for root-n convergence of the estimator and asymptotically valid inference.

Here are a few reasons why I don't still don't love this argument:

-   The simulations that use state-of-the-art regression methods use the most basic and ignorantly applied design-based methods. It's definitely an advantage for flexible analysis-based methods to be used out of the box whereas design-based methods need careful fine-tuning by the analyst, but that doesn't make the usual comparisons fair. @keeleComparingCovariatePrioritization2018 found that carefully use matching methods yielded similar results to using BART, and they weren't even using matching methods that well.

-   The whole point of designed-based methods is to capitalize on the separation between the design and analysis of the study. This isn't just for some wishy-washy honesty/objectivity reason that has been purported in the early literature [@rubinDesignAnalysisObservational2007]; rather, it is about the epistemic advantage of being able to prove to your reader that you eliminated confounding due to the measured variables. That is, design-based methods allow you to assess and demonstrate covariate balance. A balance table that completely and holistically demonstrates balance is far more justification for the confidence in an unbiased effect estimate than the asymptotic guarantees of TMLE or the good simulation performance of BART. The cross-validation accuracy of a machine learning method is not related to its performance at eliminating bias, and there are no other diagnostics to compare the performance of a complicated method like TMLE with Superlearner composed of HAL and 20 candidate learners vs a simple method like g-computation with a random forest. For many, including me, this epistemic advantage that design-based methods have is worth the decrease in precision we incur from using them. But that benefit is only realized when balance is assessed fully and holistically; the usual balance statistics people report (i.e., standardized mean differences) are not sufficient to realize these benefits.

-   Design-based methods are and always have been designed to be used with an outcome regression model. It has never been an either-or situation except in simulation studies or academic debates. The early literature on matching and many of the most influential papers on the topic recommend combining matching with regression adjustment [@rubinCombiningPropensityScore2000; @rubinUseMatchedSampling1973; @abadieBiasCorrectedMatchingEstimators2011]. Similarly, weighted g-computation and AIPW have long been recommended for use after weighting [@vansteelandtInvitedCommentaryGComputation2011; @coleConstructingInverseProbability2008]. The mainstream philosophy of matching, matching as nonparametric pre-processing as espoused by @hoMatchingNonparametricPreprocessing2007, is all about using matching to increase the robustness of and decrease extrapolation due to outcome regression. The guides on best practices for estimating effects after matching weighting using `MatchIt` and `WeightIt` only show how to estimate effects with covariates adjusted for in the outcome model. The software for matching imputation, the less mainstream philosophy of matching described by @abadieLargeSampleProperties2006, makes it extremely easy to incorporate the outcome model into the effect estimation as recommended by @abadieBiasCorrectedMatchingEstimators2011. All advantages outcome regression has over pure-designed based methods are shared by these combined methods, with the additional epistemic advantage of getting to prove to your audience you have achieved balance (and such that the price in precision for this advantage is decreased due to the increased precision afforded by the outcome model).

-   Outcome regression models are numerically equivalent to certain kinds of balancing weights that imply a given propensity score model. That is, the distinction between purely analysis-based and purely designed-based methods is blurry anyway. @zubizarretaStableWeightsThat2015 described a method of estimating weights (without involving the outcome) that minimizes the variance of the weights subject to balance constraints on the covariate means and that the weights must be positive. @chattopadhyayImpliedWeightsLinear2022a showed that g-computation using a linear regression model implies the exact same weights as that method except without the constraint that the weights are positive. That is, linear regression is identical to balancing weights except that linear regression can extrapolate. Similar equivalences have been demonstrated by @bruns-smithAugmentedBalancingWeights2023 and @ben-michaelAugmentedSyntheticControl2021.

These are the main arguments I see for why you should avoid propensity score methods (and implicitly, all design-based methods). Frank Harrell also has specific and more nuanced arguments that I want to address. His arguments are more specifically addressed to the context he works in, which is in the estimation of the treatment effects on binary outcomes in medical research. Frank's recommended alternative is to use the coefficient on treatment in a logistic regression of the outcome on the treatment and covariates, which should be modeled flexibly with splines. At worst, when there are too many covariates to adjust for and there is some use for the propensity score as a dimension reduction method, the propensity score should be included in the outcome model with a flexible spline and not used for matching or weighting.

As far as I can tell, Frank's arguments boil down to these claims:

1.  Propensity score matching needlessly destroys precision by discarding useful data and not reducing unexplained variability in the outcome.

2.  The estimand targeted by propensity score matching, the ATE or ATT, is far less clinically useful than the conditional ATE (CATE) targeted by the logistic regression method, both in terms of scientific relevance and generalizability beyond the specific dataset that happened to be used to estimate the effect.

3.  Logistic regression is the most plausible model for binary outcomes, and the odds ratio is the most plausible scale for which there could be no modification of the treatment effect by the covariates, which is why the coefficient on treatment in a logistic regression is a satisfactory estimate of the CATEs, which are assumed to be equal on the odds ratio scale.

I don't disagree with all those points, but I disagree that they imply design-based methods shouldn't be used or that Frank's logistic regression-based alternative is a viable solution. I already explained why I don't think the precision trade-off is so pernicious: it is a trade I and many researchers are willing to make in order to obtain the epistemic advantages of being confident bias has been eliminated, and it can be mitigated by using an outcome model on the adjusted sample as recommended heavily in the literature and tutorials using methods easily accessible to researchers using mainstream software. But there is no doubt that one is making this trade-off, and if you are a person for whom that trade-off isn't worth it (i.e., you are more convinced by the asymptotic guarantees of AIPW and TMLE or the empirical performance of BART), then you should avoid design-based methods. Frank is evidently one of those people, and I respect that opinion.

Frank's second point is less about propensity scores or even design-based methods and more about the estimand being used and the ability of different methods to target that estimand. There is no doubt the CATE is more clinically useful than an ATE. A CATE for a given patient profile allows a clinician to tailor treatment to such a patient and doesn't depend on the population from which the CATE was estimated; that is, in an ideal scenario, the same CATE would be estimated for a given profile regardless of the origin of the dataset as long as the same variables were conditioned on. In that sense, the CATE is more generalizable than an ATE, which involves averaging over the covariate distribution of the sample that happened to be used in the analysis. In addition, the ATE doesn't help a clinician decide on a treatment for an individual patient because most effect measures for binary treatments (e.g., the risk difference or risk ratio) necessarily depend on the level of baseline risk, and the odds ratio is not collapsible, meaning the marginal odds ratio is not equal to the conditional odds ratio even when all conditional odds ratios are equal. Frank's argument is that propensity score methods (and other design-based methods) can only estimate ATEs, and therefore should not be used, whereas logistic regression can estimate the CATE and therefore should be used.

My rebuttal to this line of reasoning is that CATEs are a pipedream and the coefficient on treatment in a logistic regression is not a valid estimate of them. This view is shared by many causal inference researchers who study methods that target the ATE and heterogeneous treatment effects. To estimate a CATE in the most conservative way, one must perform the entire analysis, whatever that may be, within each subgroup defined by the conditioning covariates. That means the estimate must be adequately powered in each subgroup, which is virtually impossible without absolutely massive datasets and oversampling small groups. One alternative is to make some smoothing assumptions or regularize the differences among the CATEs, which is done by modern machine learning methods for heterogeneous treatment effects. Even these yield CATEs that are highly imprecise or marginalize over subsets of covariates.

At the other end of the spectrum is to assume all CATEs are the same on the odds ratio scale (i.e., so treatment doesn't interact with any covariates) and the outcome-covariate relationships can be captured in a (possibly flexible) logistic regression model. To me, these are extremely strong assumptions. Simply failing to find treatment-covariate interactions in studies not powered to detect them is not evidence for the absence of such interactions. Getting the outcome model wrong will yield biased estimates for the CATE *even if the CATEs are all the same and treatment is randomized*. In addition, even if the model were correctly specified, there is no way to prove such to your audience. They simply have to take your word that the model you specified, with all its arbitrariness and strict constraints, is correct. Good cross-validated predictive accuracy (or a good AUC) is neither necessary nor sufficient for unbiased effect estimation. I, as a reader, would be extremely suspect of such an analysis and would place no faith in its estimates, even if I were to believe in strong ignorability and the homogeneity of treatment effects on the odds ratio scale.

Frank often says things like a problem with propensity score analysis is that it doesn't take into account treatment-covariate interactions. Again, I think this is not a problem with propensity score analysis in particular but simply with the choice of estimand. The ATE marginalizes over the CATEs, and in that sense does ignore treatment-covariate interactions. I agree that this makes the ATE far less scientifically and clinically useful than a CATE. And I also agree that propensity score methods can only target ATEs. But that is not a reason to avoid propensity score analysis if you want to target the ATE; it a reason to avoid targeting the ATE. And I think the ATE often genuinely is of interest, and even when it isn't, it is the best one can do without making extremely strict and unrealistic assumptions. Propensity score and other design-based analyses have the advantage over analysis-based methods in that they can be agnostic as to whether there is treatment effect heterogeneity and still arrive at a valid estimate of the ATE, whereas analysis-based method must correctly model the outcome, including all interactions between treatment and covariates, to estimate the ATE validly. That is, it is a *strength*, not a weakness, of propensity score methods that they allow you to ignore treatment effect heterogeneity.

If you categorically reject the ATE as a useful estimand, then I am sympathetic to the rejection of all design-based methods and analysis-based methods that target the ATE, including those that involve propensity scores. I still don't think a flexibly modeled logistic regression is the answer.

To summarize, I don't totally disagree with this argument because I agree that analysis-based methods often outperform design-based methods in terms of precision and validity of asymptotic inference. But this argument ignores the whole benefit of design-based methods, which is an epistemic, not statistical, advantage. Only with design-based methods can you provide evidence to your audience that you have removed structural bias due to imbalance. We have found that some analysis-based methods are equivalent to designed based methods but simply trade extrapolation for precision. Whether a researcher is willing to make such a trade-off is up to them, and whether the epistemic benefit of designed-based methods is worth the decrease in precision is up to them. Of course, we can minimize the costs of the trade-off by simply doing both, e.g., adjusting for covariates in the outcome model in the matched sample, which is a recommended and accessible strategy.

### 4. Propensity score matching is worse than inverse probability weighting

This is a somewhat esoteric argument but is one I see espoused by epidemiologists, which suggests why propensity score matching is so uncommon for epidemiological research but inverse probability weighting is more accepted. One reason for the prominence of this argument is that it appears in (at least) two major epidemiological textbooks: *Modern Epidemiology* [@rothmanModernEpidemiology2021] and *What If?* [@hernanCausalInferenceWhat2020]. An exception is in the subfield of pharmacoepidemiology, where for some reason it has flourished due to the interest of a group of researchers including XXX.

Matching and weighting, whether on the propensity score or not, are identical methods conceptually and causally, differing only in their statistical performance, which will vary from dataset to dataset and from the specific implementation of each method. There is no reason to categorically claim one method is better than the other as these textbooks do. In simulation studies comparing the most basic version of the methods, they often perform equally well or differ in performance depending on the simulation factors [@kushCovariateBalanceObservational2022; @waernbaumModelMisspecificationRobustness2012].

Liz and I try to identify some of the reasons one might prefer one or the other or expect one to perform better than the other in our paper together [@greiferMatchingMethodsConfounder2021a]. But the conclusion is what it always is when researchers ask which method is better: try both and use the one that optimizes the three key properties of a design-based method: balance, effective sample size, and representativeness. There is no guarantee that weighting will outperform matching in these domains, and there are many ways, often ignored in these epidemiology textbooks, of customizing the matching specification to avoid the problems they can face with respect to these qualities. For example, @hernanCausalInferenceWhat2020 argue that propensity score matching should be avoided because units are discarded, so the effect estimate may not transport to a relevant population. It's true that some methods of matching do this, but others, like stratification, full matching, or matching imputation can easily target the ATE, the same estimand as inverse probability weighting. In addition, sometimes targeting a narrower population is a strength, not a weakness, because it may be easier to achieve balance and maintain precision when using matching compared to weighting, and not all studies are designed to generalize to a specific population [@maoPropensityScoreWeighting2018].

The thing to remember is that 1:1 propensity score matching without replacement and standard inverse probability weighting using a logistic regression propensity score are the tips of the respective matching and weighting icebergs. On the matching side, we can do matching with replacement, which is asymptotically equivalent to inverse probability weighting [@linEstimationBasedNearest2021], or cardinality and profile matching, which use integer programming to optimize the matched sample size subject to user-specified balance constraints [@zubizarretaMatchingBalancePairing2014; @cohnProfileMatchingGeneralization2022], or genetic matching, which uses a genetic algorithm to optimally prioritize covariates to minimize imbalance [@diamondGeneticMatchingEstimating2013], or full matching, which assigns all units to subclasses to optimize a global distance criterion [@hansenOptimalFullMatching2006; @stuartUsingFullMatching2008], etc. On the weighting side, we can do entropy balancing and stable balancing weights, which maximize the effective sample size subject to balance constraints [@hainmuellerEntropyBalancingCausal2012; @zubizarretaStableWeightsThat2015; @kallbergLargeSampleProperties2022], @mccaffreyPropensityScoreEstimation2004 style generalized boosted modeling, which tunes a tuning parameter in the propensity score model to optimize balance in the weighted dataset, CBPS, which combines logistic regression with balance constraints [@imaiCovariateBalancingPropensity2014], or energy balancing, which balances the whole covariate distribution by minimizing a global imbalance statistic [@hulingEnergyBalancingCovariate2022], etc. To say weighting is superior to matching is silly when there are so many variants of each method that are targeted to different constraints and desiderata in a study.

Why I don't hate this argument is that I actually kind of agree with it. Because weights are smooth whereas matching weights involve discreteness by construction, it is easier to use standard and fast derivative-based optimization methods to find truly optimal weights, whereas optimization problems for matching are much slower due to the integer constraints or lumpiness in the objective function. The robustness benefits matching can have over weighting (which are due to the integer constraints) can be reduced by using weighting methods that specifically target the kinds of imbalance matching is meant to be robust to. That is, with weighting, it is easier to optimize criteria, manage trade-offs more effectively, and exert finer control over the resulting adjusted sample. When matching outperforms weighting, it does so by chance and in ways that are often hard to assess (e.g., balance on an unseen part of the covariate distribution).

Weighting is also easier to generalize to more complex cases, like multi-category, continuous, and longitudinal treatments, but that isn't a good reason to avoid matching when matching is appropriate. It's not like you have to pick one type of method for all scenarios and only use that one; you can and should use the method that is most appropriate for the problem at hand. It may be harder to incorporate weights into an outcome model than to simply run the model in the matched sample. Or, it may be easier to use a known-to-be-valid bootstrap procedure when weighting than to correctly incorporate pair membership into a post-matching inference.

So, in summary, I agree that weighting methods often outperform matching methods, but that is not always the case, and matching methods should not be dismissed outright as they are by epidemiology textbooks. Conceptually, matching and weighting are identical, and their only difference is in their statistical performance, which depends on the unique features of each dataset and which can often be assessed before moving forward with effect estimation.

### 5. Propensity score matching is worse than other matching methods

This is the sole thesis of @kingWhyPropensityScores2019. Propensity score matching is bad for various reasons, and other matching methods that don't share its flaws should be used instead. @kingWhyPropensityScores2019 is, in my opinion, one of the least understood papers in the literature (or at least the least understood among those that are the easiest to read). I have written at [some length](https://stats.stackexchange.com/a/481130/116195) about my thoughts on the paper, so I won't go into too much depth here. Instead, I'll focus on my evaluation of the use of the paper as an argument against using propensity scores.

I dislike this argument, mostly because it annoys me. People who say things like "propensity score matching should never be used, just see @kingWhyPropensityScores2019" piss me off, because it is clear they only read the title of the paper or at best the abstract and decided to believe it. To be fair, this is one of the most click-baity titles in the propensity score literature, written by a researcher sometimes known for his [click-baity titles](https://doi.org/10.1093/pan/mpu015). But the article, of course, makes a far more nuanced point, one which is basically impossible to disagree with but which is the opposite of what the title says: propensity score matching can be useful, but it must be done with thought and care. King and Nielsen don't trust the unwashed masses with propensity score matching, and frankly, I don't either. Their solution is to recommend against using it outright, while mine is to educate people on best practices and provide tools for implementing those practices. If you are the kind of person that believes the title of the paper, you are exactly the kind of person the paper was written for.

This argument, whether stated by King and Nielsen or by other statisticians who have found other methods to perform better than propensity score matching either theoretically or in simulations, is a purely statistical argument. It has nothing to do with causal inference or its assumptions and nothing to do with matching vs regression or matching vs instrumental variables or DiD. It is a very specific argument that takes for granted that strong ignorability is true and design-based methods are superior and matching is to be done instead of weighting. If you disagree with any of those points, @kingWhyPropensityScores2019 is immediately irrelevant for your argument.

Here are some other problems I have with this argument.

-   The problems King and Nielsen identify with propensity score matching can be assessed in one's dataset; you are not cursed to suffer its ills by considering propensity score matching as a candidate matching specification. If propensity score matching sucks in your dataset, then use another method. The ability to respecify is one of the great advantages of design-based methods and is already recommended in the literature as a best practice. Omitting propensity score matching as a candidate specification means you could be missing out on an adjustment strategy that is effective in your dataset.

-   The methods King and Nielsen recommend as replacements for propensity score matching tend to perform terribly in most datasets. They recommend Mahalanobis distance matching and coarsened exact matching, both of which are known to perform terribly in the presence of many categorical covariates and which have been found in many studies to perform worse than propensity score matching empirically [@ripolloneEvaluatingUtilityCoarsened2020]. Indeed, some go so far as to argue that coarsened exact matching should never be used [@blackTroubleCoarsenedExact2020]. Whether Mahalanobis distance matching works better than propensity score matching for a given application depends on the unique features of the dataset. @ripolloneImplicationsPropensityScore2018, who explicitly respond to King and Nielsen's argument, find that in one dataset, propensity score matching does better than Mahalanobis distance matching, and in another dataset, the opposite is true.

-   The specific implementation of propensity score matching studied by King and Nielsen is not the only one. One of their key arguments is that the propensity score matching induces the propensity score paradox, where progressively decreasing the width of the caliper makes balance worse at some point, despite making units closer on the propensity score. But not all implementations of propensity score matching involve a caliper. And even when they do, the propensity score paradox doesn't always kick in, and even when it does, it can be assessed and avoided by using a different caliper. The propensity score paradox is only harmful to the thoughtless researcher who blindly treats propensity score matching as a rote procedure expected to perform well, ignoring all best practices.

It might seem like I'm defending propensity score matching. I'm not. Propensity score matching is uniformly worse than some other matching methods. My problem is with people who use this paper as a blunt critique of any application of propensity score matching, whether the paper applies or not. And even when used in the right context, the paper's conclusions are not to avoid propensity score matching; they are to use propensity score matching carefully, looking out for the specific problems identified in the paper.

What should you do? If you are wedded to matching, try propensity score matching, and try other methods. I guarantee you will find a method better than propensity score matching for your dataset. That doesn't mean propensity score matching should never be used, and it doesn't mean that any study that used propensity score matching should be dismissed outright (at least for this reason; if you think strong ignorability is untenable, then critique the paper on those terms).

Does that mean this paper is useless? No. Here is how to correctly use @kingWhyPropensityScores2019 to critique an application of propensity score matching:

-   Did the authors use a caliper without assessing balance on a propensity score matching specification that avoided a caliper? If so, then it is possible the propensity score paradox was in effect and they made things worse by using a caliper. Ask the authors to verify and demonstrate that the caliper not only did not make balance worse but also was necessary to achieve balance. You can use @kingWhyPropensityScores2019 to justify this critique.

-   Did the authors use propensity score matching without making any attempt to guarantee close pairs on the covariates? If so, then it is possible they missed out on a better performing, more robust method of matching that reduces model dependence by being more precise. Examples of such methods include genetic matching, pair matching with exact matching constraints or calipers on the *covariates*, or Mahalanobis distance matching in a cardinality matched sample or within a propensity score calipers. If the authors' only attempt at achieving balance was to use propensity score matching with no other modifications, then you can use @kingWhyPropensityScores2019 to justify the critique that their matching specification may be poor and recommend the authors try a different matching method that prioritizes close pairs.

But if the authors used propensity score matching, did work to demonstrate that propensity score matching was optimal in their dataset, demonstrated that covariate balance was achieved in a deep and holistic way, and incorporated constraints to increase the closeness of pairs on the covariates, then you should not use @kingWhyPropensityScores2019 to critique the application.
